\documentclass[AMA,STIX1COL,]{WileyNJD-v2}


% Pandoc citation processing
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
% for Pandoc 2.8 to 2.10.1
\newenvironment{cslreferences}%
  {}%
  {\par}
% For Pandoc 2.11+
\newenvironment{CSLReferences}[3] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc} % for calculating minipage widths
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}


\articletype{Research article}

\received{2017-01-01}

\revised{2017-02-01}

\accepted{2017-03-01}

\raggedbottom

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}

\title{A demonstration of the \LaTeX class file for Statistics in
Medicine with Rmarkdown}

\author[a]{Boyi Guo*}
\author[a]{Nengjun Yi}

\authormark{Guo and Yi}

\address[a]{Department of Biostatistics, University of Alabama at
Birmingham, Birmingham, USA}

\corres{*Corresponding author name, This is sample corresponding
address. \email{xxx@uab}}

\presentaddress{This is sample for present address text this is sample
for present address text}

\abstract{TODO: Change this accordingly; Lorem ipsum dolor sit amet,
consectetur adipiscing elit. Aenean ut elit odio. Donec fermentum tellus
neque, vitae fringilla orci pretium vitae. Fusce maximus finibus
facilisis. Donec ut ullamcorper turpis. Donec ut porta ipsum. Nullam
cursus mauris a sapien ornare pulvinar. Aenean malesuada molestie erat
quis mattis. Praesent scelerisque posuere faucibus. Praesent nunc nulla,
ullamcorper ut ullamcorper sed, molestie ut est. Donec consequat libero
nisi, non semper velit vulputate et. Quisque eleifend tincidunt ligula,
bibendum finibus massa cursus eget. Curabitur aliquet vehicula quam non
pulvinar. Aliquam facilisis tortor nec purus finibus, sit amet elementum
eros sodales. Ut porta porttitor vestibulum. Integer molestie, leo ut
maximus aliquam, velit dui iaculis nibh, eget hendrerit purus risus sit
amet dolor. Sed sed tincidunt ex. Curabitur imperdiet egestas tellus in
iaculis. Maecenas ante neque, pretium vel nisl at, lobortis lacinia
neque. In gravida elit vel volutpat imperdiet. Sed ut nulla arcu. Proin
blandit interdum ex sit amet laoreet. Phasellus efficitur, sem hendrerit
mattis dapibus, nunc tellus ornare nisi, nec eleifend enim nibh ac
ipsum. Aenean tincidunt nisl sit amet facilisis faucibus. Donec odio
erat, bibendum eu imperdiet sed, gravida luctus turpis.}

\keywords{Class file; \LaTeX; Statist. Med.; Rmarkdown;}

\maketitle

\newcommand{\bs}[1]{\boldsymbol{#1}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\begin{itemize}
\tightlist
\item
  What is the problem
\item
  Previous solutions
\item
  Our method
\item
  The implication of our method
\item
  Structure of the rest of the paper
\end{itemize}

\hypertarget{spike-and-slab-additive-model}{%
\section{Spike-and-slab Additive
Model}\label{spike-and-slab-additive-model}}

Following the notation in the previous section \ref{Notation}, suppose
we have the model matrix \(\boldsymbol{X}\) derived from spline
functions of choice \(h_j, j = 1, \dots, p.\) for the \(p\) covariates
that are modeled nonparametrically, the total dimension of
\(\boldsymbol{X}\) is \(n \times \sum\limits_{j=1}^p k_j\). Column-wise
centering and re-parameterization introduced in the GAM as random effect
section{[}\^{}5{]} are performed. The re-parameterization has two
purposes. First of all the re-parameterization reshapes the columns to
be linearly independent to each other, and hence, independent prior can
be set to the coefficients. Secondly, the re-parameterization decomposes
the model matrix \(\boldsymbol{X}_j\) for the \(j\)th covariates to two
sub-matrices representing the null space of the spline and penalized
space of the spline, \[
\boldsymbol{X}_j = \begin{bmatrix}
\boldsymbol{X}_j^{0} & \boldsymbol{X}_j^{pen}
\end{bmatrix},
\] where \(\boldsymbol{X}_j^{null}\) is the column space representing
the null space of the spline model, and \(\boldsymbol{X}_j^{pen}\)
denotes the column spaces that need to be shrunk for smoothness of the
spline. The purpose for this re-parameterization is to isolate the null
space from the smoothing space, such that answering the question, if
non-linear relationship is necessary, becomes easy by deciding if the
corresponding coefficients of the \(\boldsymbol{X}_j^{pen}\) are jointly
zero. This strategy is widely used, previously mentioned in
\citet{Scheipl2012} and the references there in.\footnote{Possibly
  select some to add here.} Hereafter, we denote the coefficients of the
null space as \(\boldsymbol{\beta}^{0}_j\) and the coefficients of the
penalized space as \(\boldsymbol{\beta}^{pen}_j\). The spline function
for the \(j\)the covariate is \[
f_j(X) = f_j^0(X) + f_j^{pen}(X) = {\beta_j^0}^T X_j^0 + {\beta_j^{pen}}^T X_j^{pen},
\] where the superscript \(^t\) denotes matrix transpose.

Without loss of generality, we assume that the model doesn't include the
the parametric part of the model \(\boldsymbol{A}\). The coefficients of
\(\boldsymbol{A}\) can be estimated easily in the proposed EM algorithms
by local update conditioning on the temporary value of the spline
coefficients.

~

\hypertarget{spike-and-slab-spline-prior}{%
\subsection{Spike and slab spline
prior}\label{spike-and-slab-spline-prior}}

A generalized spline model can be formulated as \[
y_i \sim EF(\mu_i, \phi), \quad g(\mu_i) = \sum\limits_{j=1}^p f_j(x_{ij}), \quad i = 1, \dots, n.
\] The link function of the mean can be expressed in a matrix form \[
g(\mu_i) = \sum\limits_{j=1}^p\left[f_j^0(x_{ij}) + f_j^{pen}(x_{ij})\right] = \sum\limits_{j=1}^p\left[{\beta_j^0}^T X_{ij}^0 + {\beta_j^{pen}}^T X_{ij}^{pen}\right]
 = \beta^T X_i,
\] where \[
\begin{aligned}
  X_i = \begin{bmatrix}X^0_{i1}\\X^{pen}_{i1}\\\vdots \\X^0_{ip}\\X^{pen}_{ip}\end{bmatrix}
& &
  \beta = \begin{bmatrix}\beta^0_1\\\beta^{pen}_{1}\\\vdots \\\beta^0_{p}\\\beta^{pen}_{p}\end{bmatrix}
\end{aligned}
\]

\hfill\break

\hypertarget{mixture-normal-prior}{%
\subsubsection{Mixture-Normal Prior}\label{mixture-normal-prior}}

Given the full deign matrix
\(X = \begin{pmatrix} X_1 & X_2 & \dots & X_n \end{pmatrix}^T\), the
likelihood function of the generalized model can be written as \[
p(y|X\beta, \phi) = \prod\limits^n_{i=1}p(y_i|\beta^T X_i, \phi).
\]

Considering the dependency of spline bases, the grouping nature of the
bases should be taken care of. Naturally, the Bayesian hierarchical
models allow us easily to take account of the dependency of the spline
bases as grouping information and provide more reliable estimation. Our
hierarchical GAMs specify the spike-and-slab mixture normal prior on the
coefficients: for the coefficients of the \(j\)th predictor,
\(\beta^0_j\) and \(\beta^{pen}_{jk}\), we have \[
\begin{aligned}
  \beta^0_{j} |\gamma^0_{j},s_0,s_1 &\sim N(0,(1-\gamma^0_{j}) s_0 + \gamma^0_{j} s_1)\\
  \beta^{pen}_{jk} | \gamma^{pen}_{j},s_0,s_1 &\sim N(0,(1-\gamma^{pen}_{j}) s_0 + \gamma^{pen}_{j} s_1), 
\end{aligned}
\] where \(\gamma^0_{j}\in\{0,1\}\) and
\(\gamma^{pen}_{j}\in \{0,1\}^{K_j}\) are latent indicator variables,
indicating if the model includes the linear effect and non-linear effect
of the \(j\)th variable respectively; \(s_0\) and \(s_1\) are scale
parameters, assuming \(0 < s_0 < s_1\) and given. Thus, the priors are
formulated as a mixture of the shrinkage prior normal(0, \(s_0\)) and
the weakly informative prior normal(0, \(s_1\)), which are spike and
slab components respectively. Like any other spike and slab priors, the
spike density is to contain the minimum to zero effects, while the slab
density is to allow large effects. The scale parameters \(s_0\) and
\(s_1\) can be treated as tuning parameters, which can be optimized via
cross-validation. A discussion of how to choose the scale parameters
comes later.

To note here, unlike the original spike-and-slab prior where each
predictor have only indicator variable, \(\gamma_j\), we have two
indicators associated with each predictor, \(\gamma^0_j\) and
\(\gamma^{pen}_j\) to decide the inclusion of the linear effect and
non-linear effect of the predictor respectively. It is possible to add
more constraints on the model, assuming that one indicator variable
decides the inclusion of both the linear effect and non-linear effect,
i.e.~\(\gamma_j = \gamma^0_j = \gamma^{pen}_j\). Such formulations,
commonly referred as ``all-in-all-out'', has been implemented in many
high dimensional Bayesian GAMs, see Yang \& Narisetty (2020) and Bai
(Work in progress). The ``all-in-all-out'' set-up was criticized for its
inability to answering an important research question, if the effects
are non-linear or not. On the other hand, the model can be much more
flexible by allowing each dimension of the penalized space to have an
independent indicator, i.e.~\(\gamma^{pen}_{jk}\) of
\(\beta^{pen}_{jk}\) for \(k = 1\dots, K_j\). This prior set-up
generalized to the group spike-and-slab. The additional flexibility
doesn't not necessarily grand the model neither theoretical
justification nor analytic simplicity. The spline penalty assumes a
common smooth shrinkage \(\lambda_j\) applied to all coefficients of the
penalized space, which contradicts to the model setup. Besides, having
multiple indicator variables complicates the decision of if non-linear
effects are necessary.

We set up the hyper-prior of \(\boldsymbol{\gamma}\) to allow local
adaption of the shrinkage, using a binomial distribution. The two
indicators of the \(j\)th predictor, \(\gamma^{0}_j\) and
\(\gamma^{pen}_j\), shares the same probability parameter, \[
\begin{aligned}
\gamma_{j}^{0} | \theta_j &\sim Bin(\gamma^{0}_{j}|1, \theta_j)\\
\gamma_{j}^{pen} | \theta_j &\sim Bin(\gamma^{pen}_{j}|1, \theta_j).
\end{aligned}
\]

This is to leverage the fact that the probability of selecting the bases
of a smooth function should be similar, while allowing different penalty
on the null space and penalty space of the spline function. The hyper
prior of \(\gamma_{j}^{0}\) decides the sparsity of the model at the
function selection level, while that of \(\gamma_{j}^{pen}\) decides the
smoothness of the spline function at individual predictor level.
Meanwhile, we specify that \(\gamma_{j}^0\) and \(\gamma_{j}^{pen}\) are
independently distributed for analytic simplicity. However, this set-up
could possibility result in a situation that is not theoretically
possible: the non-linear component is selected, but the linear component
is not. This can be addressed analytically by forcing to include the
linear component when non-linear component is included. Another possible
solution is to impose an dependent structure of \(\gamma_{j}^{pen}\) on
\(\gamma_{j^{0}}\), i.e.~\(\gamma_j^{pen}|\gamma_{j}^{0}, \theta_j\).

We further specify the parameter \(\theta_j\) follows a beta
distribution with given shape parameters \(a\) and \(b\), \[
\theta_j \sim Beta(a, b).
\] The beta distribution is a conjugate prior for the binomial
distribution and hence provides some computation convenience.
Specifically, we focus on a special case of beta distribution, uniform
(0,1) for simplicity and convenience. To note, when the variable have
large effects in any of the bases, the parameter \(\theta_j\) will be
estimated large, which in turn encourages the model to include the rest
of bases, achieving the local adaption among spline bases. Hereafter, we
refer the hierarchical spline GAM with the spike-and-slab mixture normal
prior as the ssGAM-MN.

~

\hypertarget{mixture-double-exponential-prior}{%
\subsubsection{Mixture Double Exponential
Prior}\label{mixture-double-exponential-prior}}

One of the critics received by the spike-and-slab mixture normal prior
is that the tails of a normal distribution diminishes to zero too fast,
which renders problem when estimating the large effects. In contrast,
distributions with heavier tails are preferred, for example \(t\)
distribution with small degree of freedom and double exponential
distribution. Specifically, double exponential prior is equivalent to
the LASSO penalty in the frequentist models, providing a sparse
solution. Meanwhile, the estimation algorithm of LASSO penalty are
extremely efficient, greatly reducing the computation needs. Recent
developments of spike-and-slab priors include spike-and-slab LASSO
prior, i.e.~spike-and-slab mixture double exponential prior. We extend
the spike-and-slab LASSO prior to high-dimensional GAMs, providing
computationally efficiency and sparse solution to our proposed ssGAM-MN.

The set-up of ssGAM-LASSO model is similar to ssGAM-MN, except we
replace the normal distribution with double exponential distribution.
The prior distributions of the coefficients \(\beta^0_j, \beta^{pen}_j\)
for the \(j\) predictor can be formulated as \[
\begin{aligned}
  \beta^0_{j} |\gamma^0_{j},s_0,s_1 &\sim DE(0,(1-\gamma^0_{j}) s_0 + \gamma^0_{j} s_1)\\
  \beta^{pen}_{jk} | \gamma^{pen}_{j},s_0,s_1 &\sim DE(0,(1-\gamma^{pen}_{j}) s_0 + \gamma^{pen}_{j} s_1).
\end{aligned}
\] The hyper-priors for \(\gamma^0_j, \gamma^{pen}_j, \theta_j\) are
kept the same as specified in ssGAM-MN.

\hfill\break

\hypertarget{alforithm-for-fitting-ssgams}{%
\subsection{Alforithm for fitting
ssGAMs}\label{alforithm-for-fitting-ssgams}}

Parsimonious computation is always encouraged in high-dimension data
analysis. Normally, Bayesian methodology loses its advantages over
Frequentest penalized model because of the computation cost. Previous
Bayesian spline models heavily relies on the MCMC algorithm to establish
posterior distribution of parameters. One of the exception is Bai (in
progress), who a grouped spike-and-slab LASSO prior \citep{Rockova2018}
to address the grouping structure of spline bases. He focused on the MAP
estimator of the parameters, but failed to address the uncertainty
measure of the estimates. The main difference between Bai's work and the
proposed models are two-folded: 1) the proposed models are feasible to
answer the question if the non-linear effects are necessary for the
predictors, while Bai's work can't due to the ``all-in-all-out''
approach; 2) the proposed models can provide uncertainly measures in
addition to the point estimates of the coefficients, while Bai's work
can't not due to the model fitting algorithm.

In the follow section, we introduced two expectation-maximization (EM)
based algorithms for fast computing, EM-IRLS and EM-coordinate Descent
algorithm. Instead of simulating the posterior distribution via MCMC, we
iteratively estimate the MAP. Compared to the EM-IRLS algorithm, the
EM-coordinate descent provides an expedited model fitting process at the
cost of model uncertainty.

\hfill\break

\hypertarget{em-algorithms}{%
\subsubsection{EM algorithms}\label{em-algorithms}}

EM algorithm is an iterative algorithm to find the maximum likelihood
estimates or the Bayesian counterpart MAP estimates. It is commonly used
when some necessary data to establish the likelihood function are
missing. Instead, the algorithm maximizes the expectation, in respect to
the ``missing'' data, of the likelihood function.

The recursive algorithm consists of two steps:

\begin{itemize}
\tightlist
\item
  E-step: to calculate the expectation of the likelihood function
  conditioning on some ``missing'' data
\item
  M-step: to maximize the spectated likelihood function to calculate the
  parameter of interest
\end{itemize}

For ssGAM, we define the parameters of interest as
\(\Theta = {\boldsymbol{\beta}, \boldsymbol{\theta}, \phi}\) and
consider the latent binary indicators \(\boldsymbol{\gamma}\) as
nuisance parameters of the model. Our objective is to find the
parameters \(\Theta\) that maximize the posterior density function,or
equivalently, the logarithm of the density function, \[
\begin{aligned}
& \text{argmax}_{\Theta}
\log p(\Theta, \boldsymbol{\gamma}| \textbf{y}, \textbf{X}) \\
&= \log p(\textbf{y}|\boldsymbol{\beta}, \phi) + \log p(\phi) + \sum\limits_{j=1}^p\left[\log p(\beta^0_j|\gamma^0_j)+\sum\limits_{k=1}^{K_j} \log p(\beta^{pen}_{jk}|\gamma^{pen}_{jk})\right]\\
& +\sum\limits_{j=1}^{p} \left[ (\gamma^0_j+\gamma_{j}^{pen})\log \theta_j + (2-\gamma^0_j-\gamma_{j}^{pen}) \log (1-\theta_j)\right] +  \sum\limits_{j=1}^{p}\log p(\theta_j)
\end{aligned}
\]\\

We use the EM algorithm to find the MAP estimate of \(\Theta\). Since
the latent binary indicators \(\boldsymbol{\gamma}\) are not of our
primary interest, we treat them as as the ``missing data'' in the EM
algorithms. Naturally In the E-step, we calculate the expectation of
posterior density function of
\(\log p(\Theta, \boldsymbol{\gamma}| \textbf{y}, \textbf{X})\) with
respect to the latent indicators \(\boldsymbol{\gamma}\) conditioning on
the values from previous iteration \(\Theta^{t-1}\), \[
E_{\boldsymbol{\gamma}|\Theta^{t-1}}\log p(\Theta, \boldsymbol{\gamma}| \textbf{y}, \textbf{X}) .
\] Hereafter, we use the short hand notation
\(E(\cdot)\equiv E_{\boldsymbol{\gamma}|\Theta^{t-1}}(\cdot)\). In the
M-step, we find the \(\Theta^{t}\) that maximize
\(E_{\boldsymbol{\gamma}|\Theta^{t-1}}\log p(\Theta, \boldsymbol{\gamma}| \textbf{y}, \textbf{X})\).
To note here, the log-posterior of ssGAM (up to additive constants) can
be written as a two-part equation

\[ \log p(\Theta, \boldsymbol{\gamma}| \textbf{y}, \textbf{X}) = Q_1(\beta, \phi) + Q_2 (\gamma,\theta),\]
Where
\[ Q_1(\boldsymbol{\beta}, \phi) = \log p(\textbf{y}|\boldsymbol{\beta}, \phi) + \log p(\phi) + \sum\limits_{j=1}^p\left[\log p(\beta^0_j|\gamma^0_j)+\sum\limits_{k=1}^{K_j} \log p(\beta^{pen}_{jk}|\gamma^{pen}_{jk})\right]\]
and \[
Q_2(\boldsymbol{\gamma},\boldsymbol{\theta}) = \sum\limits_{j=1}^{p} \left[ (\gamma^0_j+\gamma_{j}^{pen})\log \theta_j + (2-\gamma^0_j-\gamma_{j}^{pen}) \log (1-\theta_j)\right] +  \sum\limits_{j=1}^{p}\log p(\theta_j) .\]
\(Q_1\) and \(Q_2\) are respectively, the log-likelihood of model
coefficients and the dispersion parameter \(\boldsymbol{\beta}, \phi\),
and latent indicators \(\boldsymbol{\gamma}, \theta\). Conditioning on
\(\gamma_{jk}\), \(Q_1\) and \(Q_2\) are independent and can be
maximized separately for \(\boldsymbol{\beta}, \phi\) and
\(\boldsymbol{\theta}\).

The E- and M- steps are iterated until the algorithm converge. Depending
on the choice of the mixture priors, either mixture normal or mixture
double exponential, the maximization in the M-step can take different
algorithms, IRLS for uncertainty measures and coordinate descent for
faster computation.

\hfill\break

\hypertarget{em-irls}{%
\subsubsection{EM-IRLS}\label{em-irls}}

The main idea of the EM-IRLS algorithm is to use the iterative
re-weighted least square algorithm to find the estimate
\(\boldsymbol{\beta}, \phi\) that maximizes \(E(Q_1)\). Recall the prior
set-up of mixture normal prior, we have the prior densities of
\(\boldsymbol{\beta}\) conditioning on \(\boldsymbol{\gamma}\)

\[
\begin{aligned}
p(\boldsymbol{\beta }| \boldsymbol{\gamma}) &\propto \prod\limits_{j=1}^{p}\left[{S^0_j}^{-1/2}\exp(-1/2({\beta^0_{j}}^2/S^0_{j}))\prod\limits_{k=1}^{K_j}S_{j}^{-1/2}\exp(-1/2(\beta_{jk}^2/S_{j}))\right],%\\
% p(\bs\gamma | \bs \theta) & = \prod\limits_{j=1}^{p}\prod\limits_{k=1}^{K_j} \theta_j^{\gamma_{jk}} (1-\theta_j)^{1-\gamma_{jk}}.
\end{aligned}
\] and \[
\begin{aligned}
E(Q_1) &= \log p(\textbf{y}|\boldsymbol{\beta}, \phi) + \log p(\phi) + \sum\limits_{j=1}^p\left[E({S^0_j}^{-1}){\beta^0_j}^2+\sum\limits_{k=1}^{K_j}E(S^{-1}_{j})\beta_{jk}^2\right]%\\
%E(Q_2) &= \sum\limits_{j=1}^{p}\sum\limits_{k=1}^{K_j} \left[ E(\gamma_{jk})\log \theta_j + (1-E(\gamma_{jk})) \log (1-\theta_j)\right] +  \sum\limits_{j=1}^{p}\log p(\theta_j)
\end{aligned}
\] where \(S_{j}^0 = (1-\gamma^{0}_{j}) s_0 + \gamma^{0}_{j} s_1\) and
\(S_{j} = (1-\gamma^{pen}_{j}) s_0 + \gamma^{pen}_{j} s_1\). To
calculate two unknown quantities \(E({S_j^0}^{-1})\) and
\(E(S^{-1}_j)\), the posterior probability
\(p^0_{j} \equiv p(\gamma^{0}_{j}=1|\Theta^{t-1})\) and
\(p_{j} \equiv p(\gamma^{pen}_{j}=1|\Theta^{t-1})\) are necessary, which
can be derived via Bayes' theorem. The calculation for \(p_j\) is
slightly different from that of \(p^0_j\), as \(p_j\) depends on the
value of \(\beta_{jk}\) for \(k=1, \dots, K_j\) and \(p^0_j\) only
depends on \(\beta_j^0\). The calculation follows the equations below.

\[
\begin{aligned}
p_{j}^0 &= \frac{Pr(\gamma_{j}^0 = 1|\theta_j)f(\beta_{j}^0|\gamma_{j}^0=1, s_1) }{Pr(\gamma_{j}^0 = 1|\theta_j)f(\beta_{j}^0|\gamma_{j}^0=1, s_1) + Pr(\gamma_{j}^0 = 0|\theta_j)f(\beta^0_{j}|\gamma^0_{j}=0, s_0)}\\
p_{j} &= \frac{Pr(\gamma^{pen}_{j} = 1|\theta_j)\prod\limits_{k=1}^{K_j}f(\beta_{jk}|\gamma^{pen}_{j}=1, s_1) }{Pr(\gamma^{pen}_{j} = 1|\theta_j)\prod\limits_{k=1}^{K_j}f(\beta_{jk}|\gamma^{pen}_{j}=1, s_1) + Pr(\gamma^{pen}_{j} = 0|\theta_j)\prod\limits_{k=1}^{K_j}f(\beta_{jk}|\gamma^{pen}_{j}=0, s_0)}
\end{aligned}
\]

and
\(Pr(\gamma_{j}^{0} = 1|\theta_j) =Pr(\gamma_{j}^{pen} = 1|\theta_j) = \theta_j\),
\(Pr(\gamma_{j}^{0} = 0|\theta_j) = Pr(\gamma_{j}^{pen} = 0|\theta_j) = 1-\theta_j\),
\(f(\beta|\gamma=1, s_1) = \text{Normal}(\beta|0 , s_1)\),
\(f(\beta|\gamma=0, s_0) = \text{Normal}(\beta|0 , s_0)\). It is trivial
to show \[
\begin{aligned}
&E(\gamma^0_{j})  = p^0_{j} & &E(\gamma^{pen}_{j})  = p_{j}\\
&E({S^0}^{-1}_{j}) = \frac{1-p^{0}_{j}}{s_0} + \frac{p^{0}_{j}}{s_1} & &E(S^{-1}_{j}) = \frac{1-p_{j}}{s_0} + \frac{p_{j}}{s_1}.
\end{aligned}
\]

With \(E({S^0}^{-1}_{j})\) and \(E(S^{-1}_{j})\) known, \(E(Q_1)\) can
be also viewed as a \(l_2\) penalized likelihood function:
\(E({S^0}^{-1}_{j})\) controls the shrinkage of the linear effect,
informing if the predictor should be included in the model;
\(E(S^{-1}_{j})\) controls the smoothness of the spline function.
\(l_2\) penalized likelihood function can be maximize using the IWLS
algorithm. The likelihood function can be approximate by a weighted
normal likelihood: \[
  p(\textbf{y}|\boldsymbol{\beta}, \phi) \approx Normal(\textbf{z}|X\beta, \phi\Sigma)
\] where the `normal response' \(z_i\) and `weight' \(w_i\) are called
the pseudo-response and pseudo-weight, respectively.The pseudo data and
the pseudo-weight are calculated by \[
    \begin{aligned}
    z_i &= \hat\eta_i - \frac{L^{'}((y_i|\hat\eta_i))}{L^{''}((y_i|\hat\eta_i))}& w_i &= - L^{''}((y_i|\hat\eta_i)),
    \end{aligned}
    \] where \(\eta_i = \sum\limits_{j=1}^p \hat f_j^{(t)}(x_{ij})\) is
the estimated linear predictor using current estimated value of
\(\boldsymbol{\beta}\), \(L^{'}\) and \(L^{''}\) are the first
derivative and second derivative of the log-likelihood function. The
\(l_2\) penalty, \$
\sum\limits\_\{j=1\}\^{}p\left[E({S^0_j}^{-1}){\beta^0_j}^2+\sum\limits_{k=1}^{K_j}E(S^{-1}_{j})\beta_{jk}^2\right]\$
can be seems as a multivariate normal distribution of mean
\(\textbf{0}\) and variance-covariance matrix
\(\Sigma(E({S^0}_1^{-1}), E(S_1^{-1}), \dots, E({S_p^0}^{-1}), E(S_p^{-1})\),
conditioning on the value of \(E({S^0_1}^{-1}), \dots E({S^0_p}^{-1})\).
An augmentation step combines the normal approximation and multivariate
normal for \(\boldsymbol{\beta}\), similarly to Ridge regression model
fitting. The estimates of \(\boldsymbol{\beta}\) and \(\phi\) can be
quickly updated in the IRLS algorithm.

The remaining parameters of interest \(\boldsymbol{\theta}\) can be
updated by maximizing \(E(Q_2)\). As the beta distribution is a
conjugate prior for Bernoulli distribution, \(\boldsymbol{\theta}\) can
be easily updated with a closed form equation: \[
\theta_j = \frac{p^0_j + p_{j} + a - 1 }{a + b}.
\]

Totally, the framework of the proposed EM IRLS algorithm was summarized
as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Choose a starting value \(\boldsymbol{\beta}^{(0)}\) and
  \(\boldsymbol{\theta}^{(0)}\) for \(\boldsymbol{\beta}\) and
  \(\boldsymbol{\theta}\). For example, we can initialize
  \(\boldsymbol{\beta}^{(0)} = \boldsymbol{0}\) and
  \(\boldsymbol{\theta}^{(0)} = \boldsymbol{0}.5\)
\item
  Iterate over the E-step and M-step until convergence
\end{enumerate}

E-step: calculate \(E(\gamma^0_{j})\), \(E(\gamma^{pen}_{j})\)and
\(E({S^0}^{-1}_{j})\) , \(E({S}^{-1}_{j})\)with estimates of
\(\Theta^{(t-1)}\) from previous iteration

M-step:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  Update \(\boldsymbol{\beta}^{(t)}\) using the IRLS algorithm
\item
  Update \(\boldsymbol{\theta}^{(t)}\) using the closed form equation
\end{enumerate}

We assess convergence by the criterion:
\(|d^{(t)}-d^{(t-1)}|/(0.1+|d^{(t)}|)<\epsilon\), where
\(d^{(t)} = -2l(\beta^{(t)},\phi^{(t)})\) is the estimate of deviance at
the \(t\)th iteration, and \(\epsilon\) is a small value (say
\(10^{-5}\)).

\hfill\break

\hypertarget{em-coordinate-descent}{%
\subsubsection{EM-Coordinate Descent}\label{em-coordinate-descent}}

When the prior distribution is set to mixture double exponential,
coordinate descent algorithm can be used to estimate
\(\boldsymbol{b}eta\), replacing IRLS, in the M-step. Coordinate descent
is an optimization algorithm that offers extreme computational
advantage, and famous for its application in optimizing the \(l_1\)
penalized likelihood function.

Recall, the density function of spike-and-slab mixture double
exponential prior can be written as \[
f(\beta|\gamma, s_0, s_1) = \frac{1}{(1-\gamma)s_0 + \gamma s_1}\exp(-\frac{|\beta|}{(1-\gamma)s_0 + \gamma s_1}).
\]

This can simplify our posterior likelihood function to \[
E(Q_1) = \log p(\textbf{y}|\boldsymbol{\beta}, \phi) + \log p(\phi) + \sum\limits_{j=1}^p\left[E({S^0_j}^{-1})|\beta^0_j|+\sum\limits_{k=1}^{K_j}E(S^{-1}_{j})|\beta_{jk}|\right]
,
\] where the approximation of \(E({S^0_j}^{-1}\) and \(E(S^{-1}_{j})\)
remains the same as in the EM-IRLS algorithm. The formulation of
\(E(Q_1)\) can be seen as a \(l_1\) penalized likelihood function, and
optimized via coordinate descent algorithm. The estimates can be used to
further estimates \(\boldsymbol{\theta}\) in the M-step, following the
same equation in the EM-IRLS.

Totally, the framework of the proposed EM-coordinate descent algorithm
was summarized as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Choose a starting value \(\boldsymbol{\beta}^{(0)}\) and
  \(\boldsymbol{\theta}^{(0)}\) for \(\boldsymbol{\beta}\) and
  \(\boldsymbol{\theta}\). For example, we can initialize
  \(\boldsymbol{\beta}^{(0)} = \boldsymbol{0}\) and
  \(\boldsymbol{\theta}^{(0)} = \boldsymbol{0}.5\)
\item
  Iterate over the E-step and M-step until convergence
\end{enumerate}

E-step: calculate \(E(\gamma^0_{j})\), \(E(\gamma^{pen}_{j})\)and
\(E({S^0}^{-1}_{j})\) , \(E({S}^{-1}_{j})\)with estimates of
\(\Theta^{(t-1)}\) from previous iteration

M-step:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  Update \(\boldsymbol{\beta}^{(t)}\) using the coordinate descent
  algorithm
\item
  Update \(\boldsymbol{\theta}^{(t)}\) using the closed form equation
\end{enumerate}

We assess convergence by the criterion:
\(|d^{(t)}-d^{(t-1)}|/(0.1+|d^{(t)}|)<\epsilon\), where
\(d^{(t)} = -2l(\beta^{(t)},\phi^{(t)})\) is the estimate of deviance at
the \(t\)th iteration, and \(\epsilon\) is a small value (say
\(10^{-5}\)).

~

\hypertarget{selecting-optimal-scale-values}{%
\subsection{Selecting Optimal Scale
Values}\label{selecting-optimal-scale-values}}

Our proposed prior, spike and slab spline prior, requires two preset
scale parameters (\(s_0\), \(s_1\)). Hence, we would construct a two
dimensional grid, consists of different pairs of (\(s_0\), \(s_1\))
value. However, previous research on the topic\footnote{TODO: add
  citation} suggested the value of slab scale \(s_1\) have less impact
on the final model and is recommended to be set as a generally large
value of \(s_1 = 1\) that provides no or weak shrinkage. More attention
is focused on examining different values of spike scale \(s_0\). Instead
of the 2-D grid, We consider a sequence of \(L\) decreasing values
\(\{s_0^l\}: 0 < s_0^1 < s_0^2 < \dots < s_0^L < s_1\). Increasing the
spike scale s\_0 tends to include more non-zero coefficients in the
model. An measure of preference, e.g.~area under the curve (AUC), mean
squared error, can be used to facilitate the selection of a final model.
The procedure is similar to the lasso implemented in the widely used R
package \texttt{glmnet}, which quickly fits the lasso model over a list
of values of \(\lambda\) covering the entire range, giving a sequence of
models for users to choose from.

\hypertarget{simulation}{%
\section{Simulation}\label{simulation}}

\hypertarget{real-data-analysis}{%
\section{Real Data Analysis}\label{real-data-analysis}}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\begin{verbatim}
x = rnorm(10)
y = rnorm(10)
plot(x, y)
\end{verbatim}

\begin{figure}
\centering
\includegraphics{main_files/figure-latex/plot-ref-1.pdf}
\caption{Fancy Caption\label{fig:plot}}
\end{figure}

\ldots and can be referenced (Figure \ref{fig:plot}) by including the
\texttt{\textbackslash{}\textbackslash{}label\{\}} tag in the
\texttt{fig.cap} attribute of the R chunk:
\texttt{fig.cap\ =\ "Fancy\ Caption\textbackslash{}\textbackslash{}label\{fig:plot\}"}.
It is a quirky hack at the moment, see
\href{https://github.com/yihui/knitr/issues/323}{here}.

Analogously, use Rmarkdown to produce tables as usual:

\begin{verbatim}
if (!require("xtable")) install.packages("xtable")
\end{verbatim}

\begin{verbatim}
## Loading required package: xtable
\end{verbatim}

\begin{verbatim}
xt <- xtable(head(cars), caption = "A table", label = "tab:table")
print(xt, comment = FALSE)
\end{verbatim}

\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
 & speed & dist \\ 
  \hline
1 & 4.00 & 2.00 \\ 
  2 & 4.00 & 10.00 \\ 
  3 & 7.00 & 4.00 \\ 
  4 & 7.00 & 22.00 \\ 
  5 & 8.00 & 16.00 \\ 
  6 & 9.00 & 10.00 \\ 
   \hline
\end{tabular}
\caption{A table} 
\label{tab:table}
\end{table}

\hypertarget{cross-referencing}{%
\subsection{Cross-referencing}\label{cross-referencing}}

The use of the Rmarkdown equivalent of the \LaTeX cross-reference system
for figures, tables, equations, etc., is encouraged (using
\texttt{{[}@\textless{}name\textgreater{}{]}}, equivalent of
\texttt{\textbackslash{}ref\{\textless{}name\textgreater{}\}} and
\texttt{\textbackslash{}label\{\textless{}name\textgreater{}\}}). That
works well for citations in Rmarkdown, not so well for figures and
tables. In that case, it is possible to revert to standard
\LaTeX syntax.

\hypertarget{references}{%
\section{References}\label{references}}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\end{CSLReferences}

\bibliography{bibfile.bib}


\end{document}
