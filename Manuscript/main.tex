\documentclass[AMA,STIX1COL,]{WileyNJD-v2}


% Pandoc citation processing


\articletype{Research article}

\received{2021-01-01}

\revised{2021-02-01}

\accepted{2021-03-01}

\raggedbottom

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}

\title{Spike-and-Slab Generalized Additive Models and Fast Algorithms
for High-Dimensional Data}

\author[a]{Boyi Guo*}
\author[a]{Nengjun Yi}

\authormark{Guo and Yi}

\address[a]{Department of Biostatistics, University of Alabama at
Birmingham, Birmingham, USA}

\corres{*Corresponding author name, This is sample corresponding
address. \email{xxx@uab}}

\presentaddress{This is sample for present address text this is sample
for present address text}

\abstract{Spike-and-slab priors have been extensively studied for
variable selection, and recently been extended to function selection in
generalized additive models (GAMs). Most of the previous works on
fitting spike-and-slab GAM relies on Markov chain Monte Carlo (MCMC)
algorithms to identify the promising subsets of functions. The
computational burden of MCMC makes these models less feasible,
especially in high-dimensional setting. Besides, most of the current
spike-and-slab GAM consider an ``all-in-all-out'' approach for function
selection, rendering less flexibility to answer if non-linear effects
are necessary in the model. We propose Bayesian hierarchical generalized
additive models for function selection and prediction. The proposed
models employ spike-and-slab priors, mixture normal and mixture double
exponential, for smooth and sparse solution. Two EM based algorithms,
EM-IWLS and EM-Coordinate Descent, are developed for accelerated
computation. Simulation studies and real data analysis demonstrated
improved performance against the state-of-art models, mgcv and COSSO.
The software implementation of the proposed methods is freely available
via the R package BHAM.}

\keywords{Spike-and-Slab Priors; High-Dimensional Data; Generalized
Additive Models; EM-IWLS; EM-Coordinate Decent}

\maketitle

\newcommand{\bs}[1]{\boldsymbol{#1}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\begin{itemize}
\item
  What is the problem

  \begin{itemize}
  \tightlist
  \item
    what is the GAM
  \item
    expanding GAM in to high-dimensional frame work
  \item
    Why this is important
  \item
    Why this is hard
  \end{itemize}
\item
  Previous solutions

  \begin{itemize}
  \tightlist
  \item
    What are the methods
  \item
    What are their problems
  \end{itemize}
\item
  Introduce the wiggliness matrix
\item
  Our method
\item
  The implication of our method
\item
  Fast
\item
  Structure of the rest of the paper
\end{itemize}

Nonlinear effect modeling has long history. Early work of GAMs can date
back to 1979 when \citet{Cleveland1979} proposed local smoothing for
bivariate analysis. Since then, many mathematical and statistical models
has been proposed, see xxx\footnote{TODO: insert an review citation}.
With recent advancement of computing technology, many machine learning
algorithms that is computationally intensive start to gainmuch
popularity in outcome prediction when non-linear effects assumed. Such
models generally have improved prediction accuracy over classic
statistical models, but have a fatal flaw, lack of transparency and
interpretability. The primary objective of biomedical studies is to
understand the etiology of diseases, which requires not only the
knowledge if a biomarker is predictive of the disease, but also how the
biomarker cause the disease. ``Black box'' models could less likely to
be helpful.

As a balance between interpretability and model flexibility,
semi-parametric and non-parametric regression models are studied. Among
those, generalized additive model (GAM) proposed in the seminal work of
\citet{Hastie1987} gains the most popularity thanks to its flexibility
and easiness to incorporate many different smoothing functions. A
response variable, \(Y\), that is assumed to follow some exponential
family distribution \(EF(\mu. \phi)\), can be expressed as the summation
of smooth functions of a given \(p\)-dimensional vector of covariates
\(\boldsymbol{x}\), written as \[
E(Y|\boldsymbol{x}) = g^{-1}(a + \sum\limits^p_{j=1}f_j(x_j)),\qquad E[f_j(x_j)] = 0.
\] where \(g^{-1}\) is the inverse function of a smooth monotonic link
function. GAM solves the model fitting problem for many distributional
outcomes, such as Poisson, binomial, gamma and normal models. In the
same article, @\citet{Hastie1987} proposed the backfitting algorithm
that estimates each smooth function of an additive model iteratively
using partial residuals. The backfitting algorithm is compatible with
almost any smoothing or modeling technique to be used as the
nonparametric components. However, it is unclear how to estimate the
degree of smoothness when necessary, for example for smoothing splines.
The computation cost for cross-validating the smooth parameters can be
expensive. \citet[PP.318-320]{Wood2017} provides more discussion on the
problem.

In many area of modern biomedical research, dealing with
high-dimensional data, also known as \(p >> n\) problem, is common, for
example sequencing data analysis, electric health record data analysis.
Many statistical models have been proposed for such problem, for
example, penalized models and Bayesian hierarchical models. However, up
to this point, most efforts of methodology development focus on modeling
linear effect in high dimensional setting. It is still not clear what is
the best practice on how to model non-linear effects in high-dimensional
setting. Similar to expanding classic models to high-dimensional models,
expanding the GAM framework for high-dimensional applications is not a
trivial task. The difficulties mainly come in two-fold: grouping nature
of the basis functions, and the balance between spline smoothness and
variable sparsity. For GAM models, a smoothing function is consists of
multiple basis functions. The basis functions of a variable is
inherently related. The grouping structure should be properly taken care
of for accurate estimation. Moreover, shrinkage is previously used
solely for sparsity of the model in high dimensional regression models,
or smoothness in GAMs. How to organically combine the two together
remains as a troubling question.

For high-dimensional data analysis, a common Bayesian modeling approach
is to impose a particular family of priors, the spike-and-slab (SS)
priors on the coeffcient in a regression modelss. The term
``spike-and-slab'' was first coined in \citet{Mitchell1988} describing
the shape of the prior. It is a mixture distribution, comprised with a
skinny spike density \(f_{spike}(\cdot)\) for weak signals and a flat
slab density \(f_{slab}(\cdot)\) for large signals, mathematically \[
 \beta_i|\gamma_i \sim (1-\gamma_i)f_{spike}(\beta_i) + \gamma_if_{slab}(\beta_i).
\]

The most distinct feature of the SS priors family is that it is
conditioned on a latent binary variable \(\gamma_i\) that indicates
whether the covariate \(x_i\) is included in the model. There are
various spike-and-slab priors depending on the choice for the spike
density \(f_{spike}(\cdot)\) and the slab density \(f_{slab}(\cdot)\),
for example \citet{George1993}; \citet{George1997}; \citet{Chipman1996}
for grouped variables; \citet{Brown1998} for multivariate outcomes;
\citet{Ishwaran2005}; \citet{Clyde2004} and reference therein. Here we
introduce the most popular spike-and-slab prior introduced in
\citet{George1993}, as stochastic search variable selection (SSVS).
Given a regression problem \[
Y = \boldsymbol{X}\beta+\epsilon, 
\] we can assume the prior distribution of the regression parameters
\(\beta\) depends on a vector of binary variables
\(\gamma=(\gamma_1, \dots \gamma_p)^\prime\) as follows: given
\(\gamma_i\), \(\beta_i\) are independent distributed as a mixture
priors \[
\beta_i | \gamma_i \sim (1-\gamma_i)N(0, \sigma^2 v_0) + \gamma_iN(0,\sigma^2 v_1).
\] The prior of \(\gamma\) is traditionally assumed to be identically
independent Bernoulli distribution. Even the conjugate normal prior of
\(\boldsymbol{\beta}\) provides analytic simplicity, there is no closed
form solution for the posterior density of \(\boldsymbol{\gamma}\).
\citet{George1993} described a Gibbs sampling algorithm to approximate
the posterior distribution of \(\boldsymbol{\gamma}\) for variable
selection. The same authors later compared SSVS with other Bayesian
variable selection approaches, providing some empirical suggestions on
the advantages and disadvantages of various formulations and estimation
algorithms. \citep{George1997} Among the various problem SSVS addresses,
high-dimensionality is not one of them, due to the intensive computation
needs of MCMC algorithms. \citet{George1997} suggested MCMC algorithms
works well for medium size of predictors (p=25), which is not feasible
for high-dimensional data, where the number of predictors easily exceeds
100.

To alleviate the computation burden, many methodology development
studies focus on the maximum a posteriori (MAP) estimate
\citep{Tipping2004}. EM-based algorithms are prevalent used to speed the
computation. \citet{Rockova2014a} proposed an EM-based algorithm for a
spike-and-slab Gaussian mixture prior, EMVS. The coefficient \(\beta_j\)
independently follows a mixture normal distribution
\(N(0, \sigma^2((1-\gamma_i)v_0 + \gamma_i v_1)\) for
\(0 \leq v_0 < v_1\). The parameter \(\sigma^2\) follows a inverse gamma
prior, \(IG(v/2, v\lambda /2)\). Like in the spike-and slab family, the
latent binary variables \(\boldsymbol{\gamma}\) follows a Bernoulli
prior with a hyper prior \(\theta\) follows a beta distribution, or
simpler the uniform (0,1). In the E step, the latent variable are
treated as the missing data, and calculate the posterior mean of
\(\gamma\); in the M step, a ridge estimator was used to update the
coefficients, and \(\sigma^2, \theta\) are updated accordingly. As the
ridge regression creates small but nonzero coefficients, a further step
is needed to select the variable: EMVS further suggests to threshold the
inclusion probability for variable selection or generate regularization
plot with different values of the spike scale parameter. EMVS is also
compatible with structured prior when dealing with grouped variables.
The group structure can be imposed on the prior of the latent binary
variable \(\gamma\) via either logistic regression or markov random
field.

The same authors \citep{Rockova2018b, Rockova2018} further extended the
continuous spike-and-slab Gaussian prior to the spike-and-slab double
exponential distribution, as known as spike-and-slab lasso (SSL). The
authors also lay the theoretical discussion of comparing spike-and-slab
from the penalty perspective and the Bayesian perspective. The SSL also
mitigated the problem of EMVS that the weak signals are not shrink to
zero. Yi and his group incorporated the SSL prior in the EM-IWLS
algorithm and proposed another EM algorithm, EM-cyclic coordinate
descent, based on the Lasso penalty SSL exhibit. Such application were
used in generalized linear models\citep{Tang2017a}, cox models
\citep{Tang2017} and their grouped
counterparts\citep{Tang2018, Tang2019}. Both EM based algorithm provide
deterministic solutions, which becomes a more popular property as
reproducible research becomes more valued. However, while the EMVS are
not able to provide inference of the coefficients easily, the EM-IWLS
can easily extract the variance-covariance matrix due to the IWLS
algorithm. \citet{Bai2020} provides an up-to-date summary of SSL methods
and theoretical discussion.

To the best of our knowledge, there are few GAM models proposed for high
dimensional application. In common practice, model matrix of spline
terms are generated using base functions of preference, and then feed to
a penalized model. Without proper handle of the grouping nature of bases
functions, the estimation of the smooth function is less accurate.
\citet{Bai2020Spline} shows inferior performance of such practice
compared to using an ad hoc high-dimensional additive model.

Among the specialized high-dimensional spline methods,
\citet{Ravikumar2009} extended the original backfitting algorithm
\citep{Hastie1987} to accommodate the high-dimensional setting. The
sparsity and smoothness was handled separately with a soft threshold.
The authors also demonstrated its similarity to Group Lasso.
\citet{Meier2009} proposed an a sparsity-smoothness penalty,

\[
J(f_j) = \lambda_1 \sqrt{||f_j||^2_n + \lambda_2 \int(f^{''}_j(x))^2dx},
\] which can re-formulated as a group lasso problem as well. However,
the model can not estimate the smoothness of the spline functions
correctly, as the smoothness of all spline models are controlled by the
lambda\_2. The inflexible of the model creates problems in prediction.

From the Bayesian perspective, recent works that expands to the
high-dimensional settings includes \citet{Scheipl2012} and
\citet{Bai2020Spline}. \citet{Scheipl2012} proposed an new
spike-and-slab structure prior, normal-mixture-of-inverse gamma for
semi-parametric regression using MCMC. The normal-mixture-of-inverse
gamma prior is mixture of t-distribution with an additional coefficients
mixing vector, where the vector follows a mixture of two Gaussian
distribution. \citet{Bai2020Spline} proposed an novel high dimensional
Bayesian GAM using the spike-and-slab group lasso prior. An fast fitting
EM-based algorithm is proposed. Asymptotic theory, such as posterior
contraction rates and model selection consistency are proved. Even
though, the algorithm is flexible to cover many non-Gaussian family in a
high dimensional setting, the proposed method didn't address uncertainty
quantification, i.e.~calculating credible intervals etc. However, most
of these Bayesian GAMs still suffer a common problem, computation cost.
These Bayesian GAMs uses MCMC to estimate the posterior distribution,
which would create an intense computation load.

In this article, we aim to address the challenges of non-linear signal
modeling in high-dimensional application using Bayesian Hierarchical
model. Specifically, we will propose a new Bayesian spike-and-slab prior
for the generalized additive models and Cox spline models. The
contribution of the proposed prior comes in two-fold: 1) it supports
simultaneous variable selection and function selection; 2) it provides
robust estimation and prediction for the coefficients and outcomes. As
previously described, simultaneous variable selection and its functional
form selection is not a trivial problem. Nevertheless, the Bayesian
spike-and-slab prior is a perfect solution to the problem, as the latent
binary indicator can be used to for variable selection and the spike and
slab densities are used to adaptive adjust the smoothness of the spline
function. Moreover, compared to currently available Bayesian spline
models which mostly conduct a `all-in-all-out' strategy, i.e.~either
including a non-linear form of a variable or excluding the variable
completely, we offer a more flexible solution, rending one of three
possibilities: no effect of a predictor, only linear effect of a
predictor, or non-linear effect. This simplifies the process of
determining what effect a predictor have on the outcome, without
involving hypothesis testing where the degree of freedom for non-linear
effect is complicated to estimate due to the shrinkage. Moreover, out
model inherent the the robust estimation from Bayesian models.

Given the proposed model specification, the estimation of the parameters
can be easily achieved with MCMC algorithm. Nevertheless, considering
the computational burden MCMC algorithms bear especially in
high-dimensional analysis, we propose parsimonious solutions, stemming
from previous EM-IWLS \citep{Rockova2014a}and EM-coordinate descent
algorithms\citep{Tang2018, Tang2019}. The computational friendliness
allows for fitting a model with different sets of initial values,
mimicking the posterior sampling in the MCMC algorithms. We are hopeful
that this could improve the sacrificed robustness of the estimation
process due to the MAP estimation used by the EM algorithms in
comparison to the model averaging approach.

The third aim of the dissertation work will focus on the software
implementation of the proposed models and algorithms. We will implement
the proposed models and algorithms in one of the most commonly used
statistical computing environment R. Ancillary functions will be
provided in assistance to model diagnostic, interpretation and graphing.
The R implementation of the Bayesian Hierarchical additive model is
available via a freely available R package, BHAM at
(\url{https://github.com/boyiguo1/BHAM}){[}\url{https://github.com/boyiguo1/BHAM}{]}.

The rest of the paper are arranged as the followings: in Chapter 32 we
demonstrate the Bayesian Hierarchical generalized additive model, where
we provide the setup of the proposed model and two fast-fitting
algorithms. In Chapter 3, we Application to real-world datasets
demonstrates the usefulness of the proposed methodologies in Chpater 4.
Conclusion and discussions are made in Chapter 5.

\hypertarget{spike-and-slab-additive-model}{%
\section{Spike-and-slab Additive
Model}\label{spike-and-slab-additive-model}}

Following the notation in the previous section \ref{Notation}, suppose
we have the model matrix \(\boldsymbol{X}\) derived from spline
functions of choice \(h_j, j = 1, \dots, p.\) for the \(p\) covariates
that are modeled nonparametrically, the total dimension of
\(\boldsymbol{X}\) is \(n \times \sum\limits_{j=1}^p k_j\). Column-wise
centering and re-parameterization introduced in the GAM as random effect
section{[}\^{}5{]} are performed. The re-parameterization has two
purposes. First of all the re-parameterization reshapes the columns to
be linearly independent to each other, and hence, independent prior can
be set to the coefficients. Secondly, the re-parameterization decomposes
the model matrix \(\boldsymbol{X}_j\) for the \(j\)th covariates to two
sub-matrices representing the null space of the spline and penalized
space of the spline, \[
\boldsymbol{X}_j = \begin{bmatrix}
\boldsymbol{X}_j^{0} & \boldsymbol{X}_j^{pen}
\end{bmatrix},
\] where \(\boldsymbol{X}_j^{null}\) is the column space representing
the null space of the spline model, and \(\boldsymbol{X}_j^{pen}\)
denotes the column spaces that need to be shrunk for smoothness of the
spline. The purpose for this re-parameterization is to isolate the null
space from the smoothing space, such that answering the question, if
non-linear relationship is necessary, becomes easy by deciding if the
corresponding coefficients of the \(\boldsymbol{X}_j^{pen}\) are jointly
zero. This strategy is widely used, previously mentioned in
\citet{Scheipl2012} and the references there in.\footnote{Possibly
  select some to add here.} Hereafter, we denote the coefficients of the
null space as \(\boldsymbol{\beta}^{0}_j\) and the coefficients of the
penalized space as \(\boldsymbol{\beta}^{pen}_j\). The spline function
for the \(j\)the covariate is \[
f_j(X) = f_j^0(X) + f_j^{pen}(X) = {\beta_j^0}^T X_j^0 + {\beta_j^{pen}}^T X_j^{pen},
\] where the superscript \(^t\) denotes matrix transpose.

Without loss of generality, we assume that the model doesn't include the
the parametric part of the model \(\boldsymbol{A}\). The coefficients of
\(\boldsymbol{A}\) can be estimated easily in the proposed EM algorithms
by local update conditioning on the temporary value of the spline
coefficients.

~

\hypertarget{spike-and-slab-spline-prior}{%
\subsection{Spike and slab spline
prior}\label{spike-and-slab-spline-prior}}

A generalized spline model can be formulated as \[
y_i \sim EF(\mu_i, \phi), \quad g(\mu_i) = \sum\limits_{j=1}^p f_j(x_{ij}), \quad i = 1, \dots, n.
\] The link function of the mean can be expressed in a matrix form \[
g(\mu_i) = \sum\limits_{j=1}^p\left[f_j^0(x_{ij}) + f_j^{pen}(x_{ij})\right] = \sum\limits_{j=1}^p\left[{\beta_j^0}^T X_{ij}^0 + {\beta_j^{pen}}^T X_{ij}^{pen}\right]
 = \beta^T X_i,
\] where \[
\begin{aligned}
  X_i = \begin{bmatrix}X^0_{i1}\\X^{pen}_{i1}\\\vdots \\X^0_{ip}\\X^{pen}_{ip}\end{bmatrix}
& &
  \beta = \begin{bmatrix}\beta^0_1\\\beta^{pen}_{1}\\\vdots \\\beta^0_{p}\\\beta^{pen}_{p}\end{bmatrix}
\end{aligned}
\]

\hfill\break

\hypertarget{mixture-normal-prior}{%
\subsubsection{Mixture-Normal Prior}\label{mixture-normal-prior}}

Given the full deign matrix
\(X = \begin{pmatrix} X_1 & X_2 & \dots & X_n \end{pmatrix}^T\), the
likelihood function of the generalized model can be written as \[
p(y|X\beta, \phi) = \prod\limits^n_{i=1}p(y_i|\beta^T X_i, \phi).
\]

Considering the dependency of spline bases, the grouping nature of the
bases should be taken care of. Naturally, the Bayesian hierarchical
models allow us easily to take account of the dependency of the spline
bases as grouping information and provide more reliable estimation. Our
hierarchical GAMs specify the spike-and-slab mixture normal prior on the
coefficients: for the coefficients of the \(j\)th predictor,
\(\beta^0_j\) and \(\beta^{pen}_{jk}\), we have \[
\begin{aligned}
  \beta^0_{j} |\gamma^0_{j},s_0,s_1 &\sim N(0,(1-\gamma^0_{j}) s_0 + \gamma^0_{j} s_1)\\
  \beta^{pen}_{jk} | \gamma^{pen}_{j},s_0,s_1 &\sim N(0,(1-\gamma^{pen}_{j}) s_0 + \gamma^{pen}_{j} s_1), 
\end{aligned}
\] where \(\gamma^0_{j}\in\{0,1\}\) and
\(\gamma^{pen}_{j}\in \{0,1\}^{K_j}\) are latent indicator variables,
indicating if the model includes the linear effect and non-linear effect
of the \(j\)th variable respectively; \(s_0\) and \(s_1\) are scale
parameters, assuming \(0 < s_0 < s_1\) and given. Thus, the priors are
formulated as a mixture of the shrinkage prior normal(0, \(s_0\)) and
the weakly informative prior normal(0, \(s_1\)), which are spike and
slab components respectively. Like any other spike and slab priors, the
spike density is to contain the minimum to zero effects, while the slab
density is to allow large effects. The scale parameters \(s_0\) and
\(s_1\) can be treated as tuning parameters, which can be optimized via
cross-validation. A discussion of how to choose the scale parameters
comes later.

To note here, unlike the original spike-and-slab prior where each
predictor have only indicator variable, \(\gamma_j\), we have two
indicators associated with each predictor, \(\gamma^0_j\) and
\(\gamma^{pen}_j\) to decide the inclusion of the linear effect and
non-linear effect of the predictor respectively. It is possible to add
more constraints on the model, assuming that one indicator variable
decides the inclusion of both the linear effect and non-linear effect,
i.e.~\(\gamma_j = \gamma^0_j = \gamma^{pen}_j\). Such formulations,
commonly referred as ``all-in-all-out'', has been implemented in many
high dimensional Bayesian GAMs, see Yang \& Narisetty (2020) and Bai
(Work in progress). The ``all-in-all-out'' set-up was criticized for its
inability to answering an important research question, if the effects
are non-linear or not. On the other hand, the model can be much more
flexible by allowing each dimension of the penalized space to have an
independent indicator, i.e.~\(\gamma^{pen}_{jk}\) of
\(\beta^{pen}_{jk}\) for \(k = 1\dots, K_j\). This prior set-up
generalized to the group spike-and-slab. The additional flexibility
doesn't not necessarily grand the model neither theoretical
justification nor analytic simplicity. The spline penalty assumes a
common smooth shrinkage \(\lambda_j\) applied to all coefficients of the
penalized space, which contradicts to the model setup. Besides, having
multiple indicator variables complicates the decision of if non-linear
effects are necessary.

We set up the hyper-prior of \(\boldsymbol{\gamma}\) to allow local
adaption of the shrinkage, using a binomial distribution. The two
indicators of the \(j\)th predictor, \(\gamma^{0}_j\) and
\(\gamma^{pen}_j\), shares the same probability parameter, \[
\begin{aligned}
\gamma_{j}^{0} | \theta_j &\sim Bin(\gamma^{0}_{j}|1, \theta_j)\\
\gamma_{j}^{pen} | \theta_j &\sim Bin(\gamma^{pen}_{j}|1, \theta_j).
\end{aligned}
\]

This is to leverage the fact that the probability of selecting the bases
of a smooth function should be similar, while allowing different penalty
on the null space and penalty space of the spline function. The hyper
prior of \(\gamma_{j}^{0}\) decides the sparsity of the model at the
function selection level, while that of \(\gamma_{j}^{pen}\) decides the
smoothness of the spline function at individual predictor level.
Meanwhile, we specify that \(\gamma_{j}^0\) and \(\gamma_{j}^{pen}\) are
independently distributed for analytic simplicity. However, this set-up
could possibility result in a situation that is not theoretically
possible: the non-linear component is selected, but the linear component
is not. This can be addressed analytically by forcing to include the
linear component when non-linear component is included. Another possible
solution is to impose an dependent structure of \(\gamma_{j}^{pen}\) on
\(\gamma_{j^{0}}\), i.e.~\(\gamma_j^{pen}|\gamma_{j}^{0}, \theta_j\).

We further specify the parameter \(\theta_j\) follows a beta
distribution with given shape parameters \(a\) and \(b\), \[
\theta_j \sim Beta(a, b).
\] The beta distribution is a conjugate prior for the binomial
distribution and hence provides some computation convenience.
Specifically, we focus on a special case of beta distribution, uniform
(0,1) for simplicity and convenience. To note, when the variable have
large effects in any of the bases, the parameter \(\theta_j\) will be
estimated large, which in turn encourages the model to include the rest
of bases, achieving the local adaption among spline bases. Hereafter, we
refer the hierarchical spline GAM with the spike-and-slab mixture normal
prior as the ssGAM-MN.

\includegraphics[width=24.76in]{Fig/SS_prior_flow_chart}

~

\hypertarget{mixture-double-exponential-prior}{%
\subsubsection{Mixture Double Exponential
Prior}\label{mixture-double-exponential-prior}}

One of the critics received by the spike-and-slab mixture normal prior
is that the tails of a normal distribution diminishes to zero too fast,
which renders problem when estimating the large effects. In contrast,
distributions with heavier tails are preferred, for example \(t\)
distribution with small degree of freedom and double exponential
distribution. Specifically, double exponential prior is equivalent to
the LASSO penalty in the frequentist models, providing a sparse
solution. Meanwhile, the estimation algorithm of LASSO penalty are
extremely efficient, greatly reducing the computation needs. Recent
developments of spike-and-slab priors include spike-and-slab LASSO
prior, i.e.~spike-and-slab mixture double exponential prior. We extend
the spike-and-slab LASSO prior to high-dimensional GAMs, providing
computationally efficiency and sparse solution to our proposed ssGAM-MN.

The set-up of ssGAM-LASSO model is similar to ssGAM-MN, except we
replace the normal distribution with double exponential distribution.
The prior distributions of the coefficients \(\beta^0_j, \beta^{pen}_j\)
for the \(j\) predictor can be formulated as \[
\begin{aligned}
  \beta^0_{j} |\gamma^0_{j},s_0,s_1 &\sim DE(0,(1-\gamma^0_{j}) s_0 + \gamma^0_{j} s_1)\\
  \beta^{pen}_{jk} | \gamma^{pen}_{j},s_0,s_1 &\sim DE(0,(1-\gamma^{pen}_{j}) s_0 + \gamma^{pen}_{j} s_1).
\end{aligned}
\] The hyper-priors for \(\gamma^0_j, \gamma^{pen}_j, \theta_j\) are
kept the same as specified in ssGAM-MN.

\hfill\break

\hypertarget{alforithm-for-fitting-ssgams}{%
\subsection{Alforithm for fitting
ssGAMs}\label{alforithm-for-fitting-ssgams}}

Parsimonious computation is always encouraged in high-dimension data
analysis. Normally, Bayesian methodology loses its advantages over
Frequentest penalized model because of the computation cost. Previous
Bayesian spline models heavily relies on the MCMC algorithm to establish
posterior distribution of parameters. One of the exception is Bai (in
progress), who a grouped spike-and-slab LASSO prior \citep{Rockova2018}
to address the grouping structure of spline bases. He focused on the MAP
estimator of the parameters, but failed to address the uncertainty
measure of the estimates. The main difference between Bai's work and the
proposed models are two-folded: 1) the proposed models are feasible to
answer the question if the non-linear effects are necessary for the
predictors, while Bai's work can't due to the ``all-in-all-out''
approach; 2) the proposed models can provide uncertainly measures in
addition to the point estimates of the coefficients, while Bai's work
can't not due to the model fitting algorithm.

In the follow section, we introduced two expectation-maximization (EM)
based algorithms for fast computing, EM-IRLS and EM-coordinate Descent
algorithm. Instead of simulating the posterior distribution via MCMC, we
iteratively estimate the MAP. Compared to the EM-IRLS algorithm, the
EM-coordinate descent provides an expedited model fitting process at the
cost of model uncertainty.

\hfill\break

\hypertarget{em-algorithms}{%
\subsubsection{EM algorithms}\label{em-algorithms}}

EM algorithm is an iterative algorithm to find the maximum likelihood
estimates or the Bayesian counterpart MAP estimates. It is commonly used
when some necessary data to establish the likelihood function are
missing. Instead, the algorithm maximizes the expectation, in respect to
the ``missing'' data, of the likelihood function.

The recursive algorithm consists of two steps:

\begin{itemize}
\tightlist
\item
  E-step: to calculate the expectation of the likelihood function
  conditioning on some ``missing'' data
\item
  M-step: to maximize the spectated likelihood function to calculate the
  parameter of interest
\end{itemize}

For ssGAM, we define the parameters of interest as
\(\Theta = {\boldsymbol{\beta}, \boldsymbol{\theta}, \phi}\) and
consider the latent binary indicators \(\boldsymbol{\gamma}\) as
nuisance parameters of the model. Our objective is to find the
parameters \(\Theta\) that maximize the posterior density function,or
equivalently, the logarithm of the density function, \[
\begin{aligned}
& \text{argmax}_{\Theta}
\log p(\Theta, \boldsymbol{\gamma}| \textbf{y}, \textbf{X}) \\
&= \log p(\textbf{y}|\boldsymbol{\beta}, \phi) + \log p(\phi) + \sum\limits_{j=1}^p\left[\log p(\beta^0_j|\gamma^0_j)+\sum\limits_{k=1}^{K_j} \log p(\beta^{pen}_{jk}|\gamma^{pen}_{jk})\right]\\
& +\sum\limits_{j=1}^{p} \left[ (\gamma^0_j+\gamma_{j}^{pen})\log \theta_j + (2-\gamma^0_j-\gamma_{j}^{pen}) \log (1-\theta_j)\right] +  \sum\limits_{j=1}^{p}\log p(\theta_j)
\end{aligned}
\]\\

We use the EM algorithm to find the MAP estimate of \(\Theta\). Since
the latent binary indicators \(\boldsymbol{\gamma}\) are not of our
primary interest, we treat them as as the ``missing data'' in the EM
algorithms. Naturally In the E-step, we calculate the expectation of
posterior density function of
\(\log p(\Theta, \boldsymbol{\gamma}| \textbf{y}, \textbf{X})\) with
respect to the latent indicators \(\boldsymbol{\gamma}\) conditioning on
the values from previous iteration \(\Theta^{t-1}\), \[
E_{\boldsymbol{\gamma}|\Theta^{t-1}}\log p(\Theta, \boldsymbol{\gamma}| \textbf{y}, \textbf{X}) .
\] Hereafter, we use the short hand notation
\(E(\cdot)\equiv E_{\boldsymbol{\gamma}|\Theta^{t-1}}(\cdot)\). In the
M-step, we find the \(\Theta^{t}\) that maximize
\(E_{\boldsymbol{\gamma}|\Theta^{t-1}}\log p(\Theta, \boldsymbol{\gamma}| \textbf{y}, \textbf{X})\).
To note here, the log-posterior of ssGAM (up to additive constants) can
be written as a two-part equation

\[ \log p(\Theta, \boldsymbol{\gamma}| \textbf{y}, \textbf{X}) = Q_1(\beta, \phi) + Q_2 (\gamma,\theta),\]
Where
\[ Q_1(\boldsymbol{\beta}, \phi) = \log p(\textbf{y}|\boldsymbol{\beta}, \phi) + \log p(\phi) + \sum\limits_{j=1}^p\left[\log p(\beta^0_j|\gamma^0_j)+\sum\limits_{k=1}^{K_j} \log p(\beta^{pen}_{jk}|\gamma^{pen}_{jk})\right]\]
and \[
Q_2(\boldsymbol{\gamma},\boldsymbol{\theta}) = \sum\limits_{j=1}^{p} \left[ (\gamma^0_j+\gamma_{j}^{pen})\log \theta_j + (2-\gamma^0_j-\gamma_{j}^{pen}) \log (1-\theta_j)\right] +  \sum\limits_{j=1}^{p}\log p(\theta_j) .\]
\(Q_1\) and \(Q_2\) are respectively, the log-likelihood of model
coefficients and the dispersion parameter \(\boldsymbol{\beta}, \phi\),
and latent indicators \(\boldsymbol{\gamma}, \theta\). Conditioning on
\(\gamma_{jk}\), \(Q_1\) and \(Q_2\) are independent and can be
maximized separately for \(\boldsymbol{\beta}, \phi\) and
\(\boldsymbol{\theta}\).

The E- and M- steps are iterated until the algorithm converge. Depending
on the choice of the mixture priors, either mixture normal or mixture
double exponential, the maximization in the M-step can take different
algorithms, IRLS for uncertainty measures and coordinate descent for
faster computation.

\hfill\break

\hypertarget{em-irls}{%
\subsubsection{EM-IRLS}\label{em-irls}}

The main idea of the EM-IRLS algorithm is to use the iterative
re-weighted least square algorithm to find the estimate
\(\boldsymbol{\beta}, \phi\) that maximizes \(E(Q_1)\). Recall the prior
set-up of mixture normal prior, we have the prior densities of
\(\boldsymbol{\beta}\) conditioning on \(\boldsymbol{\gamma}\)

\[
\begin{aligned}
p(\boldsymbol{\beta }| \boldsymbol{\gamma}) &\propto \prod\limits_{j=1}^{p}\left[{S^0_j}^{-1/2}\exp(-1/2({\beta^0_{j}}^2/S^0_{j}))\prod\limits_{k=1}^{K_j}S_{j}^{-1/2}\exp(-1/2(\beta_{jk}^2/S_{j}))\right],%\\
% p(\bs\gamma | \bs \theta) & = \prod\limits_{j=1}^{p}\prod\limits_{k=1}^{K_j} \theta_j^{\gamma_{jk}} (1-\theta_j)^{1-\gamma_{jk}}.
\end{aligned}
\] and \begin{align*}
E(Q_1) &= \log p(\textbf{y}|\boldsymbol{\beta}, \phi) + \log p(\phi) + \sum\limits_{j=1}^{p} \left[E({S^0_j}^{-1}){\beta^0_j}^2+\sum\limits_{k=1}^{K_j}E(S^{-1}_{j})\beta_{jk}^2\right]\\
E(Q_2) &= \sum\limits_{j=1}^{p}\sum\limits_{k=1}^{K_j} \left[ E(\gamma_{jk})\log \theta_j + (1-E(\gamma_{jk})) \log (1-\theta_j)\right] +  \sum\limits_{j=1}^{p}\log p(\theta_j)
\end{align*} where
\(S_{j}^0 = (1-\gamma^{0}_{j}) s_0 + \gamma^{0}_{j} s_1\) and
\(S_{j} = (1-\gamma^{pen}_{j}) s_0 + \gamma^{pen}_{j} s_1\). To
calculate two unknown quantities \(E({S_j^0}^{-1})\) and
\(E(S^{-1}_j)\), the posterior probability
\(p^0_{j} \equiv p(\gamma^{0}_{j}=1|\Theta^{t-1})\) and
\(p_{j} \equiv p(\gamma^{pen}_{j}=1|\Theta^{t-1})\) are necessary, which
can be derived via Bayes' theorem. The calculation for \(p_j\) is
slightly different from that of \(p^0_j\), as \(p_j\) depends on the
value of \(\beta_{jk}\) for \(k=1, \dots, K_j\) and \(p^0_j\) only
depends on \(\beta_j^0\). The calculation follows the equations below.

\[
\begin{aligned}
p_{j}^0 &= \frac{Pr(\gamma_{j}^0 = 1|\theta_j)f(\beta_{j}^0|\gamma_{j}^0=1, s_1) }{Pr(\gamma_{j}^0 = 1|\theta_j)f(\beta_{j}^0|\gamma_{j}^0=1, s_1) + Pr(\gamma_{j}^0 = 0|\theta_j)f(\beta^0_{j}|\gamma^0_{j}=0, s_0)}\\
p_{j} &= \frac{Pr(\gamma^{pen}_{j} = 1|\theta_j)\prod\limits_{k=1}^{K_j}f(\beta_{jk}|\gamma^{pen}_{j}=1, s_1) }{Pr(\gamma^{pen}_{j} = 1|\theta_j)\prod\limits_{k=1}^{K_j}f(\beta_{jk}|\gamma^{pen}_{j}=1, s_1) + Pr(\gamma^{pen}_{j} = 0|\theta_j)\prod\limits_{k=1}^{K_j}f(\beta_{jk}|\gamma^{pen}_{j}=0, s_0)}
\end{aligned}
\]

and
\(Pr(\gamma_{j}^{0} = 1|\theta_j) =Pr(\gamma_{j}^{pen} = 1|\theta_j) = \theta_j\),
\(Pr(\gamma_{j}^{0} = 0|\theta_j) = Pr(\gamma_{j}^{pen} = 0|\theta_j) = 1-\theta_j\),
\(f(\beta|\gamma=1, s_1) = \text{Normal}(\beta|0 , s_1)\),
\(f(\beta|\gamma=0, s_0) = \text{Normal}(\beta|0 , s_0)\). It is trivial
to show \[
\begin{aligned}
&E(\gamma^0_{j})  = p^0_{j} & &E(\gamma^{pen}_{j})  = p_{j}\\
&E({S^0}^{-1}_{j}) = \frac{1-p^{0}_{j}}{s_0} + \frac{p^{0}_{j}}{s_1} & &E(S^{-1}_{j}) = \frac{1-p_{j}}{s_0} + \frac{p_{j}}{s_1}.
\end{aligned}
\]

With \(E({S^0}^{-1}_{j})\) and \(E(S^{-1}_{j})\) known, \(E(Q_1)\) can
be also viewed as a \(l_2\) penalized likelihood function:
\(E({S^0}^{-1}_{j})\) controls the shrinkage of the linear effect,
informing if the predictor should be included in the model;
\(E(S^{-1}_{j})\) controls the smoothness of the spline function.
\(l_2\) penalized likelihood function can be maximize using the IWLS
algorithm. The likelihood function can be approximate by a weighted
normal likelihood: \[
  p(\textbf{y}|\boldsymbol{\beta}, \phi) \approx Normal(\textbf{z}|X\beta, \phi\Sigma)
\] where the `normal response' \(z_i\) and `weight' \(w_i\) are called
the pseudo-response and pseudo-weight, respectively.The pseudo data and
the pseudo-weight are calculated by \[
    \begin{aligned}
    z_i &= \hat\eta_i - \frac{L^{'}((y_i|\hat\eta_i))}{L^{''}((y_i|\hat\eta_i))}& w_i &= - L^{''}((y_i|\hat\eta_i)),
    \end{aligned}
    \] where \(\eta_i = \sum\limits_{j=1}^p \hat f_j^{(t)}(x_{ij})\) is
the estimated linear predictor using current estimated value of
\(\boldsymbol{\beta}\), \(L^{'}\) and \(L^{''}\) are the first
derivative and second derivative of the log-likelihood function. The
\(l_2\) penalty,
\(\sum\limits_{j=1}^{p}\left[E({S^0_j}^{-1}){\beta^0_j}^2+\sum\limits_{k=1}^{K_j}E(S^{-1}_{j})\beta_{jk}^2\right]\)
can be seems as a multivariate normal distribution of mean
\(\textbf{0}\) and variance-covariance matrix
\(\Sigma(E({S^0}_1^{-1}), E(S_1^{-1}), \dots, E({S_p^0}^{-1}), E(S_p^{-1})\),
conditioning on the value of \(E({S^0_1}^{-1}), \dots E({S^0_p}^{-1})\).
An augmentation step combines the normal approximation and multivariate
normal for \(\boldsymbol{\beta}\), similarly to Ridge regression model
fitting. The estimates of \(\boldsymbol{\beta}\) and \(\phi\) can be
quickly updated in the IRLS algorithm.

The remaining parameters of interest \(\boldsymbol{\theta}\) can be
updated by maximizing \(E(Q_2)\). As the beta distribution is a
conjugate prior for Bernoulli distribution, \(\boldsymbol{\theta}\) can
be easily updated with a closed form equation: \[
\theta_j = \frac{p^0_j + p_{j} + a - 1 }{a + b}.
\]

Totally, the framework of the proposed EM IRLS algorithm was summarized
as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Choose a starting value \(\boldsymbol{\beta}^{(0)}\) and
  \(\boldsymbol{\theta}^{(0)}\) for \(\boldsymbol{\beta}\) and
  \(\boldsymbol{\theta}\). For example, we can initialize
  \(\boldsymbol{\beta}^{(0)} = \boldsymbol{0}\) and
  \(\boldsymbol{\theta}^{(0)} = \boldsymbol{0}.5\)
\item
  Iterate over the E-step and M-step until convergence
\end{enumerate}

E-step: calculate \(E(\gamma^0_{j})\), \(E(\gamma^{pen}_{j})\)and
\(E({S^0}^{-1}_{j})\) , \(E({S}^{-1}_{j})\)with estimates of
\(\Theta^{(t-1)}\) from previous iteration

M-step:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  Update \(\boldsymbol{\beta}^{(t)}\) using the IRLS algorithm
\item
  Update \(\boldsymbol{\theta}^{(t)}\) using the closed form equation
\end{enumerate}

We assess convergence by the criterion:
\(|d^{(t)}-d^{(t-1)}|/(0.1+|d^{(t)}|)<\epsilon\), where
\(d^{(t)} = -2l(\beta^{(t)},\phi^{(t)})\) is the estimate of deviance at
the \(t\)th iteration, and \(\epsilon\) is a small value (say
\(10^{-5}\)).

\hfill\break

\hypertarget{em-coordinate-descent}{%
\subsubsection{EM-Coordinate Descent}\label{em-coordinate-descent}}

When the prior distribution is set to mixture double exponential,
coordinate descent algorithm can be used to estimate
\(\boldsymbol{b}eta\), replacing IRLS, in the M-step. Coordinate descent
is an optimization algorithm that offers extreme computational
advantage, and famous for its application in optimizing the \(l_1\)
penalized likelihood function.

Recall, the density function of spike-and-slab mixture double
exponential prior can be written as \[
f(\beta|\gamma, s_0, s_1) = \frac{1}{(1-\gamma)s_0 + \gamma s_1}\exp(-\frac{|\beta|}{(1-\gamma)s_0 + \gamma s_1}).
\]

This can simplify our posterior likelihood function to \[
E(Q_1) = \log p(\textbf{y}|\boldsymbol{\beta}, \phi) + \log p(\phi) + \sum\limits_{j=1}^p\left[E({S^0_j}^{-1})|\beta^0_j|+\sum\limits_{k=1}^{K_j}E(S^{-1}_{j})|\beta_{jk}|\right]
,
\] where the approximation of \(E({S^0_j}^{-1}\) and \(E(S^{-1}_{j})\)
remains the same as in the EM-IRLS algorithm. The formulation of
\(E(Q_1)\) can be seen as a \(l_1\) penalized likelihood function, and
optimized via coordinate descent algorithm. The estimates can be used to
further estimates \(\boldsymbol{\theta}\) in the M-step, following the
same equation in the EM-IRLS.

Totally, the framework of the proposed EM-coordinate descent algorithm
was summarized as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Choose a starting value \(\boldsymbol{\beta}^{(0)}\) and
  \(\boldsymbol{\theta}^{(0)}\) for \(\boldsymbol{\beta}\) and
  \(\boldsymbol{\theta}\). For example, we can initialize
  \(\boldsymbol{\beta}^{(0)} = \boldsymbol{0}\) and
  \(\boldsymbol{\theta}^{(0)} = \boldsymbol{0}.5\)
\item
  Iterate over the E-step and M-step until convergence
\end{enumerate}

E-step: calculate \(E(\gamma^0_{j})\), \(E(\gamma^{pen}_{j})\)and
\(E({S^0}^{-1}_{j})\) , \(E({S}^{-1}_{j})\)with estimates of
\(\Theta^{(t-1)}\) from previous iteration

M-step:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  Update \(\boldsymbol{\beta}^{(t)}\) using the coordinate descent
  algorithm
\item
  Update \(\boldsymbol{\theta}^{(t)}\) using the closed form equation
\end{enumerate}

We assess convergence by the criterion:
\(|d^{(t)}-d^{(t-1)}|/(0.1+|d^{(t)}|)<\epsilon\), where
\(d^{(t)} = -2l(\beta^{(t)},\phi^{(t)})\) is the estimate of deviance at
the \(t\)th iteration, and \(\epsilon\) is a small value (say
\(10^{-5}\)).

~

\hypertarget{selecting-optimal-scale-values}{%
\subsection{Selecting Optimal Scale
Values}\label{selecting-optimal-scale-values}}

Our proposed prior, spike and slab spline prior, requires two preset
scale parameters (\(s_0\), \(s_1\)). Hence, we would construct a two
dimensional grid, consists of different pairs of (\(s_0\), \(s_1\))
value. However, previous research on the topic\footnote{TODO: add
  citation} suggested the value of slab scale \(s_1\) have less impact
on the final model and is recommended to be set as a generally large
value of \(s_1 = 1\) that provides no or weak shrinkage. More attention
is focused on examining different values of spike scale \(s_0\). Instead
of the 2-D grid, We consider a sequence of \(L\) decreasing values
\(\{s_0^l\}: 0 < s_0^1 < s_0^2 < \dots < s_0^L < s_1\). Increasing the
spike scale s\_0 tends to include more non-zero coefficients in the
model. An measure of preference, e.g.~area under the curve (AUC), mean
squared error, can be used to facilitate the selection of a final model.
The procedure is similar to the lasso implemented in the widely used R
package \texttt{glmnet}, which quickly fits the lasso model over a list
of values of \(\lambda\) covering the entire range, giving a sequence of
models for users to choose from.

\hypertarget{simulation}{%
\section{Simulation}\label{simulation}}

\hypertarget{real-data-analysis}{%
\section{Real Data Analysis}\label{real-data-analysis}}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\begin{figure}
\centering
\includegraphics{main_files/figure-latex/plot-ref-1.pdf}
\caption{Fancy Caption\label{fig:plot}}
\end{figure}

\ldots and can be referenced (Figure \ref{fig:plot}) by including the
\texttt{\textbackslash{}\textbackslash{}label\{\}} tag in the
\texttt{fig.cap} attribute of the R chunk:
\texttt{fig.cap\ =\ "Fancy\ Caption\textbackslash{}\textbackslash{}label\{fig:plot\}"}.
It is a quirky hack at the moment, see
\href{https://github.com/yihui/knitr/issues/323}{here}.

Analogously, use Rmarkdown to produce tables as usual:

\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
 & speed & dist \\ 
  \hline
1 & 4.00 & 2.00 \\ 
  2 & 4.00 & 10.00 \\ 
  3 & 7.00 & 4.00 \\ 
  4 & 7.00 & 22.00 \\ 
  5 & 8.00 & 16.00 \\ 
  6 & 9.00 & 10.00 \\ 
   \hline
\end{tabular}
\caption{A table} 
\label{tab:table}
\end{table}

\hypertarget{cross-referencing}{%
\subsection{Cross-referencing}\label{cross-referencing}}

The use of the Rmarkdown equivalent of the \LaTeX cross-reference system
for figures, tables, equations, etc., is encouraged (using
\texttt{{[}@\textless{}name\textgreater{}{]}}, equivalent of
\texttt{\textbackslash{}ref\{\textless{}name\textgreater{}\}} and
\texttt{\textbackslash{}label\{\textless{}name\textgreater{}\}}). That
works well for citations in Rmarkdown, not so well for figures and
tables. In that case, it is possible to revert to standard
\LaTeX syntax.

\hypertarget{references}{%
\section{References}\label{references}}

\bibliography{bibfile.bib}


\end{document}
