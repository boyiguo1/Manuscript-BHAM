# Introduction

Using additive models to interpolate nonlinear effects has a long history. Early work of additive models can date back to 1979 when \citet{Cleveland1979} proposed local smoothing for bivariate analysis. Since then, many mathematical and statistical models have been proposed, see @Wood2020. With the recent advancement of computational technology, many machine learning algorithms that are computationally intensive start to gain much popularity in prognostic modeling and functional selection, especially when complex signals, for example nonlinear effects, are assumed. Such models generally have improved prediction accuracy over classic statistical models. However, these models have a fatal flaw, the lack of transparency and interpretability, and hence are referred to as "black box" models. Given that the primary objective of biomedical studies is to understand the etiology of diseases, "Black box" models are less helpful as they can hardly explain how the change of biomarkers is associated with the disease.

Semi-parametric regression models are a more appropriate alternative to the "black-box" models, thanks to their balancing between model flexibility and interpretability. Among those, generalized additive models (GAMs), proposed in the seminal work of @Hastie1987, grew to be one of the most utilized modeling tools. In a GAM, the response variable, $Y$, which is assumed to follow some exponential family distribution $EF(\mu. \phi)$, can be expressed as the summation of smooth functions, $f_j, j = 1, \dots, p$, of a given $p$-dimensional vector of covariates $\bs x$, written as 
$$
E(Y|\bs x) = g^{-1}(a + \sum\limits^p_{j=1}f_j(x_j)),
$$
where $g^{-1}$ is the inverse function of a smooth monotonic link function. The smooth functions can take many forms and are estimated using a pseudo-weighted version of the backfitting algorithm [@Breiman1985]. Nevertheless, the classic GAMs cannot fulfill the increasing analytic demands for high-dimensional data analysis in biomedical studies.

Many applications of modern biomedical research, e.g. sequencing data analysis, electric health record data analysis, require special treatment of high-dimensionality, commonly known as $p >> n$ problem. There is extensive literature on high-dimensional linear models via penalized models or Bayesian hierarchical models, see @Mallick2013. These models assume sparsity among the predictors, i.e. most predictors have small or no effects on the outcome. Further development focuses on addressing the grouping structure among the predictors, where AMs and GAMs can be considered special cases. Various penalties are imposed on the basis function coefficients: @Ravikumar2009 extended the grouped lasso [@Yuan2006] to additive models; @Huang2010 further developed adaptive grouped lasso for additive models; @Wang2007 and @Xue2009 applied grouped SCAD penalty [@Fan2001] to the additive models. Recent Bayesian hierarchical models are also used in the context of high-dimensional additive models. Various group spike-and-slab priors combining with computationally intensive MCMC algorithms [@Xu2015; @Yang2020] are proposed where the application on AMs are by-products. @Bai2020 was the first to apply group spike-and-slab lasso prior to AMs using a fast optimization algorithm, and further generalized the framework to GAMs in @Bai2021. Regardless of the nice statistical properties, a practical and pivotal question, if non-linear effects exist, yet to be addressed by these methods. In addition, a common criticism received by these methods is that the basis function coefficients tend to be overly penalized due to the sparsity assumption directly applied on the basis function coefficients. [@Scheipl2013] 

To address this problem.



where majority of the efforts ware focusing on the  

[TODO a sentence of sparsity. What it means in a penalized model and Bayesian model] Directly apply to additive models, spline models specifically. Problem, oversmoothing of the spline function. An more appropriate treatment would be incorporating the smooth penalty of spline functions.[TODO: a sentence about smoothness penalty]


What is the smooth penalty of spline function?




Many statistical models have been proposed for such problem, for example, penalized models and Bayesian hierarchical models. However, up to this point, most efforts of methodology development focus on modeling linear effect in high dimensional setting. It is still not clear what is the best practice on how to model non-linear effects in high-dimensional setting. 



Similar to expanding classic models to high-dimensional models, expanding the GAM framework for high-dimensional applications is not a trivial task. The difficulties mainly come in two-fold: grouping nature of the basis functions, and the balance between spline smoothness and variable sparsity. For GAM models, a smoothing function is consists of multiple basis functions. The basis functions of a variable is inherently related. The grouping structure should be properly taken care of for accurate estimation. Moreover, shrinkage is previously used solely for sparsity of the model in high dimensional regression models, or smoothness in GAMs. How to organically combine the two together remains as a troubling question. 

To the best of our knowledge, there are few GAM models proposed for high dimensional application. In common practice, model matrix of spline terms are generated using base functions of preference, and then feed to a penalized model. Without proper handle of the grouping nature of bases functions, the estimation of the smooth function is less accurate. @Bai2020Spline shows inferior performance of such practice compared to using an ad hoc high-dimensional additive model. Among the specialized high-dimensional spline methods, @Ravikumar2009 extended the original backfitting algorithm [@Hastie1987] to accommodate the high-dimensional setting. The sparsity and smoothness was handled separately with a soft threshold. The authors also demonstrated its similarity to Group Lasso. @Meier2009 proposed an a sparsity-smoothness penalty,

$$
J(f_j) = \lambda_1 \sqrt{||f_j||^2_n + \lambda_2 \int(f^{''}_j(x))^2dx},
$$
which can re-formulated as a group lasso problem as well. However, the model can not estimate the smoothness of the spline functions correctly, as the smoothness of all spline models are controlled by the lambda_2. The inflexible of the model creates problems in prediction.

From the Bayesian perspective, recent works that expands to the high-dimensional settings includes @Scheipl2012 and @Bai2020Spline. @Scheipl2012 proposed an new spike-and-slab structure prior, normal-mixture-of-inverse gamma for semi-parametric regression using MCMC. The normal-mixture-of-inverse gamma prior is mixture of t-distribution with an additional coefficients mixing vector, where the vector follows a mixture of two Gaussian distribution. @Bai2020Spline proposed an novel high dimensional Bayesian GAM using the spike-and-slab group lasso prior. An fast fitting EM-based algorithm is proposed. Asymptotic theory, such as posterior contraction rates and model selection consistency are proved. Even though, the algorithm is flexible to cover many non-Gaussian family in a high dimensional setting, the proposed method didn't address uncertainty quantification, i.e. calculating credible intervals etc. However, most of these Bayesian GAMs still suffer a common problem, computation cost. These Bayesian GAMs uses MCMC to estimate the posterior distribution, which would create an intense computation load.

<!-- Among all the choices of smoothing function, one particular family stood out, splines. Splines are piece-wise polynomial functions to interpolate nonlinear signals. Most commonly used spline is cubic spline, thanks to its smoothness property. [@Wood2017, PP.197] Early spline research focused on regression splines, where number and location of knots are pre-determined and ordinary least squares is used to estimate the smooth function. However, the specification of knots is not a trivial task, as the the positioning of knots are influential on the performance of the model. As an improvement, researchers proposed smoothing splines where as many knots are used and later shrunk using penalized algorithms. The smoothing spline concept was first introduced [@OSullivan1986; @OSullivan1988], and later generalized and made famous by @Eilers1996, called P-spline. Independently, @Ruppert1999 proposed similar idea. Later, Bayesian GAMs were introduced as an alternative to address measurement error in covariates [@Berry2002] or local adaptive smoothing [@Lang2004]. -->

In this article, we aim to address the challenges of non-linear signal modeling in high-dimensional application using Bayesian Hierarchical model. Specifically, we will propose a new Bayesian spike-and-slab prior for the generalized additive models and Cox spline models. The contribution of the proposed prior comes in two-fold: 1) it supports simultaneous variable selection and function selection; 2) it provides robust estimation and prediction for the coefficients and outcomes. As previously described, simultaneous variable selection and its functional form selection is not a trivial problem. Nevertheless, the Bayesian spike-and-slab prior is a perfect solution to the problem, as the latent binary indicator can be used to for variable selection and the spike and slab densities are used to adaptive adjust the smoothness of the spline function. Moreover, compared to currently available Bayesian spline models which mostly conduct a 'all-in-all-out' strategy, i.e. either including a non-linear form of a variable or excluding the variable completely, we offer a more flexible solution, rending one of three possibilities: no effect of a predictor, only linear effect of a predictor, or non-linear effect. This simplifies the process of determining what effect a predictor have on the outcome, without involving hypothesis testing where the degree of freedom for non-linear effect is complicated to estimate due to the shrinkage. Moreover, out model inherent the the robust estimation from Bayesian models. Lastly, The third aim of the dissertation work will focus on the software implementation of the proposed models and algorithms. We will implement the proposed models and algorithms in one of the most commonly used statistical computing environment R. Ancillary functions will be provided in assistance to model diagnostic, interpretation and graphing. The R implementation of the Bayesian hierarchical additive model is available via a freely available R package, BHAM at (https://github.com/boyiguo1/BHAM)[https://github.com/boyiguo1/BHAM]. 

The rest of the paper are arranged as the followings: in Chapter 2 we demonstrate the Bayesian Hierarchical generalized additive model, where we provide the setup of the proposed model and two fast-fitting algorithms. In Chapter 3, we Application to real-world datasets demonstrates the usefulness of the proposed methodologies in Chpater 4. Conclusion and discussions are made in Chapter 5. 


