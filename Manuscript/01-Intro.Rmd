# Introduction

* What is the problem
  * what is the GAM
  * expanding GAM in to high-dimensional frame work
  * Why this is important
  * Why this is hard
* Previous solutions
  * What are the methods
  * What are their problems
  
* Introduce the wiggliness matrix

* Our method
* The implication of our method
* Fast
* Structure of the rest of the paper

Nonlinear effect modeling has long history. Early work of GAMs can date back to 1979 when @Cleveland1979 proposed local smoothing for bivariate analysis. Since then, many mathematical and statistical models has been proposed, see xxx^[TODO: insert an review citation]. With recent advancement of computing technology, many machine learning algorithms that is computationally intensive start to gainmuch popularity in outcome prediction when non-linear effects assumed. Such models generally have improved prediction accuracy over classic statistical models, but have a fatal flaw, lack of transparency and interpretability. The primary objective of biomedical studies is to understand the etiology of diseases, which requires not only the knowledge if a biomarker is predictive of the disease, but also how the biomarker cause the disease. "Black box" models could less likely to be helpful.

As a balance between interpretability and model flexibility, semi-parametric and non-parametric regression models are studied. Among those, generalized additive model (GAM) proposed in the seminal work of @Hastie1987 gains the most popularity thanks to its flexibility and easiness to incorporate many different smoothing functions. A response variable, $Y$, that is assumed to follow some exponential family distribution $EF(\mu. \phi)$, can be expressed as the summation of smooth functions of a given $p$-dimensional vector of covariates $\bs x$, written as 
$$
E(Y|\bs x) = g^{-1}(a + \sum\limits^p_{j=1}f_j(x_j)),\qquad E[f_j(x_j)] = 0.
$$
where $g^{-1}$ is the inverse function of a smooth monotonic link function. GAM solves the model fitting problem for many distributional outcomes, such as Poisson, binomial, gamma and normal models. In the same article, @@Hastie1987 proposed the backfitting algorithm that estimates each smooth function of an additive model iteratively using partial residuals. The backfitting algorithm is compatible with almost any smoothing or modeling technique to be used as the nonparametric components. However, it is unclear how to estimate the degree of smoothness when necessary, for example for smoothing splines. The computation cost for cross-validating the smooth parameters can be expensive. @Wood2017[PP.318-320] provides more discussion on the problem.


In many area of modern biomedical research, dealing with high-dimensional data, also known as $p >> n$ problem, is common,  for example sequencing data analysis, electric health record data analysis. Many statistical models have been proposed for such problem, for example, penalized models and Bayesian hierarchical models. However, up to this point, most efforts of methodology development focus on modeling linear effect in high dimensional setting. It is still not clear what is the best practice on how to model non-linear effects in high-dimensional setting. Similar to expanding classic models to high-dimensional models, expanding the GAM framework for high-dimensional applications is not a trivial task. The difficulties mainly come in two-fold: grouping nature of the basis functions, and the balance between spline smoothness and variable sparsity. For GAM models, a smoothing function is consists of multiple basis functions. The basis functions of a variable is inherently related. The grouping structure should be properly taken care of for accurate estimation. Moreover, shrinkage is previously used solely for sparsity of the model in high dimensional regression models, or smoothness in GAMs. How to organically combine the two together remains as a troubling question.


<!-- Among all the choices of smoothing function, one particular family stood out, splines. Splines are piece-wise polynomial functions to interpolate nonlinear signals. Most commonly used spline is cubic spline, thanks to its smoothness property. [@Wood2017, PP.197] Early spline research focused on regression splines, where number and location of knots are pre-determined and ordinary least squares is used to estimate the smooth function. However, the specification of knots is not a trivial task, as the the positioning of knots are influential on the performance of the model. As an improvement, researchers proposed smoothing splines where as many knots are used and later shrunk using penalized algorithms. The smoothing spline concept was first introduced [@OSullivan1986; @OSullivan1988], and later generalized and made famous by @Eilers1996, called P-spline. Independently, @Ruppert1999 proposed similar idea. Later, Bayesian GAMs were introduced as an alternative to address measurement error in covariates [@Berry2002] or local adaptive smoothing [@Lang2004]. -->

#### Spike-and-slab prior
The term "spike-and-slab" was first coined in @Mitchell1988 describing the shape of the prior. It is a mixture distribution, comprised with a skinny spike density $f_{spike}(\cdot)$ for weak signals and a flat slab density $f_{slab}(\cdot)$ for large signals, mathematically
$$
 \beta_i|\gamma_i \sim (1-\gamma_i)f_{spike}(\beta_i) + \gamma_if_{slab}(\beta_i).
$$

The most distinct feature of the SS priors family is that it is conditioned on a latent binary variable $\gamma_i$ that indicates whether the covariate $x_i$ is included in the model. There are various spike-and-slab priors depending on the choice for the spike density $f_{spike}(\cdot)$ and the slab density $f_{slab}(\cdot)$, for example @George1993; @George1997; @Chipman1996 for grouped variables; @Brown1998 for multivariate outcomes; @Ishwaran2005; @Clyde2004 and reference therein. Here we introduce the most popular spike-and-slab prior introduced in @George1993, as  stochastic search variable selection (SSVS). Given a regression problem
$$
Y = \bs X\beta+\epsilon, 
$$
we can assume the prior distribution of the regression parameters $\beta$ depends on a vector of binary variables $\gamma=(\gamma_1, \dots \gamma_p)^\prime$ as follows: given $\gamma_i$, $\beta_i$ are independent distributed as a mixture priors
$$
\beta_i | \gamma_i \sim (1-\gamma_i)N(0, \sigma^2 v_0) + \gamma_iN(0,\sigma^2 v_1).
$$
The prior of $\gamma$ is traditionally assumed to be identically independent Bernoulli distribution. Even the conjugate normal prior of $\bs \beta$ provides analytic simplicity, there is no closed form solution for the posterior density of $\bs \gamma$. @George1993 described a Gibbs sampling algorithm to approximate the posterior distribution of $\bs \gamma$ for variable selection. The same authors later compared SSVS with other Bayesian variable selection approaches, providing some empirical suggestions on the advantages and disadvantages of various formulations and estimation algorithms. [@George1997] Among the various problem SSVS addresses, high-dimensionality is not one of them, due to the intensive computation needs of MCMC algorithms. @George1997 suggested MCMC algorithms works well for medium size of predictors (p=25), which is not feasible for high-dimensional data, where the number of predictors easily exceeds 100.

To alleviate the computation burden, many methodology development studies focus on the maximum a posteriori (MAP) estimate [@Tipping2004]. EM-based algorithms are prevalent used to speed the computation. @Rockova2014a proposed an EM-based algorithm for a spike-and-slab Gaussian mixture prior, EMVS. The coefficient $\beta_j$ independently follows a mixture normal distribution $N(0, \sigma^2((1-\gamma_i)v_0 + \gamma_i v_1)$ for $0 \leq v_0 < v_1$. The parameter $\sigma^2$ follows a inverse gamma prior, $IG(v/2, v\lambda /2)$. Like in the spike-and slab family, the latent binary variables $\bs \gamma$ follows a Bernoulli prior with a hyper prior $\theta$ follows a beta distribution, or simpler the uniform (0,1). In the E step, the latent variable are treated as the missing data, and calculate the posterior mean of $\gamma$; in the M step, a ridge estimator was used to update the coefficients, and $\sigma^2, \theta$ are updated accordingly. As the ridge regression creates small but nonzero coefficients, a further step is needed to select the variable: EMVS further suggests to threshold the inclusion probability for variable selection or generate regularization plot with different values of the spike scale parameter. EMVS is also compatible with structured prior when dealing with grouped variables. The group structure can be imposed on the prior of the latent binary variable $\gamma$ via either logistic regression or markov random field.

The same authors [@Rockova2018b; @Rockova2018] further extended the continuous spike-and-slab Gaussian prior to the spike-and-slab double exponential distribution, as known as spike-and-slab lasso (SSL). The authors also lay the theoretical discussion of comparing spike-and-slab from the penalty perspective and the Bayesian perspective. The SSL also mitigated the problem of EMVS that the weak signals are not shrink to zero. Yi and his group incorporated the SSL prior in the EM-IWLS algorithm and proposed another EM algorithm, EM-cyclic coordinate descent, based on the Lasso penalty SSL exhibit. Such application were used in generalized linear models[@Tang2017a], cox models [@Tang2017] and their grouped counterparts[@Tang2018; @Tang2019]. Both EM based algorithm provide deterministic solutions, which becomes a more popular property as reproducible research becomes more valued. However, while the EMVS are not able to provide inference of the coefficients easily, the EM-IWLS can easily extract the variance-covariance matrix due to the IWLS algorithm. @Bai2020 provides an up-to-date summary of SSL methods and theoretical discussion.

To the best of our knowledge, there are few GAM models proposed for high dimensional application. In common practice, model matrix of spline terms are generated using base functions of preference, and then feed to a penalized model. Without proper handle of the grouping nature of bases functions, the estimation of the smooth function is less accurate. @Bai2020Spline shows inferior performance of such practice compared to using an ad hoc high-dimensional additive model. 

Among the specialized high-dimensional spline methods, @Ravikumar2009 extended the original backfitting algorithm [@Hastie1987] to accommodate the high-dimensional setting. The sparsity and smoothness was handled separately with a soft threshold. The authors also demonstrated its similarity to Group Lasso. @Meier2009 proposed an a sparsity-smoothness penalty,

$$
J(f_j) = \lambda_1 \sqrt{||f_j||^2_n + \lambda_2 \int(f^{''}_j(x))^2dx},
$$
which can re-formulated as a group lasso problem as well. However, the model can not estimate the smoothness of the spline functions correctly, as the smoothness of all spline models are controlled by the lambda_2. The inflexible of the model creates problems in prediction.

From the Bayesian perspective, recent works that expands to the high-dimensional settings includes @Scheipl2012 and @Bai2020Spline. @Scheipl2012 proposed an new spike-and-slab structure prior, normal-mixture-of-inverse gamma for semi-parametric regression using MCMC. The normal-mixture-of-inverse gamma prior is mixture of t-distribution with an additional coefficients mixing vector, where the vector follows a mixture of two Gaussian distribution. @Bai2020Spline proposed an novel high dimensional Bayesian GAM using the spike-and-slab group lasso prior. An fast fitting EM-based algorithm is proposed. Asymptotic theory, such as posterior contraction rates and model selection consistency are proved. Even though, the algorithm is flexible to cover many non-Gaussian family in a high dimensional setting, the proposed method didn't address uncertainty quantification, i.e. calculating credible intervals etc. However, most of these Bayesian GAMs still suffer a common problem, computation cost. These Bayesian GAMs uses MCMC to estimate the posterior distribution, which would create an intense computation load.

In this dissertation, we aim to address the challenges of non-linear signal modeling in high-dimensional application using Bayesian Hierarchical model. Specifically, we will propose a new Bayesian spike-and-slab prior for the generalized additive models and Cox spline models. The contribution of the proposed prior comes in two-fold: 1) it supports simultaneous variable selection and function selection; 2) it provides robust estimation and prediction for the coefficients and outcomes. As previously described, simultaneous variable selection and its functional form selection is not a trivial problem. Nevertheless, the Bayesian spike-and-slab prior is a perfect solution to the problem, as the latent binary indicator can be used to for variable selection and the spike and slab densities are used to adaptive adjust the smoothness of the spline function. Moreover, compared to currently available Bayesian spline models which mostly conduct a 'all-in-all-out' strategy, i.e. either including a non-linear form of a variable or excluding the variable completely, we offer a more flexible solution, rending one of three possibilities: no effect of a predictor, only linear effect of a predictor, or non-linear effect. This simplifies the process of determining what effect a predictor have on the outcome, without involving hypothesis testing where the degree of freedom for non-linear effect is complicated to estimate due to the shrinkage. Moreover, out model inherent the the robust estimation from Bayesian models. 

Given the proposed model specification, the estimation of the parameters can be easily achieved with MCMC algorithm. Nevertheless, considering the computational burden MCMC algorithms bear especially in high-dimensional analysis, we propose parsimonious solutions, stemming from previous EM-IWLS [@Rockova2014a]and EM-coordinate descent algorithms[@Tang2018; @Tang2019]. The computational friendliness allows for fitting a model with different sets of initial values, mimicking the posterior sampling in the MCMC algorithms. We are hopeful that this could improve the sacrificed robustness of the estimation process due to the MAP estimation used by the EM algorithms in comparison to the model averaging approach.

The third aim of the dissertation work will focus on the software implementation of the proposed models and algorithms. We will implement the proposed models and algorithms in one of the most commonly used statistical computing environment R. Ancillary functions will be provided in assistance to model diagnostic, interpretation and graphing. The R implementation of the Bayesian Hierarchical additive model is available via a freely available R package, BHAM at (https://github.com/boyiguo1/BHAM)[https://github.com/boyiguo1/BHAM]. 

The rest of the paper are arranged as the followings: in Chapter 32 we demonstrate the Bayesian Hierarchical generalized additive model, where we provide the setup of the proposed model and two fast-fitting algorithms. In Chapter 3, we Application to real-world datasets demonstrates the usefulness of the proposed methodologies in Chpater 4. Conclusion and discussions are made in Chapter 5. 


