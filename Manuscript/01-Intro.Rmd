\section{Introduction}
\label{sec:intro}

Many modern biomedical research, e.g. sequencing data analysis, electric health record data analysis, require special treatment of high-dimensionality, commonly known as $p >> n$ problem. There is extensive literature on high-dimensional linear models via penalized models or Bayesian hierarchical models, see Mallick and Yi [@Mallick2013] for review. These models are built upon a restrictive and unrealistic assumption, linearity. In classical statistical modeling, many strategies and models are proposed to relax the linearity assumption with various degrees of complexity. For example, variable categorization is a simple and common practice in epidemiology, but suffers from power and interpretation issues. More complex models to address nonlinear effects include random forest and other so-called "black box" models [@Breiman2001]. These models are useful for statistical prediction but do not estimate parameters relevant to the data generation process that one can draw inferences from. In addition, how to generalize these \bg{"black box"} models to the high-dimensional setting remains unclear.

\bg{For their easy interpretation and flexibility, nonparametric regression models serve as great alternatives to the "black-box" models in the context of prediction and variable selection}. Among those, generalized additive models (GAMs), proposed in the seminal work of Hastie and Tibshirani [@Hastie1987], grew to be one of the most popular modeling tools. In a GAM, the response variable, $Y$, which is assumed to follow some exponential family distribution with mean $\mu$ and dispersion $\phi$, can be modeled with the summation of smoothing functions, $B_j(\cdot), j = 1, \dots, p$, of a given $p$-dimensional vector of covariates $\bs x$, written as 
$$
 E(Y|\bs x) = g^{-1}(\beta_0 + \sum\limits^p_{j=1}B_j(x_j)),
$$
where $g^{-1}(\cdot)$ is the inverse of a monotonic link function. Nevertheless, the classical GAMs cannot fulfill the increasing analytic demands for high-dimensional data analysis.

There exists some proposals to generalize the classical GAM to accommodate high-dimensional applications. The regularized models, branching out from group regularized linear models, are used to fit GAMs by accounting for the structure introduced when expanding smoothing functions. Ravikumar et al. [@Ravikumar2009] extended the grouped lasso [@Yuan2006] to additive models (AMs); Huang et al. [@Huang2010] further developed adaptive grouped lasso for additive models; Wang et al. [@Wang2007] and Xue [@Xue2009] respectively applied grouped SCAD penalty [@Fan2001] to additive models. Recently Bayesian hierarchical models are also used in the context of high-dimensional additive models.\bg{[TODO: limit the scope, "particuaraly with spike-and-slab literatures."]} Various group spike-and-slab priors combining with computationally intensive Markov chain Monte Carlo (MCMC) algorithms [@Xu2015; @Yang2020] are proposed, where the application on AMs are by-products.\bg{[TODO: adjust citation position for clarity.]} Bai et al. [@Bai2020] was the first to apply group spike-and-slab lasso prior to Gaussian AMs using a fast optimization algorithm, and further generalized the framework to GAMs[@Bai2021]. Focus on addressing the sparsity, these methods can overly penalize the basis function coefficients and produce inaccurate predictions and curve interpolation, particularly when complex signals are assumed and large number of knots are used. [@Scheipl2013] In addition, these methods adapt an 'all-in-all-out' strategy, i.e. either including or excluding the variable completely, rendering no space for bi-level selection. Scheipl et al. [@Scheipl2012] proposed a spike-and-slab structure prior that address the previous challenges. But the model fitting relies on computational intensive MCMC algorithms and creates scalability concern. It would be of special interest to develop a fast, flexible and accurate generalized additive model framework. 


\bg{To address these challenges, we propose a novel Bayesian hierarchical generalized additive model (BHAM) for high dimensional data analysis. [TODO: add a sentences on the contribution. Focuse on prediction of high-dimensional data, and possibly mentioning of vairbale selection.] Specifically, we incorporate smoothing penalties, derived from the smoothing spline literature [TODO: add Woods citation], via re-parameterization [of the smoothing function] to avoid overly shrinking basis function coefficients. Smoothing penalties were also previously used in the spike-and-slab GAM [@Scheipl2012] and the sparsity-smoothness penalty [@Meier2009].  We then impose a new two-part spike-and-slab spline prior on the smoothing functions for bi-level selection such that the linear and nonlinear spaces of smoothing functions can be selected separately.
In addition, a scalable optimization-based algorithms, EM-Coordinate Descent (EM-CD) algorithm are developed. While the primary focus of this model is improvement in prediction, the proposed model also provides utility in fucntional selection. Particuaraly, the porposed prior setup encourages a bib-level selection/flexible solution, rendering one of three possibilities for each predictor: no effect, only linear effect, or linear and nonlinear effects, as the re-parameterization allows the separation of the linear space of a smoothing function from the nonlinear space. The proposed model is implemented in an publicly available R package \texttt{BHAM} via [https://github.com/boyiguo1/BHAM](https://github.com/boyiguo1/BHAM), making translational science more accessible.}

The proposed framework, BHAM, differs from previous spike-and-slab based GAMs, i.e. the spike-and-slab GAM [@Scheipl2012] and the SB-GAM [@Bai2021] in three ways. First of all, the proposed spike-and-slab spline prior is a spike-and-slab lasso type prior using independent mixture double exponential distribution, compared to spike-and-slab GAM that uses normal-mixture-of-inverse gamma prior. Spike-and-slab lasso priors provide computational convenience during model fitting by using optimization algorithms instead of intensive sampling algorithms. They make fitting high-dimensional models more feasible without sacrificing performance in prediction and variable selection. Secondly, SB-GAM uses a group spike-and-slab lasso prior with an EM-Coordinate Descent (EM-CD hereafter) algorithm to fit the model. While both methods use the combination of expectation maximization algorithm and coordient descent algorithm, there are subtle difference in the implementation due to the difference in prior specification. The proposed model sets up independent priors among basis function coefficients after the re-parameterization step, which provides some advantage in computation. In addition, we offer an alternative algorithm, EM-IWLS, that can provide variance-covariance matrix of the coefficients. Last but not least, the proposed model addresses the incapability of bi-level selection in SB-GAM.

In Section \ref{sec:BHAM}, we establish the Bayesian hierarchical generalized additive model, introduce the proposed spike-and-slab spline priors, and describe the fast-fitting EM-CD algorithmalgorithm. In Section \ref{sec:sim}, we compare the proposed framework to state-of-the-art models, mgcv, COSSO and sparse Bayesian GAM \bg{[TODO: add other models]} via Monte Carlo simulation studies. Analyses of two metabolomics datasets are presented in Section \ref{sec:real_data}. Conclusion and discussions are given in Section \ref{sec:concl}.






