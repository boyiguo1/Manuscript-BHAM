# Introduction
\label{sec:intro}

Using additive models to interpolate nonlinear effects has a long history. Early work of additive models can date back to 1979 when Cleveland [@Cleveland1979] proposed local smoothing for bivariate analysis. Since then, many mathematical and statistical models have been proposed. [@Wood2020] With the recent advancement of computational technology, many machine learning algorithms that are computationally intensive start to gain much popularity in prognostic modeling and functional selection, especially when complex signals, for example nonlinear effects, are assumed. Such models generally have improved prediction accuracy over classic statistical models. However, these models have a fatal flaw, the lack of transparency and interpretability, and hence are referred to as "black box" models [@Breiman2001]. Given that the primary objective of biomedical studies is to understand the etiology of diseases, "Black box" models are less helpful as they can hardly explain how the change of biomarkers is associated with the disease.

Semi-parametric regression models are a more appropriate alternative to the "black-box" models, thanks to their balancing between model flexibility and interpretability. Among those, generalized additive models (GAMs), proposed in the seminal work of Hastie and Tibshirani [@Hastie1987], grew to be one of the most popular modeling tools. In a GAM, the response variable, $Y$, which is assumed to follow some exponential family distribution $EF(\mu, \phi)$ with mean $\mu$ and dispersion $\phi$, can be expressed as the summation of smooth functions, $f_j, j = 1, \dots, p$, of a given $p$-dimensional vector of covariates $\bs x$, written as 
$$
 E(Y|\bs x) = g^{-1}(a + \sum\limits^p_{j=1}f_j(x_j)),
$$
where $g^{-1}$ is the inverse function of a smooth monotonic link function. The smooth functions can take many forms and are estimated using a pseudo-weighted version of the backfitting algorithm [@Breiman1985]. Nevertheless, the classic GAMs cannot fulfill the increasing analytic demands for high-dimensional data analysis in biomedical studies.

Many modern biomedical research, e.g. sequencing data analysis, electric health record data analysis, require special treatment of high-dimensionality, commonly known as $p >> n$ problem. There is extensive literature on high-dimensional linear models via penalized models or Bayesian hierarchical models, see Mallick and Yi [@Mallick2013] for review. These models assume sparsity among the predictors, i.e. most predictors have small or no effects on the outcome. Further development focuses on addressing the grouping structure among the predictors, where AMs and GAMs can be considered special cases. Various penalties are imposed on the basis function coefficients: Ravikumar et al. [@Ravikumar2009] extended the grouped lasso [@Yuan2006] to additive models; Huang et al. [@Huang2010] further developed adaptive grouped lasso for additive models; Wang et al. [@Wang2007] and Xue [@Xue2009] respectively applied grouped SCAD penalty [@Fan2001] to the additive models. Recent Bayesian hierarchical models are also used in the context of high-dimensional additive models. Various group spike-and-slab priors combining with computationally intensive Markov chain Monte Carlo (MCMC) algorithms [@Xu2015; @Yang2020] are proposed, where the application on AMs are by-products. Scheipl et al. [@Scheipl2012] proposed an ad hoc spike-and-slab structure prior, normal-mixture-of-inverse gamma, specifically for semi-parametric regression using MCMC. The normal-mixture-of-inverse gamma prior is mixture of t-distribution with an additional coefficients mixing vector, where the vector follows a mixture of two Gaussian distribution. Bai et al. [@Bai2020] was the first to apply group spike-and-slab lasso prior to AMs using a fast optimization algorithm, and further generalized the framework to GAMs[@Bai2021]. Regardless of the nice statistical properties, a practical and pivotal question, if non-linear effects exist, yet to be addressed by these methods. Besides Scheipl et al. [@Scheipl2012], most proposed methods adapt an 'all-in-all-out' strategy, i.e. either including or excluding the variable completely, rendering no space for within group selection. In addition, a common criticism received by these methods is that the basis function coefficients tend to be overly penalized due to the sparsity assumption directly applied on the basis function coefficients.[@Scheipl2013] It would be of special interest to develop a fast yet flexible generalized additive model.

To address these challenges, we propose a novel Bayesian hierarchical generalized additive model (BHAM) for high dimensional data analysis. Specifically, we impose a new bi-part spike-and-slab spline prior on the smooth functions to perform bilevel selection, where the linear and nonlinear spaces of smooth functions can be selected separately. The prior setup encourages a flexible solution, rending one of three possibilities: no effect, only linear effect, or nonlinear effect of a predictor. Additionally, two fast computing optimization-based algorithms are developed for high and ultrahigh-dimensional[@Fan2009] settings respectively. Meanwhile, to avoid the over-penalization of the basis function coefficients, smoothness penalties are incorporated in the model via re-parameterization. Smoothness penalties are commonly implemented in GAMs through smoothing and penalized regression splines. They are quadratic norms of the coefficients and allow locally adaptive penalties on each smoothing function. For the most common cases of cubic splines, the smoothness penalty conditioning on a smoothness penalty parameter $\lambda_j$ is a function of the integration of the second derivative of the spline function, expressed mathematically as
\begin{equation}\label{eq:smoothpen}
  \text{pen}\left[f_j(x)\right] = \lambda_j \int f^{"}_j(x)dx = \lambda_j \bs \beta_j^T \bs S_j \bs \beta_j ,
\end{equation}
where $\bs S_j$ is a known smoothness penalty matrix, and $\bs \beta_j$ is the basis function coefficients. Smoothness penalties are also previously used in the spike-and-slab GAM [@Scheipl2012] and the sparsity-smoothness penalty [@Meier2009]. Moreover, incorporating the smoothness penalty allows the separation of the linear space of the smooth function, also know as the null space, from the nonlinear space. Marra and Wood [@Marra2011] introduced a re-parameterization of the basis function matrix based on the decomposition of the smoothness penalty matrix $\bs S$. The re-parameterization allows different penalties imposed on the null and nonlinear spaces separately, and further allows the sparsity in linear space and smoothness in nonlinear effect interpolation. In addition, the priors of the linear and nonlinear effects are independent, creating computational convenience and further improving the scalability of the model.

The proposed BHAM uses a spike-and-slab prior for function selections, but it substantially differs from previous spike-and-slab based GAMs, i.e. the spike-and-slab GAM [@Scheipl2012] and the SB GAM [@Bai2021] in many ways. First of all, the proposed spike-and-slab spline prior is a spike-and-slab lasso type prior using independent mixture double exponential distribution, compared to spike-and-slab GAM. Spike-and-slab lasso priors provide computational convenience during model fitting through optimization algorithms versus computational intensive sampling algorithms. They make fitting high- and ultrahigh-dimensional models more feasible without sacrificing performance in prediction and variable selection. Secondly, the optimization algorithms introduced in this article include EM-Coordinate Descent algorithm and EM-Iterative weighted least square algorithm for different utilities. The applications of the two aforementioned algorithm are fruitful in Bayesian hierarchical models in high-dimensional data analysis. [@Yi2012; @Rockova2014a; @Rockova2018] Even though SB-GAM with a group spike-and-slab lasso prior also uses EM-CD for model fitting, the proposed EM-IWLS provides uncertainty measures that could be further used for confidence bound calculation and hypothesis testing. EM-IWLS complements the lack of inference potential from EM-CD in a median and high dimensional setting. Furthermore, the proposed model addresses the incapability of bilevel selection in SB-GAM. Lastly, the R implementation of the Bayesian hierarchical additive model is available publicly via BHAM at [https://github.com/boyiguo1/BHAM](https://github.com/boyiguo1/BHAM), make transnational science more accessible.


The rest of the paper are arranged as the followings: in Chapter 2 we demonstrate the Bayesian hierarchical generalized additive model, where we provide the setup of the proposed model and describe the two fast-fitting algorithms. In Chapter 3, we compare the proposed via monte carlo simulation. .Application to real-world datasets demonstrates the usefulness of the proposed methodologies in Chpater 4. Conclusion and discussions are made in Chapter 5. ^[TODO: rewrite this sentence.]










<!-- Similar to expanding classic models to high-dimensional models, expanding the GAM framework for high-dimensional applications is not a trivial task. The difficulties mainly come in two-fold: grouping nature of the basis functions, and the balance between spline smoothness and variable sparsity. For GAM models, a smoothing function is consists of multiple basis functions. The basis functions of a variable is inherently related. The grouping structure should be properly taken care of for accurate estimation. Moreover, shrinkage is previously used solely for sparsity of the model in high dimensional regression models, or smoothness in GAMs. How to organically combine the two together remains as a troubling question.  -->

<!-- To the best of our knowledge, there are few GAM models proposed for high dimensional application. In common practice, model matrix of spline terms are generated using base functions of preference, and then feed to a penalized model. Without proper handle of the grouping nature of bases functions, the estimation of the smooth function is less accurate. @Bai2020Spline shows inferior performance of such practice compared to using an ad hoc high-dimensional additive model. Among the specialized high-dimensional spline methods, @Ravikumar2009 extended the original backfitting algorithm [@Hastie1987] to accommodate the high-dimensional setting. The sparsity and smoothness was handled separately with a soft threshold. The authors also demonstrated its similarity to Group Lasso. @Meier2009 proposed an a sparsity-smoothness penalty, -->

<!-- $$ -->
<!-- J(f_j) = \lambda_1 \sqrt{||f_j||^2_n + \lambda_2 \int(f^{''}_j(x))^2dx}, -->
<!-- $$ -->
<!-- which can re-formulated as a group lasso problem as well. However, the model can not estimate the smoothness of the spline functions correctly, as the smoothness of all spline models are controlled by the lambda_2. The inflexible of the model creates problems in prediction. -->


<!-- Among all the choices of smoothing function, one particular family stood out, splines. Splines are piece-wise polynomial functions to interpolate nonlinear signals. Most commonly used spline is cubic spline, thanks to its smoothness property. [@Wood2017, PP.197] Early spline research focused on regression splines, where number and location of knots are pre-determined and ordinary least squares is used to estimate the smooth function. However, the specification of knots is not a trivial task, as the the positioning of knots are influential on the performance of the model. As an improvement, researchers proposed smoothing splines where as many knots are used and later shrunk using penalized algorithms. The smoothing spline concept was first introduced [@OSullivan1986; @OSullivan1988], and later generalized and made famous by @Eilers1996, called P-spline. Independently, @Ruppert1999 proposed similar idea. Later, Bayesian GAMs were introduced as an alternative to address measurement error in covariates [@Berry2002] or local adaptive smoothing [@Lang2004]. -->

<!-- Lastly, The third aim of the dissertation work will focus on the software implementation of the proposed models and algorithms. We will implement the proposed models and algorithms in one of the most commonly used statistical computing environment R.  The R implementation of the Bayesian hierarchical additive model is available via a freely available R package, BHAM at (https://github.com/boyiguo1/BHAM)[https://github.com/boyiguo1/BHAM].  -->





