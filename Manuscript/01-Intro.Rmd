# Introduction
\label{sec:intro}

Many modern biomedical research, e.g. sequencing data analysis, electric health record data analysis, require special treatment of high-dimensionality, commonly known as $p >> n$ problem. There is extensive literature on high-dimensional linear models via penalized models or Bayesian hierarchical models, see @Mallick2013 for review. These models are built upon a restrictive and unrealistic assumption, linearity. In classical statistical modeling, many strategies and models are proposed to relax the linearity assumption with various degrees of complexity. For example, variable categorization is a simple and common practice in epidemiology, but suffers from power and interpretation issues. More complex models to address nonlinear effects include random forest and other so-called "black box" models [@Breiman2001]. These models are not particularly attractive in biomedical studies, as they lack the transparency to understand the etiology of diseases. In addition, how to generalize these models to the high-dimensional setting remains unclear.

<!-- Using additive models to interpolate nonlinear effects has a long history. Early work of additive models can date back to 1979 when Cleveland [@Cleveland1979] proposed local smoothing for bivariate analysis. Since then, many mathematical and statistical models have been proposed. [@Wood2020] With the recent advancement of computational technology, many machine learning algorithms that are computationally intensive start to gain much popularity in prognostic modeling and functional selection, especially when complex signals, for example nonlinear effects, are assumed. Such models generally have improved prediction accuracy over classical statistical models.  -->

<!-- However, these models have a fatal flaw, the lack of transparency and interpretability, and hence are referred to as "black box" models [@Breiman2001]. Given that the primary objective of biomedical studies is to understand , "Black box" models are less helpful as they can hardly explain how the change of biomarkers is associated with the disease. -->

Nonparametric regression models are appropriate alternatives to the "black-box" models thanks to their balance between model flexibility and interpretability. Among those, generalized additive models (GAMs), proposed in the seminal work of @Hastie1987, grew to be one of the most popular modeling tools. 
<!-- In a GAM, the response variable, $Y$, which is assumed to follow some exponential family distribution, denoted by $EF(\mu, \phi)$, with mean $\mu$ and dispersion $\phi$, can be modeled with the summation of smoothing functions, $B_j(\cdot), j = 1, \dots, p$, of a given $p$-dimensional vector of covariates $\bs x$, written as  -->
<!-- $$ -->
<!--  E(Y|\bs x) = g^{-1}(\beta_0 + \sum\limits^p_{j=1}B_j(x_j)), -->
<!-- $$ -->
<!-- where $g^{-1}(\cdot)$ is the inverse of a monotonic link function. The smoothing functions can take many forms and are estimated using a pseudo-weighted version of the backfitting algorithm [@Breiman1985]. Nevertheless, the classical GAMs cannot fulfill the increasing analytic demands for high-dimensional data analysis in biomedical studies. -->
There exists some proposals to generalize the classical GAM to accommodate high-dimensional applications. The regularized models, branching out from group regularized linear models, are used to fit GAMs by accounting for the structure introduced when expanding smoothing functions.
<!-- These models assume sparsity among the predictors, i.e. most predictors have small or no effects on the outcome. Further development focuses on addressing the grouping structure among the predictors, where AMs and GAMs can be considered special cases. Various penalties are imposed on the basis function coefficients:  -->
Ravikumar et al. [@Ravikumar2009] extended the grouped lasso [@Yuan2006] to additive models (AMs); Huang et al. [@Huang2010] further developed adaptive grouped lasso for additive models; Wang et al. [@Wang2007] and Xue [@Xue2009] respectively applied grouped SCAD penalty [@Fan2001] to additive models. Recent Bayesian hierarchical models are also used in the context of high-dimensional additive models. Various group spike-and-slab priors combining with computationally intensive Markov chain Monte Carlo (MCMC) algorithms [@Xu2015; @Yang2020] are proposed, where the application on AMs are by-products. Bai et al. [@Bai2020] was the first to apply group spike-and-slab lasso prior to Gaussian AMs using a fast optimization algorithm, and further generalized the framework to GAMs[@Bai2021]. As these methods focus on addressing the sparsity, it is very likely to overly penalize the basis function coefficients and produce inaccurate predictions and curve interpolation, particularly when complex signals are assumed and large number of knots are used. [@Scheipl2013] In addition, these methods adapt an 'all-in-all-out' strategy, i.e. either including or excluding the variable completely, rendering no space for bi-level selection. Scheipl et al. [@Scheipl2012] proposed a spike-and-slab structure prior that resolves the previous problems. But the model fitting relies on computational intensive MCMC algorithmic and creates scalability concern. It would be of special interest to develop a fast, flexible and accurate generalized additive model framework. 


To address these challenges, we propose a novel Bayesian hierarchical generalized additive model (BHAM) for high dimensional data analysis. Specifically, we incorporate smoothing penalties in the model via reparameterization of the smoothing function to avoid overly shrinking basis function coefficients. <!-- Smoothing penalties are commonly implemented in the classical GAMs through smoothing regression splines. They are quadratic norms of the coefficients and allow locally adaptive penalties on each smoothing function. A smoothing penalty conditioning on a smoothing parameter $\lambda_j$ is a function of the integration of the second derivative of the spline function, expressed mathematically as -->
<!-- \begin{equation}\label{eq:smoothpen} -->
<!--   \text{pen}\left[B_j(x)\right] = \lambda_j \int B^{\prime\prime}_j(x)dx = \lambda_j \bs \beta_j^T \bs S_j \bs \beta_j , -->
<!-- \end{equation} -->
<!-- where $\bs S_j$ is a known smoothing penalty matrix, and $\bs \beta_j$ are the basis function coefficients.  -->
Smoothing penalties were also previously used in the spike-and-slab GAM [@Scheipl2012] and the sparsity-smoothness penalty [@Meier2009]. Moreover, incorporating the smoothing penalty allows the separation of the linear space of smoothing functions from the nonlinear space. We then impose a new bi-part spike-and-slab spline prior on the smoothing functions for bi-level selection such that the linear and nonlinear spaces of smoothing functions can be selected separately. The prior setup encourages a flexible solution, rending one of three possibilities: no effect, only linear effect, or nonlinear effect of a predictor. In addition, two scalable optimization-based algorithms are developed for high and ultrahigh-dimensional[@Fan2009] settings respectively. 
<!-- Marra and Wood [@Marra2011] introduced a reparameterization of the basis function matrix based on the decomposition of the smoothness penalty matrix $\bs S$. The reparameterization allows different penalties imposed on the null and nonlinear spaces separately, and further allows the sparsity in linear space and smoothness in nonlinear effect interpolation. -->
 <!-- In addition, the priors of the linear and nonlinear effects are independent, creating computational convenience and further improving the scalability of the model. -->
<!-- The proposed framework, BHAM, differs from previous spike-and-slab based GAMs, i.e. the spike-and-slab GAM [@Scheipl2012] and the SB-GAM [@Bai2021] in many ways. First of all, the proposed spike-and-slab spline prior is a spike-and-slab lasso type prior using independent mixture double exponential distribution, compared to spike-and-slab GAM. Spike-and-slab lasso priors provide computational convenience during model fitting by using optimization algorithms instead of computational intensive sampling algorithms. They make fitting high- and ultrahigh-dimensional models more feasible without sacrificing performance in prediction and variable selection. Secondly, the optimization algorithms introduced in this article include EM-Coordinate Descent (EM-CD) algorithm and EM-Iterative weighted least square (EM-IWLS) algorithm for different utilities. The applications of the two aforementioned algorithms are fruitful in Bayesian hierarchical models for high-dimensional data. [@Yi2012; @Rockova2014a; @Rockova2018] Even though SB-GAM with a group spike-and-slab lasso prior also uses EM-CD for model fitting, the proposed EM-IWLS provides uncertainty measures that could be further used for confidence bound calculation and hypothesis testing. EM-IWLS complements the lack of inference potential from EM-CD in a median and high dimensional setting. Furthermore, the proposed model addresses the incapability of bi-level selection in SB-GAM. -->
Lastly, the R implementation of the Bayesian hierarchical additive model is available publicly via \texttt{BHAM} at [https://github.com/boyiguo1/BHAM](https://github.com/boyiguo1/BHAM), making transnational science more accessible.

In Section \ref{sec:BHAM}, we establish the Bayesian hierarchical generalized additive model, introduce the proposed spike-and-slab spline priors, and describe the two fast-fitting algorithms. In Section \ref{sec:sim}, we compare the proposed framework to state-of-art models, mgcv, COSSO and sparse Bayesian GAM via Monte Carlo simulation studies. Analysis of metabolomics datasets are presented in Section \ref{sec:real_data}. Conclusion and discussions are given in Section \ref{sec:concl}.










<!-- Similar to expanding classical models to high-dimensional models, expanding the GAM framework for high-dimensional applications is not a trivial task. The difficulties mainly come in two-fold: grouping nature of the basis functions, and the balance between spline smoothness and variable sparsity. For GAM models, a smoothing function is consists of multiple basis functions. The basis functions of a variable is inherently related. The grouping structure should be properly taken care of for accurate estimation. Moreover, shrinkage is previously used solely for sparsity of the model in high dimensional regression models, or smoothness in GAMs. How to organically combine the two together remains as a troubling question.  -->

<!-- To the best of our knowledge, there are few GAM models proposed for high dimensional application. In common practice, model matrix of spline terms are generated using base functions of preference, and then feed to a penalized model. Without proper handle of the grouping nature of bases functions, the estimation of the smoothing function is less accurate. @Bai2020Spline shows inferior performance of such practice compared to using an ad hoc high-dimensional additive model. Among the specialized high-dimensional spline methods, @Ravikumar2009 extended the original backfitting algorithm [@Hastie1987] to accommodate the high-dimensional setting. The sparsity and smoothness was handled separately with a soft threshold. The authors also demonstrated its similarity to Group Lasso. @Meier2009 proposed an a sparsity-smoothness penalty, -->

<!-- $$ -->
<!-- J(f_j) = \lambda_1 \sqrt{||f_j||^2_n + \lambda_2 \int(f^{''}_j(x))^2dx}, -->
<!-- $$ -->
<!-- which can re-formulated as a group lasso problem as well. However, the model can not estimate the smoothness of the spline functions correctly, as the smoothness of all spline models are controlled by the lambda_2. The inflexible of the model creates problems in prediction. -->


<!-- Among all the choices of smoothing function, one particular family stood out, splines. Splines are piece-wise polynomial functions to interpolate nonlinear signals. Most commonly used spline is cubic spline, thanks to its smoothness property. [@Wood2017, PP.197] Early spline research focused on regression splines, where number and location of knots are pre-determined and ordinary least squares is used to estimate the smoothing function. However, the specification of knots is not a trivial task, as the the positioning of knots are influential on the performance of the model. As an improvement, researchers proposed smoothing splines where as many knots are used and later shrunk using penalized algorithms. The smoothing spline concept was first introduced [@OSullivan1986; @OSullivan1988], and later generalized and made famous by @Eilers1996, called P-spline. Independently, @Ruppert1999 proposed similar idea. Later, Bayesian GAMs were introduced as an alternative to address measurement error in covariates [@Berry2002] or local adaptive smoothing [@Lang2004]. -->






