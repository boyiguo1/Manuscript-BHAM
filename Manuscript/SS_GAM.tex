\documentclass[AMA,STIX1COL,]{WileyNJD-v2}


% Pandoc citation processing

\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{colortbl}
\usepackage{hhline}
\usepackage{longtable}
\usepackage{array}
\usepackage{hyperref}

\articletype{Research article}

\received{2021-01-01}

\revised{2021-02-01}

\accepted{2021-03-01}

\raggedbottom

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}

\title{Spike-and-Slab Generalized Additive Models and Scalable
Algorithms for High-Dimensional Data}

\author[a]{Boyi Guo*}
\author[b]{Byron C. Jaeger}
\author[a]{AKM Fazlur Rahman}
\author[a]{D. Leann Long}
\author[a]{Nengjun Yi*}

\authormark{Guo et al.}

\address[a]{Department of Biostatistics, University of Alabama at
Birmingham, Birmingham, USA}
\address[b]{Department of Biostatistics and Data Science, Wake Forest
School of Medicine, Winston-Salem, USA}

\corres{Boyi Guo and Nengjun Yi, Department of Biostatistics, University
of Alabama at Birmingham, Birmingham, USA. \email{boyiguo1@uab.edu},
\email{nyi@uab.edu}}

\presentaddress{This is sample for present address text this is sample
for present address text}

\abstract{There are proposals that extend the classical generalized
additive models (GAMs) to accommodate high-dimensional data (\(p>>n\))
using group sparse regularization. However, the sparse reuglarization
may induce excess shrinkage when estimating smoothing functions,
damaging predictive performance. Moreover, most of these GAMs consider
an ``all-in-all-out'' approach for function selection, rendering them
difficult to answer if nonlinear effects are necessary. While some
Bayesian models can address these shortcomings, using Markov chain Monte
Carlo (MCMC) algorithms for model fitting creates a new challenge,
scalability. Hence, we propose Bayesian hierarchical generalized
additive models as a solution: we incorporate the smoothing penalty that
can separate linear and nonlinear spaces of smoothing function. A novel
spike-and-slab spline prior is used to select the components of
smoothing functions. Two scalable and deterministic algorithms,
EM-Coordinate Descent and EM-Iterative Weighted Least Squares, are
developed for different utilities. Simulation studies and metabolomics
data analyses demonstrate improved predictive or computational
performance against state-of-the-art models, mgcv, COSSO and sparse
Bayesian GAM. The software implementation of the proposed models is
freely available via an R package BHAM.}

\keywords{Spike-and-Slab Priors; High-Dimensional Data; Generalized
Additive Models; EM-IWLS; EM-Coordinate Decent; Scalablility}

\maketitle

\pgfplotsset{compat=1.18}
\usetikzlibrary{shapes.geometric, arrows, positioning, calc, matrix, backgrounds, fit}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\tp}{*}
\newcommand{\pr}{\text{Pr}}
\newcommand{\repa}{\text{repa}}
\newcommand{\simiid}{\overset{\text{iid}}{\sim}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\label{sec:intro}

Many modern biomedical research, e.g.~sequencing data analysis, electric
health record data analysis, require special treatment of
high-dimensionality, commonly known as \(p >> n\) problem. There is
extensive literature on high-dimensional linear models via penalized
models or Bayesian hierarchical models, see Mallick and Yi
\citep{Mallick2013} for review. These models are built upon a
restrictive and unrealistic assumption, linearity. In classical
statistical modeling, many strategies and models are proposed to relax
the linearity assumption with various degrees of complexity. For
example, variable categorization is a simple and common practice in
epidemiology, but suffers from power and interpretation issues. More
complex models to address nonlinear effects include random forest and
other so-called ``black box'' models \citep{Breiman2001}. These models
are useful for statistical prediction but do not estimate parameters
relevant to the data generation process that one can draw inferences
from. In addition, how to generalize these models to the
high-dimensional setting remains unclear.

Nonparametric regression models are appropriate alternatives to the
``black-box'' models thanks to their balance between model flexibility
and interpretability. Among those, generalized additive models (GAMs),
proposed in the seminal work of Hastie and Tibshirani
\citep{Hastie1987}, grew to be one of the most popular modeling tools.
In a GAM, the response variable, \(Y\), which is assumed to follow some
exponential family distribution with mean \(\mu\) and dispersion
\(\phi\), can be modeled with the summation of smoothing functions,
\(B_j(\cdot), j = 1, \dots, p\), of a given \(p\)-dimensional vector of
covariates \(\boldsymbol{x}\), written as \[
 E(Y|\boldsymbol{x}) = g^{-1}(\beta_0 + \sum\limits^p_{j=1}B_j(x_j)),
\] where \(g^{-1}(\cdot)\) is the inverse of a monotonic link function.
The smoothing functions can take many forms and are estimated using a
pseudo-weighted version of the backfitting algorithm
\citep{Breiman1985}. Nevertheless, the classical GAMs cannot fulfill the
increasing analytic demands for high-dimensional data analysis in
biomedical studies.

There exists some proposals to generalize the classical GAM to
accommodate high-dimensional applications. The regularized models,
branching out from group regularized linear models, are used to fit GAMs
by accounting for the structure introduced when expanding smoothing
functions. Ravikumar et al. \citep{Ravikumar2009} extended the grouped
lasso \citep{Yuan2006} to additive models (AMs); Huang et al.
\citep{Huang2010} further developed adaptive grouped lasso for additive
models; Wang et al. \citep{Wang2007} and Xue \citep{Xue2009}
respectively applied grouped SCAD penalty \citep{Fan2001} to additive
models. Recent Bayesian hierarchical models are also used in the context
of high-dimensional additive models. Various group spike-and-slab priors
combining with computationally intensive Markov chain Monte Carlo (MCMC)
algorithms \citep{Xu2015, Yang2020} are proposed, where the application
on AMs are by-products. Bai et al. \citep{Bai2020} was the first to
apply group spike-and-slab lasso prior to Gaussian AMs using a fast
optimization algorithm, and further generalized the framework to
GAMs\citep{Bai2021}. Focus on addressing the sparsity, these methods can
overly penalize the basis function coefficients and produce inaccurate
predictions and curve interpolation, particularly when complex signals
are assumed and large number of knots are used. \citep{Scheipl2013} In
addition, these methods adapt an `all-in-all-out' strategy, i.e.~either
including or excluding the variable completely, rendering no space for
bi-level selection. Scheipl et al. \citep{Scheipl2012} proposed a
spike-and-slab structure prior, normal-mixture-of-inverse gamma prior,
that address the previous challenges. But the model fitting relies on
computational intensive MCMC algorithmic and creates scalability
concern. It would be of special interest to develop a fast, flexible and
accurate generalized additive model framework.

To address these challenges, we propose a novel Bayesian hierarchical
generalized additive model (BHAM) for high dimensional data analysis.
Specifically, we incorporate smoothing penalties in the model via
re-parameterization of the smoothing function to avoid overly shrinking
basis function coefficients. Smoothing penalties are commonly
implemented in the classical GAMs through smoothing regression splines.
They are quadratic norms of the coefficients and allow locally adaptive
penalties on each smoothing function. A smoothing penalty conditioning
on a smoothing parameter \(\lambda_j\) is a function of the integration
of the second derivative of the spline function, expressed
mathematically as \begin{equation}\label{eq:smoothpen}
  \text{pen}\left[B_j(x)\right] = \lambda_j \int B^{\prime\prime}_j(x)dx = \lambda_j \boldsymbol{\beta}_j^T \boldsymbol{S}_j \boldsymbol{\beta}_j ,
\end{equation} where \(\boldsymbol{S}_j\) is a known smoothing penalty
matrix, and \(\boldsymbol{\beta}_j\) are the basis function
coefficients. Smoothing penalties were also previously used in the
spike-and-slab GAM \citep{Scheipl2012} and the sparsity-smoothness
penalty \citep{Meier2009}. Moreover, incorporating the smoothing penalty
allows the separation of the linear space of smoothing functions from
the nonlinear space. We then impose a new bi-part spike-and-slab spline
prior on the smoothing functions for bi-level selection such that the
linear and nonlinear spaces of smoothing functions can be selected
separately. The prior setup encourages a flexible solution, rending one
of three possibilities: no effect, only linear effect, or nonlinear
effect of a predictor. In addition, two scalable optimization-based
algorithms are developed for high and
ultrahigh-dimensional\citep{Fan2009} settings respectively.

The proposed framework, BHAM, differs from previous spike-and-slab based
GAMs, i.e.~the spike-and-slab GAM \citep{Scheipl2012} and the SB-GAM
\citep{Bai2021} in three ways. First of all, the proposed spike-and-slab
spline prior is a spike-and-slab lasso type prior using independent
mixture double exponential distribution, compared to spike-and-slab GAM.
Spike-and-slab lasso priors provide computational convenience during
model fitting by using optimization algorithms instead of computational
intensive sampling algorithms. They make fitting high- and
ultrahigh-dimensional models more feasible without sacrificing
performance in prediction and variable selection. Secondly, the
optimization algorithms introduced in this article include EM-Coordinate
Descent (EM-CD) algorithm and EM-Iterative weighted least square
(EM-IWLS) algorithm for different utilities. The applications of the two
aforementioned algorithms are fruitful in Bayesian hierarchical models
for high-dimensional data. \citep{Yi2012, Rockova2014a, Rockova2018}
Even though SB-GAM with a group spike-and-slab lasso prior also uses
EM-CD for model fitting, the proposed EM-IWLS provides
variance-covariance matrix of the coefficients, which has the potential
to provide uncertainty inference. EM-IWLS complements the lack of
inference potential from EM-CD in a median and high dimensional setting.
Furthermore, the proposed model addresses the incapability of bi-level
selection in SB-GAM. Lastly, the R implementation of the Bayesian
hierarchical additive model is available publicly via BHAM at
\url{https://github.com/boyiguo1/BHAM}, making translational science
more accessible.

In Section \ref{sec:BHAM}, we establish the Bayesian hierarchical
generalized additive model, introduce the proposed spike-and-slab spline
priors, and describe the two fast-fitting algorithms. In Section
\ref{sec:sim}, we compare the proposed framework to state-of-the-art
models, mgcv, COSSO and sparse Bayesian GAM via Monte Carlo simulation
studies. Analyses of two metabolomics datasets are presented in Section
\ref{sec:real_data}. Conclusion and discussions are given in Section
\ref{sec:concl}.

\hypertarget{bayesian-hierarchical-additive-models-bhams}{%
\section{Bayesian Hierarchical Additive Models
(BHAMs)}\label{bayesian-hierarchical-additive-models-bhams}}

\label{sec:BHAM}

Following the GLM notation introduced in Section \ref{sec:intro}, we
have a generalized additive model with link function \(g(\cdot)\) and
linear predictor \begin{equation}\label{eq:gam}
\eta = \beta_0 + \sum\limits^p_{j=1}B_j(x_j) = \beta_0 + \sum\limits^p_{j=1} \boldsymbol{\beta}_j^T \boldsymbol{X}_j,
\end{equation} with smoothing functions \(B_j(x_j)\) of the variable
\(x_j, j = 1, \dots, p.\) The outcome \(Y\) follows a exponential family
distribution with density function \(f(y)\), mean \(\mu = g^{-1}(\eta)\)
and dispersion parameter \(\phi\), and the data distribution is
\begin{equation}
f(\boldsymbol{Y} = \boldsymbol{y}| \boldsymbol{\beta}, \phi) = \prod\limits^n_{i=1}f( Y = y_i|\boldsymbol{\beta}, \phi),\nonumber
\end{equation} The basis function matrix, i.e.~the design matrix derived
from the smoothing function \(B_j(x_j)\), is denoted
\(\boldsymbol{X}_j\) for the variable \(x_j\). The dimension of the
design matrix depends on the choice of the smoothing function. For
example, a cubic spline function with \(k\) knots produces a \(k+3\)
elements column vector. Its corresponding smoothing penalty matrix is
denoted as \(\boldsymbol{S}_j\). \(\boldsymbol{\beta}_j\) denotes the
basis function coefficients for the \(j\)th variable such that
\(B_j(x_j) = \boldsymbol{\beta}_j^T \boldsymbol{X}_j\). With slight
abuse of notation, we denote vectors and matrices in bold fonts
\(\boldsymbol{\beta}, \boldsymbol{X}\) with conformable dimensions,
where scalar and random variables are denoted in with unbold fonts
\(\beta, X\). The matrix transposing operation is denoted with a
superscript \(^T\). To note, the proposed model can include parametric
forms of variables in the model, treating the parametric function as a
special case of the smoothing function, e.g.~\(B_j(x_j) = x_j\) with the
smoothing penalty matrix defined as
\(\boldsymbol{S}_j = \begin{bmatrix}0\end{bmatrix}\).

To encourage proper smoothing of the functions, we adopt the idea of
smoothing penalties from smoothing spline models. The basic idea is to
set up a smoothing penalty, described in Equation (\ref{eq:smoothpen}),
in the prior density function. However, the direct integration of
smoothing penalty with sparsity penalty is not obvious. Marra and Wood
\citep{Marra2011} proposed a re-parameterization procedure to
accommodate the smoothing penalty implicitly. Given the smoothing
penalty matrix \(\boldsymbol{S}_j\) is symmetric and positive
semi-definite for univariate smoothing functions, we apply
eigendecomposition on the penalty matrix
\(\boldsymbol{S} = \boldsymbol{U} \boldsymbol{D} \boldsymbol{U}^T\) ,
where the matrix \(\boldsymbol{D}\) is diagonal with the eigenvalues
arranged in the ascending order. To note, \(\boldsymbol{D}\) can contain
elements of zeros on the diagonal, where the zeros are associated with
the linear space of the smoothing function. For the most popular
smoothing function, cubic splines, the dimension of the linear space is
one. Hereafter, we focus on discussing a uni-dimensional linear space
for simplicity; however, it generalizes easily to the cases where the
linear space is multidimensional. We further write the orthonormal
matrix
\(\boldsymbol{U} \equiv \begin{bmatrix} \boldsymbol{U}^0 : \boldsymbol{U}^{*}\end{bmatrix}\)
containing the eigenvectors as columns in the corresponding order to
\(\boldsymbol{D}\). That is, \(\boldsymbol{U}\) contains the
eigenvectors \(U^0\) with zero eigenvalues for the linear space and
\(\boldsymbol{U}^{*}\) contains the eigenvectors (as columns) for the
non-zero eigenvalues, i.e.~the non-linear space. We multiply the basis
function matrix \(\boldsymbol{X}\) with the orthonormal matrix
\(\boldsymbol{U}\) for the new design matrix
\({\boldsymbol{X}}^\text{repa}= \boldsymbol{X} \boldsymbol{U} \equiv \begin{bmatrix} X^0 : \boldsymbol{X}^{*} \end{bmatrix}\).
An additional scaling step is imposed on \(\boldsymbol{X}^{*}\) by the
non-zero eigenvalues of \(\boldsymbol{D}\) such that the new basis
function matrix \(\boldsymbol{X}^\ast\) can receive uniform penalty on
each of its dimensions. With slight abuse of the notation, we drop the
superscript \(^\text{repa}\) and denote
\(\boldsymbol{X}_j \equiv \begin{bmatrix} X_j^0 : \boldsymbol{X}_j^{*} \end{bmatrix}\)
as the basis function matrix for the \(j\)th variable after the
re-parameterization. A spline function can be expressed in the matrix
form \[
B_j(x_j) = B_j^0(x_j) + B_j^*(x_j) = \beta_j X^0_j + \boldsymbol{\beta_j^*}^T \boldsymbol{X}_j^*,
\] and the generalized additive model in Equation (\ref{eq:gam}) be
\begin{equation}\label{eq:gam-repa}
E(Y|\boldsymbol{x}) = g^{-1}(\beta_0 + \sum\limits^p_{j=1} B_j(x_j)) = g^{-1}(\beta_0 + \sum\limits^p_{j=1} \boldsymbol{\beta}_j^T \boldsymbol{X}_j) = g^{-1}\left[\beta_0 + \sum\limits^p_{j=1} (\beta_j X^0_j + {\boldsymbol{\beta}_j^*}^T \boldsymbol{X}_j^*)\right],
\end{equation} where the coefficients
\(\boldsymbol{\beta}_j \equiv \begin{bmatrix} \beta_j : \boldsymbol{\beta}^*_j \end{bmatrix}\)
is an augmentation of the coefficient scalar \(\beta_j\) of linear space
and the coefficient vector \(\boldsymbol{\beta}^*_j\) of non-linear
space.

The re-parameterization step sets up the foundation of the proposed
model and provides three benefits. First of all, the re-parameterization
integrates the smoothing penalty into the design matrix, and encourages
models to properly smooth the nonlinear function in addition to the
sparse penalty for function selection. Secondly, the eigendecomposition
of the smoothing penalty allows the isolation of the linear from the
nonlinear space, improving the feasibility of bi-level function
selection and inference. The eigendecomposition facilitates the
construction of orthonormal design matrix, which makes imposing
independent priors on the coefficients possible. This reduces the
computational complexity compared to using a multivariate priors, and
greatly broadens the choices of priors and further model choices. Last
but not least, the scaling step of \(\boldsymbol{X}^*\) further
simplifies the choice of priors, from column-specific priors to a
unified prior.

\hypertarget{spike-and-slab-spline-priors}{%
\subsection{Spike-and-Slab Spline
Priors}\label{spike-and-slab-spline-priors}}

The family of spike-and-slab (SS) priors regression models is one of
most commonly used models in high-dimensional data analysis for its
utility in outcome prediction and variable selection. We defer to Bai et
al. \citep{Bai2021Review} for an in-depth introduction to spike-and-slab
priors. To summarize, spike-and-slab priors are a family of mixture
distributions that comprises a skinny spike density
\(f_{\text{spike}}(\cdot)\) for weak signals and a flat slab density
\(f_{\text{slab}}(\cdot)\) for strong signals, mathematically \[
 \beta|\gamma \sim (1-\gamma)f_{\text{spike}}(\beta) + \gamma f_{\text{slab}}(\beta).
\] The most distinct feature of SS priors is that it is conditioned on a
latent binary variable \(\gamma \in \{0,1\}\) that indicates whether the
variable \(x\) is included in the model. There are various
spike-and-slab priors depending on the choice for the spike density
\(f_{\text{spike}}(\cdot)\) and the slab density
\(f_{\text{slab}}(\cdot)\), see George and McCulloch
\citep{George1993, George1997}; Chipman \citep{Chipman1996} for grouped
variables; Brown et al. \citep{Brown1998} for multivariate outcomes;
Ishwaran and Rao \citep{Ishwaran2005}; Clyde and George
\citep{Clyde2004} and reference therein. Two of most popular spike and
slab priors are spike-and-slab normal prior \citep{George1993} and
spike-and-slab double exponential prior\citep{Rockova2018}.

The major criticism of early spike-and-slab models is being
computationally prohibitive. \citep{Bai2021Review} Since then, many
studies focus on alleviating the computational burden that sampling
algorithms bear, which include EMVS based on spike-and-slab normal
prior\citep{Rockova2014a} and spike-and-slab Lasso
\citep{Rockova2018b, Rockova2018}. Particularly, the development of
spike-and-slab Lasso substantially improves the scalability of SS
models, setting up the theoretical foundation for generalized models in
-omics data analysis \citep{Tang2017a, Tang2017, Tang2018, Tang2019}.
The spike-and-slab Lasso (SSL) prior is composed of two double
exponential distributions with mean 0 and different dispersion
parameters, \(0 < s_0 < s_1\), mathematically, \begin{equation} 
\beta | \gamma \sim (1-\gamma)DE(0, s_0) + \gamma DE(0, s_1), 0 < s_0 < s_1.\nonumber
\end{equation} Given that both double exponential distributions have a
mean of 0 and the latent indicator \(\gamma\) can only take the value of
0 or 1, the mixture double exponential distribution can be formulated as
one single double exponential density, \begin{equation} \label{eq:ssl}
\beta | \gamma \sim DE(0, S), 0 < s_0 < s_1,
\end{equation} with the scale parameter
\(S = (1-\gamma)s_0 + \gamma s_1\). The SSL also mitigates the problem
of EMVS where the weak signals are not shrink to zero, and hence is
preferred in high-dimensional data analysis. We notice that Bai
\citep{Bai2021} is the first to apply spike-and-slab lasso prior in the
GAM framework, where the densities of the spike and slab components take
the group lasso density \citep{Xu2015}. Bai \citep{Bai2021} takes an
``all-in-all-out'' strategy for function selection.

\hypertarget{two-part-spike-and-slab-lasso-prior}{%
\subsubsection{Two-part Spike-and-Slab Lasso
Prior}\label{two-part-spike-and-slab-lasso-prior}}

We introduce a novel prior for GAMs, particularly for high-dimensional
nonlinear modeling with bi-level selection. The proposed prior extends
from the spike-and-slab lasso prior described in Equation
(\ref{eq:ssl}). Given the re-parameterized design matrix
\(\boldsymbol{X}_j = \begin{bmatrix} X^0_j : \boldsymbol{X}_j^*\end{bmatrix}\)
for the \(j\)th variable, we impose a two-part SSL prior to the
coefficients
\(\boldsymbol{\beta}_j = \begin{bmatrix} \beta_j : \boldsymbol{\beta}_j^*\end{bmatrix}\).
Specifically, we impose independent group priors on the linear space
coefficients and on the nonlinear space coefficients respectively,
\begin{align}\label{eq:bham_ssl}
  \beta_{j} |\gamma_{j},s_0,s_1 &\sim DE(0,(1-\gamma_{j}) s_0 + \gamma_{j} s_1) \nonumber \\
  \beta^*_{jk} | \gamma^*_{j},s_0,s_1 &\overset{\text{iid}}{\sim}DE(0,(1-\gamma^*_{j}) s_0 + \gamma^*_{j} s_1), k=1,\dots, K_j
\end{align} where \(\gamma_{j}\in\{0,1\}\) and
\(\gamma^*_{j}\in \{0,1\}\) are two latent indicator variables,
indicating if the model includes the linear effect and the nonlinear
effect of the \(j\)th variable respectively. \(s_0\) and \(s_1\) are
scale parameters, assuming \(0 < s_0 < s_1\) and given. These scale
parameters \(s_0\) and \(s_1\) can be treated as tuning parameters and
optimized via cross-validation. A discussion of how to choose the scale
parameters comes in Section \ref{sec:tune}. To note, this prior differs
from previous group spike-and-slab lasso priors
\citep{Tang2018, Tang2019}, as the \(\beta_j\) and
\(\boldsymbol{\beta}^*_j\) have different indicator variables
\(\gamma_j\), \(\gamma_j^*\) respectively. It is possible to add a more
restrictive assumption on the priors, assuming that one indicator
variable decides the inclusion of both the linear effect and nonlinear
effect, i.e.~\(\gamma_j = \gamma^*_j\). This converges to the SB-GAM
\citep{Bai2021}. Conversely, it is also possible to relax the assumption
such that each coefficient \(\beta_{jk} \in \boldsymbol{\beta}^*_j\) has
its own latent indicator \(\gamma_{jk}\), but at the cost of
complicating the bi-level function selection. This reduces the proposed
prior to the classic spike-and-slab lasso prior.

The re-parameterization introduced in Section \ref{sec:BHAM} grants the
validity of the proposed prior. First of all, the smoothing function
bases are linear dependent and necessitate extra attention. The
eigeondecomposition remedies the problem and hence our prior can be set
to be conditionally independent. Secondly, the eigenvalue scaling
provides a panacea to allow unified scale parameters for all bases of
all smoothing functions of variables.

The rest of the hierarchical prior follows the traditional
spike-and-slab lasso prior: we set up hyper-priors on
\(\gamma_j, \gamma^*_j\) to allow local adaption of the shrinkage using
a Bernoulli distribution, written as binomial distribution of one trial.
The two indicators of the \(j\)th predictor, \(\gamma_j\) and
\(\gamma^*_j\), shares the same probability parameter \(\theta_j\), \[
\begin{aligned}
&\gamma_{j} | \theta_j \sim Bin(1, \theta_j) & & 
&\gamma_{j}^*| \theta_j \sim Bin(1, \theta_j).
\end{aligned}
\] This is to leverage the fact that the probability of selecting the
bases of a smoothing function should be similar, while allowing
different penalty on the linear space and non-linear space of the
smoothing function. The hyper prior of \(\gamma_{j}\) decides the
sparsity of the model at the function selection level, while that of
\(\gamma_{j}^*\) decides the smoothness of the spline function at basis
function level. Meanwhile, we specify that \(\gamma_{j}\) and
\(\gamma_{j}^*\) are independently distributed for analytic simplicity.
We further specify the parameter \(\theta_j\) follows a beta
distribution with given shape parameters \(a\) and \(b\), \[
\theta_j \sim Beta(a, b).
\] The beta distribution is a conjugate prior for the binomial
distribution and hence provides some computation convenience. For
simplicity, we focus on a special case of beta distribution, uniform
(0,1), i.e.~\(a = 1, b = 1\). When the variable have large effects in
any of the bases, the parameter \(\theta_j\) will be estimated large,
which in turn encourages the model to include the rest of bases,
achieving the local adaption among spline bases. Hereafter, we refer
Bayesian hierarchical generalized additive models with the
spike-and-slab lasso prior as the BHAM-SSL, and visually presented in
Figure \ref{fig:SSprior}.

\begin{center}
Figure \ref{fig:SSprior} here
\end{center}

\hypertarget{other-priors}{%
\subsubsection{Other Priors}\label{other-priors}}

With the re-parameterization step of the basis function matrix
\(\boldsymbol{X}\), it is possible to generalized the SSL prior to other
priors, for example normal priors for ridge-type regularization and
mixture normal prior for spike-and-slab regularization. These priors
would work better in low and medium dimensional settings where the
sparse assumption is not necessary. Here we elaborate the mixture normal
prior as a demonstration of applying continuous spike-and-slab prior in
BHAM.

A spike-and-slab mixture normal spline prior can be expressed as
\begin{align*}
  \beta_{j} |\gamma_{j},s_0,s_1 &\sim N(0,(1-\gamma_{j}) s_0 + \gamma_{j} s_1)\\
  \beta^*_{jk} | \gamma^*_{j},s_0,s_1 &\overset{\text{iid}}{\sim}N(0,(1-\gamma^*_{j}) s_0 + \gamma^*_{j} s_1), k=1,\dots, K_j.
\end{align*} Similar to the BHAM-SSL prior in Equation
(\ref{eq:bham_ssl}), \(0 < s_0 < s_1\) are tuning parameters and can be
optimized via cross-validation. One of the critics received by the
spike-and-slab mixture normal prior is that the tails of a normal
distribution diminishes to zero too fast, which causes problems when
estimating the large effects. Distributions with heavier tails can be
used as an alternative, for example mixture Student's \(t\) distribution
with small degree of freedom.

\hypertarget{algorithms-for-fitting-bhams}{%
\subsection{Algorithms for Fitting
BHAMs}\label{algorithms-for-fitting-bhams}}

The proposed models can be fitted with MCMC algorithms. Nevertheless,
the computational burden of MCMC algorithms creates scalability issues.
George and McCulloch\citep{George1997} examined the computation speed
for various MCMC algorithms with spike-and-slab mixture normal priors,
and suggested MCMC algorithms works well for medium size (\(p\)=25) of
predictors with only linear effects. However, it is not feasible for
high-dimensional data analysis where the number of predictors easily
exceeds 100. Specific to additive models, each predictor would expand to
multiple new ``predictors'' via basis functions, creating greater
computational demands. Scheipl et al. \citep{Scheipl2013} demonstrated
the computational demands of a MCMC algorithm for fitting spike-and-slab
GAM grow exponentially as \(p\) increases modestly via simulation
studies. Hence, we feel compelled to develop scalable algorithms for
fitting Bayesian hierarchical additive models in high-dimensional
settings.

As alternatives to the sampling algorithms for fitting Bayesian models,
optimization algorithms focus on the maximum a posteriori (MAP)
estimates and speed up the model fitting process at the cost of
uncertainty measures. The earlier work for fitting spike-and-slab models
using optimization algorithms includes EMVS \citep{Rockova2014a}.
Rockova and George\citep{Rockova2014a} proposed an
expectation-maximization (EM) based algorithm to fit models that use
continuous mixture normal priors. In the E step, the latent binary
indicators \(\gamma\)s are treated as the missing data, and the
posterior means are calculated conditioning on the current value of
other parameters; in the M step, a ridge estimator was used to update
the coefficients, followed by updating \(\phi, \theta\). The same
authors \citep{Rockova2018b, Rockova2018} further combined the EM
algorithm with coordinate descent algorithm to fit SSL models. Yi and
his group independently developed the EM-Iterative Weighted Least Square
and EM-cyclic coordinate descent algorithms to fit models with broader
class of priors, SSL included.\citep{Yi2019} These algorithms were
implemented for generalized linear models\citep{Tang2017a}, Cox
proportional hazards models \citep{Tang2017} and their grouped
counterparts\citep{Tang2018, Tang2019}. Both EM based algorithms provide
deterministic solutions, which becomes a popular property for
reproducible research

In this section, we extend the two EM-based algorithms, EM-CD and
EM-IWLS algorithms, to fit BHAMs. To note, the two proposed algorithms
provides different utilities. The EM-CD algorithm is specifically for
fitting BHAM-SSL with an expedited performance, recommending to use in
high and ultra-high dimensional setting. A specific concern of EM-CD
algorithm is that it provides no information for inference. In contrast,
the EM-IWLS can estimate the variance-covariance matrix of the
coefficients. Moreover, the EM-IWLS is a more general model fitting
algorithm that can be used for fitting not only SSL and continuous SS
priors but also Student's t-priors and double exponential priors. To
note, SB-GAM\citep{Bai2020, Bai2021} also used an EM-CD algorithm. The
main difference between proposed EM-CD algorithm and that in SB-GAM is
that SB-GAM uses a blocked CD algorithm for their group prior, while the
proposed prior is pairwise independent requiring no special treatment in
the CD algorithm.

\hypertarget{em-algorithms}{%
\subsubsection{EM algorithms}\label{em-algorithms}}

EM algorithm is an iterative algorithm to find MAP estimates or the
maximum likelihood estimates. It is commonly used when some necessary
data to establish the likelihood function are missing. Instead of
maximizing the the likelihood function, the algorithm maximizes the
expectation of the likelihood function with respect to the ``missing''
data.

The recursive algorithm consists of two steps:

\begin{itemize}
\tightlist
\item
  E-step: to calculate the expectation of the posterior density function
  with respect to some ``missing'' data
\item
  M-step: to maximize the expectation derived in the E-step to calculate
  the parameter of interest
\end{itemize}

For BHAMs, we define the parameters of interest as
\(\Theta = \{\boldsymbol{\beta}, \boldsymbol{\theta}, \phi\}\) and
consider the latent binary indicators \(\boldsymbol{\gamma}\) as
nuisance parameters of the model, in other words the ``missing'' data.
Our objective is to find the parameters \(\Theta\) that maximize the
posterior density function, or equivalently, the logarithm of the
density function, \[
\begin{aligned}
& \text{argmax}_{\Theta}
\log f(\Theta, \boldsymbol{\gamma}| \textbf{y}, \textbf{X}) \\
&= \log f(\textbf{y}|\boldsymbol{\beta}, \phi) + \sum\limits_{j=1}^p\left[\log f(\beta_j|\gamma_j)+\sum\limits_{k=1}^{K_j} \log f(\beta^{*}_{jk}|\gamma^{*}_{j})\right]\\
& +\sum\limits_{j=1}^{p} \left[ (\gamma_j+\gamma_{j}^{*})\log \theta_j + (2-\gamma_j-\gamma_{j}^{*}) \log (1-\theta_j)\right] +  \sum\limits_{j=1}^{p}\log f(\theta_j),
\end{aligned}
\]\\
where \(f(\textbf{y}|\boldsymbol{\beta}, \phi)\) is the data
distribution and \(f(\theta)\) is the Beta(1,1) density. We choose
non-informative prior for the intercept \(\beta_0\) and the dispersion
parameter \(\phi\); for example, \(f(\beta_0|\tau_0^2)=N(0,\tau_0^2)\)
with \(\tau^2_0\) set to a large value and \(f(\log \phi) \propto 1\).

We use the EM algorithm to find the MAP estimate of \(\Theta\). Since
the latent binary indicators \(\boldsymbol{\gamma}\) are not of our
primary interest, we treat them as the ``missing data'' in the EM
algorithm. Naturally, in the E-step, we calculate the expectation of
posterior density function of
\(\log f(\Theta, \boldsymbol{\gamma}| \textbf{y}, \textbf{X})\) with
respect to the latent indicators \(\boldsymbol{\gamma}\) conditioning on
the values from previous iteration \(\Theta^{(t-1)}\), \[
E_{\boldsymbol{\gamma}|\Theta^{(t-1)}}\log f(\Theta, \boldsymbol{\gamma}| \textbf{y}, \textbf{X}) .
\] Hereafter, we use the shorthand notation
\(E(\cdot)\equiv E_{\boldsymbol{\gamma}|\Theta^{(t-1)}}(\cdot)\). In the
M-step, we find the \(\Theta^{(t)}\) that maximize
\(E\log f(\Theta, \boldsymbol{\gamma}| \textbf{y}, \textbf{X})\). The E-
and M- steps are iterated until the algorithm converge.

To note here, the log-posterior density of BHAMs (up to additive
constants) can be written as a two-part equation
\[ \log f(\Theta, \boldsymbol{\gamma}| \textbf{y}, \textbf{X}) = Q_1(\boldsymbol{\beta}, \phi) + Q_2 (\boldsymbol{\gamma},\boldsymbol{\theta}),\]
where
\[ Q_1\equiv Q_1(\boldsymbol{\beta}, \phi) = \log f(\textbf{y}|\boldsymbol{\beta}, \phi) + \sum\limits_{j=1}^p\left[\log f(\beta_j|\gamma_j)+\sum\limits_{k=1}^{K_j} \log f(\beta^{*}_{jk}|\gamma^{*}_{jk})\right]\]
and
\[Q_2 \equiv Q_2(\boldsymbol{\gamma},\boldsymbol{\theta}) = \sum\limits_{j=1}^{p} \left[ (\gamma_j+\gamma_{j}^{*})\log \theta_j + (2-\gamma_j-\gamma_{j}^{*}) \log (1-\theta_j)\right] +  \sum\limits_{j=1}^{p}\log f(\theta_j).\]
\(Q_1\) and \(Q_2\) are respectively the log posterior density of the
coefficients \(\boldsymbol{\beta}\) and the log posterior density of the
probability parameters \(\boldsymbol{\theta}\) conditioning on
\(\boldsymbol{\gamma}\). Meanwhile, conditioning on
\(\boldsymbol{\gamma}\), \(Q_1\) and \(Q_2\) are independent and can be
maximized separately for \(\boldsymbol{\beta}, \phi\) and
\(\boldsymbol{\theta}\). Depending on the choice of coefficient priors,
\(Q_1\) can be treated as penalized likelihood function and maximization
of \(E(Q_1)\) can be solved via CD algorithm or IWLS algorithm in each
iteration. Maximization of \(E(Q_2)\) can be solved via closed form
equations following the beta-binomial conjugate relationship.

\hypertarget{em-coordinate-descent}{%
\subsubsection{EM-Coordinate Descent}\label{em-coordinate-descent}}

\label{sec:EMCD}

When the prior distribution of the coefficients is set to mixture double
exponential, coordinate descent algorithm can be used to estimate the
parameters in the M-step. Coordinate descent is an optimization
algorithm that offers extreme computational advantage, and famous for
its application in optimizing the \(l_1\) penalized likelihood function.

The density function of spike-and-slab mixture double exponential prior
can be written as \[
f(\beta|\gamma, s_0, s_1) = \frac{1}{2\left[(1-\gamma)s_0 + \gamma s_1\right]}\exp(-\frac{|\beta|}{(1-\gamma)s_0 + \gamma s_1}),
\] and \(E(Q_1)\) can be expressed as a likelihood function with \(l_1\)
penalty \begin{equation}\label{eq:Q1_CD}
E(Q_1) = \log f(\textbf{y}|\boldsymbol{\beta}, \phi) - \sum\limits_{j=1}^p\left[E({S_j}^{-1})|\beta_j|+\sum\limits_{k=1}^{K_j}E({S^{*}}^{-1}_{j})|\beta_{jk}|\right],
\end{equation} where
\(S_{j} = (1-\gamma^{0}_{j}) s_0 + \gamma^{0}_{j} s_1\) and
\(S^*_{j} = (1-\gamma^*_{j}) s_0 + \gamma^*_{j} s_1\). To calculate two
unknown quantities \(E({S_j}^{-1})\) and \(E({S^*}^{-1}_j)\), the
posterior probability
\(p_{j} \equiv \text{Pr}(\gamma^{0}_{j}=1|\Theta^{(t-1)})\) and
\(p_{j}^*\equiv \text{Pr}(\gamma^*_{j}=1|\Theta^{(t-1)})\) are
necessary, which can be derived via Bayes' theorem. The calculation of
\(p_j^*\) is slightly different from that of \(p_j\), as \(p_j^*\)
depends on the value of the vector \(\boldsymbol{\beta}^*_{j}\) and
\(p_j\) only depends on the scalar \(\beta_j\). The calculation follows
the equations below. \begin{align*}
p_{j} &= \frac{\text{Pr}(\gamma_{j} = 1|\theta_j)f(\beta_{j}|\gamma_{j}=1, s_1) }{\text{Pr}(\gamma_{j} = 1|\theta_j)f(\beta_{j}|\gamma_{j}=1, s_1) + \text{Pr}(\gamma_{j} = 0|\theta_j)f(\beta_{j}|\gamma_{j}=0, s_0)}\\
p^*_{j} &= \frac{\text{Pr}(\gamma^{*}_{j} = 1|\theta_j)\prod\limits_{k=1}^{K_j}f(\beta_{jk}|\gamma^{*}_{j}=1, s_1) }{\text{Pr}(\gamma^{*}_{j} = 1|\theta_j)\prod\limits_{k=1}^{K_j}f(\beta_{jk}|\gamma^{*}_{j}=1, s_1) + \text{Pr}(\gamma^{*}_{j} = 0|\theta_j)\prod\limits_{k=1}^{K_j}f(\beta_{jk}|\gamma^{*}_{j}=0, s_0)}
\end{align*} and
\(\text{Pr}(\gamma_{j}^{0} = 1|\theta_j) = \text{Pr}(\gamma_{j}^*= 1|\theta_j) = \theta_j\),
\(\text{Pr}(\gamma_{j}^{0} = 0|\theta_j) = \text{Pr}(\gamma_{j}^*= 0|\theta_j) = 1-\theta_j\),
\(f(\beta|\gamma=1, s_1) = \text{DE}(\beta|0 , s_1)\),
\(f(\beta|\gamma=0, s_0) = \text{DE}(\beta|0 , s_0)\). It is trivial to
show \begin{align}\label{eq:exp_scale}
&E(\gamma_{j})  = p_{j} & &E(\gamma^{*}_{j}) = p_{j}^{*}\nonumber\\
&E({S}^{-1}_{j}) = \frac{1-p_{j}}{s_0} + \frac{p_{j}}{s_1} & &E({S^*}^{-1}_{j}) = \frac{1-p_{j}^{*}}{s_0} + \frac{p_{j}^{*}}{s_1}.
\end{align} After replacing withe calculated quantities, \(E(Q_1)\) can
be seen as a \(l_1\) penalized likelihood function with the
regularization parameter \(\lambda = E(S^{-1})\), and hence be optimized
via coordinate descent algorithm \citep{Friedman2010}. Independently,
the remaining parameters of interest \(\boldsymbol{\theta}\) can be
updated by maximizing \(E(Q_2)\). As the beta distribution is a
conjugate prior for Bernoulli distribution, \(\boldsymbol{\theta}\) can
be easily updated with a closed form equation,
\begin{equation}\label{eq:update_theta}
\theta_j = \frac{p_j + p^*_{j} + a - 1 }{a + b}.
\end{equation}

Totally, the proposed EM-coordinate descent algorithm is summarized as
follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Choose a starting value \(\boldsymbol{\beta}^{(0)}\) and
  \(\boldsymbol{\theta}^{(0)}\) for \(\boldsymbol{\beta}\) and
  \(\boldsymbol{\theta}\). For example, we can initialize
  \(\boldsymbol{\beta}^{(0)} = \boldsymbol{0}\) and
  \(\boldsymbol{\theta}^{(0)} = \boldsymbol{0}.5\)
\item
  Iterate over the E-step and M-step until convergence

  E-step: calculate \(E(\gamma_{j})\), \(E(\gamma^*_{j})\) and
  \(E({S}^{-1}_{j})\), \(E({S^*}^{-1}_{j})\) with estimates of
  \(\Theta^{(t-1)}\) from previous iteration

  M-step:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \item
    Update \(\boldsymbol{\beta}^{(t)}\), and the dispersion parameter
    \(\phi^{(t)}\) if exists, using the coordinate descent algorithm
    with the penalized likelihood function in Equation (\ref{eq:Q1_CD})
  \item
    Update \(\boldsymbol{\theta}^{(t)}\) using Equation
    (\ref{eq:update_theta})
  \end{enumerate}
\end{enumerate}

We assess convergence by the criterion:
\(|d^{(t)}-d^{(t-1)}|/(0.1+|d^{(t)}|)<\epsilon\), where
\(d^{(t)} = -2\log f(\textbf{y}| \textbf{X}, \boldsymbol{\beta}^{(t)},\phi^{(t)})\)
is the estimate of deviance at the \(t\)th iteration, and \(\epsilon\)
is a small value (say \(10^{-5}\)).

\hypertarget{em-iwls}{%
\subsubsection{EM-IWLS}\label{em-iwls}}

Similar to the EM-CD algorithm, the EM-IWLS algorithm is an iterative
EM-based algorithm where the iterative weighted least squares algorithm
is used to find the estimate of \(\boldsymbol{\beta}, \phi\) that
maximizes \(E(Q_1)\). The iterative weighted least squares algorithm was
originally proposed to fit the classical generalized linear models, and
generalized to fit some Bayesian hierarchical models.\citep{Gelman2013}
Yi and Ma \citep{Yi2012} formulated Student's t-distribution and double
exponential distribution as hierarchical normal distributions such that
generalized linear models with shrinkage priors can be easily fitted
using IWLS in combination with EM algorithm. In this work, we adapt the
EM-IWLS paradigm to fit BHAM with spike-and-slab spline prior .

A double exponential prior, \(\beta|S \sim DE(0, S)\) can be formulated
as a hierarchical normal prior with unknown variance \(\tau^2\)
integrated out: \begin{align*}
  \beta|\tau^2 &\sim N(0, \tau^2)\\
  \tau^2|S & \sim Gamma(1, 1/(2S^2)), 
\end{align*} For the mixture double exponential priors, we can define
the scale parameter \(S = (1-\gamma)s_0 + \gamma s_1\) following
Equation (\ref{eq:ssl}). The change in the prior formulation in turn
leads to the change in the log posterior density function, as \(Q_1\)
needs to account for the hyperprior of \(\tau^2\):
\begin{equation}\label{eq:Q1_IWLS}
Q_1(\boldsymbol{\beta}, \phi) = \log f(\textbf{y}|\boldsymbol{\beta}, \phi) + \sum\limits_{j=1}^p\left[\log f(\beta_j|{\tau}^2_j) + \log f({\tau}^2_j| S_j)+\sum\limits_{k=1}^{K_j} \{\log f(\beta^{*}_{jk}|{\tau^{*}}^2_{jk})+\log f({\tau^*}^2_{jk}| S^*_j)\}\right].
\end{equation} Since \(\boldsymbol{\tau}^2\) are not of our primary
interest, we treat them as the ``missing'' data in addition to the
latent indicators \(\boldsymbol{\gamma}\), and hence construct the
expectation
\(E_{\boldsymbol{\gamma}, \boldsymbol{\tau}^2|\Theta^{(t-1)}}(Q_1)\) in
the E-step. To note, unlike the same latent indicator \(\gamma^*_j\)
which is shared by the coefficients of the non-linear terms
\(\beta^*_{jk}\) for \(k = 1, \dots, K_j\) , \(\tau^2_{jk}\) is
coefficient specific for \(\beta^*_{jk}\).
\(E({S_j}^{-1}|\beta_j, s_0, s_1), E({S^*}^{-1}_j|\boldsymbol{\beta}_j^*, s_0, s_1), E({\tau}^2_{j}|S_j, \beta_j) \text{ and } E({\tau^*}^2_{jk}|S_j^*, \beta^*_{jk})\)
needs to be calculated to formulate \(E(Q_1)\). As neither
\(E({S_j}^{-1}|\beta_j, s_0, s_1)\) nor
\(E({S^*}^{-1}_j|\boldsymbol{\beta}_j^*, s_0, s_1)\) depends on
\(\tau^2\)s, they can be derived using Equation (\ref{eq:exp_scale}). On
the other hand, \(\tau^{2}\) follows a gamma distribution is a conjugate
prior for the normal variance, and the conditional posterior density of
\(\tau^{-2}\) is an inverse Gaussian distribution.
\(E({\tau}^{-2}_{j})\) and \(E({\tau^*}^{-2}_{jk})\) are calculated
using the closed form equation \begin{align*}
 E({\tau}^{-2}_{j}|S_j, \beta_j) ={S_j}^{-1}/|\beta_j| \qquad E({\tau^*}^{-2}_{jk}|S_j^*, \beta^*_{jk})={S_j^*}^{-1}/|\beta^*_{jk}|,
\end{align*} where \(S_j\) and \(S_j^*\) are replaced by the expectation
and \(\beta\)s are replaced with \(\beta^{(t-1)}\). With simplification
(up to constant additive terms), we have
\begin{equation}\label{eq:EQ1_IWLS}
E(Q_1) = \log f(\textbf{y}|\boldsymbol{\beta}, \phi) - \sum\limits_{j=1}^p\left[ {2E({\tau_j}^{-2})}{\beta_j}^2 +\sum\limits_{k=1}^{K_j} {2E({\tau_{jk}^*}^{-2})}{\beta_{jk}^*}^2\right].
\end{equation} \(2E({\tau}^{-2})\beta^2\) can be seen as the kernel of a
normal density with mean 0 and variance \(E(\tau^{2})\), and we can
formulate the coefficients \(\boldsymbol{\beta}\) as a multivariate
normal distribution with means \(\boldsymbol{0}\) and variance
covariance matrix \(\boldsymbol{\Sigma}_{\tau^2}\), where
\(\boldsymbol{\Sigma}_{\tau^2}\) is a diagonal matrix with
\(E(\tau^2)\)s on the diagonal, \[
\boldsymbol{\beta }\sim \text{MVN}(0, \boldsymbol{\Sigma}_{\tau^2}).
\]

Meanwhile, following the classical IWLS, we can approximate the
generalized model likelihood at each iteration with a weighted normal
likelihood: \[
f(\textbf{y}|\boldsymbol{\beta}, \phi) \approx \text{MVN}(\textbf{z}|\boldsymbol{X} \boldsymbol{\beta}, \phi\boldsymbol{\Sigma })
\] where the `normal response' \(z_i\) and `weight' \(w_i\) are called
the pseudo-response and pseudo-weight respectively. The pseudo-response
and the pseudo-weight are calculated by \[
\begin{aligned}
z_i &= \hat\eta_i - \frac{L^{'}(y_i|\hat\eta_i)}{L^{''}(y_i|\hat\eta_i)}& w_i &= - L^{''}(y_i|\hat\eta_i),
\end{aligned}
\] where \(\hat\eta_i = (\boldsymbol{X} {\hat{\boldsymbol{\beta}}})_i\),
\(L^{'}(y_i|\hat\eta_i, \hat \phi)\) and
\(L^{''}(y_i|\hat\eta_i, \hat \phi)\) are the first and second
derivative of the log density,
\(\log f(\textbf{y}_i|\boldsymbol{\beta}, \phi)\) with respect to
\(\eta_i\).

With
\(\boldsymbol{z}\sim \text{MVN}(\boldsymbol{X} \boldsymbol{\beta}, \phi \boldsymbol{\Sigma})\)
and
\(\boldsymbol{\beta }\sim \text{MVN}(0, \phi \boldsymbol{\Sigma}_{\tau^2})\),
we can augment the two multivariate normal distributions and update the
estimates for \(\boldsymbol{\beta}\) and \(\phi\) via least squares in
each iteration of the EM algorithm. We create the augmented response,
augmented data, and augmented variance-covariance matrix following
\begin{align*}
& \boldsymbol{z}_* = \begin{bmatrix} \boldsymbol{z}\\ \boldsymbol{0}\end{bmatrix} &&
  \boldsymbol{X}_* = \begin{bmatrix} \boldsymbol{X} \\ \boldsymbol{I} \end{bmatrix} &&
  \boldsymbol{\Sigma}_* = \begin{bmatrix} \boldsymbol{\Sigma }& \boldsymbol{0}  \\ \boldsymbol{0} & \boldsymbol{\Sigma}_{\tau^2}/\phi \end{bmatrix}, &
\end{align*} such that \[
\boldsymbol{z}_* \sim \text{MVN}(\boldsymbol{X}_* \boldsymbol{\beta }, \phi \Sigma_*).
\] Using the least squares estimators to update \(\boldsymbol{\beta}\)
and \(\phi\), we have \begin{align*}
& \boldsymbol{\beta}^{(t)} = (\boldsymbol{X}_*^T \boldsymbol{\Sigma}^{-1} \boldsymbol{X}_*)^{-1}\boldsymbol{X}_*^T \boldsymbol{\Sigma}^{-1} \boldsymbol{z}_* && \phi^{(t)} = \frac{1}{n}(\boldsymbol{z}_*-X_*\boldsymbol{\beta}^{(t)})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{z}_*-X_*\boldsymbol{\beta}^{(t)}).&
\end{align*} To note, the variance-covariance matrixof the coefficient
estimates variance-covariance matrix can be derived in the EM-IWLS
algorithm and in turn can be used for statistical inferences, \[
  \text{Var}(\boldsymbol{\beta}^{(t)}) = (\boldsymbol{X}_*^T\boldsymbol{\Sigma}^{-1} \boldsymbol{X}_*)^{-1}\phi^{(t)}.
\]

Totally, the proposed EM-iterative weighted least square algorithm is
summarized as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Choose a starting value \(\boldsymbol{\beta}^{(0)}\) and
  \(\boldsymbol{\theta}^{(0)}\) for \(\boldsymbol{\beta}\) and
  \(\boldsymbol{\theta}\). For example, we can initialize
  \(\boldsymbol{\beta}^{(0)} = \boldsymbol{0}\) and
  \(\boldsymbol{\theta}^{(0)} = \boldsymbol{0}.5\)
\item
  Iterate over the E-step and M-step until convergence

  E-step: calculate \(E(\gamma_{j})\), \(E(\gamma^*_{j})\) and
  \(E(\tau^{-2}_{j})\), \(E({\tau^*}^{-2}_{jk})\) with estimates of
  \(\Theta^{(t-1)}\) from the previous iteration

  M-step:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Based on the current value of \(\beta\), calculate the pseudo-data
    \(z_i^{(t)}\) and the pseudo-weights \(w_i^{(t)}\)
  \item
    Update \(\boldsymbol{\beta}^{(t)}\) by runing the augmented weighted
    least squared
  \item
    If \(\phi\) is present, update \(\phi\)
  \end{enumerate}
\end{enumerate}

Similar to EM-CD, we assess convergence by the criterion,
\(|d^{(t)}-d^{(t-1)}|/(0.1+|d^{(t)}|)<\epsilon\), where \(\epsilon\) is
a small value (say \(10^{-5}\)).

\hypertarget{selecting-optimal-scale-values}{%
\subsection{Selecting Optimal Scale
Values}\label{selecting-optimal-scale-values}}

\label{sec:tune} Our proposed models, BHAM, require two preset scale
parameters (\(s_0\), \(s_1\)). Hence, we need to find the optimal values
for the scale parameters such that the model reaches its best prediction
performance regarding a criteria of preference. This would be achieved
by constructing a two dimensional grid, consists of different pairs of
(\(s_0\), \(s_1\)) value. However, previous research suggested the value
of slab scale \(s_1\) have less impact on the final model and is
recommended to be set as a generally large value, e.g.~\(s_1 = 1\), that
provides no or weak shrinkage. \citep{Rockova2018} As a result, we focus
on examining different values of spike scale \(s_0\). Instead of the 2-D
grid, We consider a sequence of \(L\) decreasing values
\(\{s_0^l\}: 0 < s_0^1 < s_0^2 < \dots < s_0^L < s_1\). Increasing the
spike scale \(s_0\) tends to include more non-zero coefficients in the
model. A measure of preference calculated with cross-validations,
e.g.~deviance, area under the curve (AUC), mean squared error, can be
used to facilitate the selection of a final model. The procedure is
similar to the Lasso implemented in the widely used R package
\texttt{glmnet}, which quickly fits Lasso models over a list of values
of regularization parameters \(\lambda\) , giving a sequence of models
for users to choose from.

\hypertarget{simulation-study}{%
\section{Simulation Study}\label{simulation-study}}

\label{sec:sim}

In this section, we compare the performance of the proposed models to
four alternative models: component selection and smoothing operator
(COSSO) \citep{Zhang2006GAM}, adaptive COSSO \citep{Storlie2011},
generalized additive models with automatic smoothing \citep{Wood2011},
SB-GAM \citep{Bai2021}. COSSO is one of the earliest smoothing spline
models that consider sparsity-smoothness penalty. Adaptive COSSO
improved upon COSSO by using adaptive weight for penalties such that the
penalty of each functional component are different for extra
flexibility. Generalized additive models with automatic smoothing,
hereafter \textit{mgcv}, is one of the most popular models for nonlinear
effect interpolation and prediction. SB-GAM is the first spike-and-slab
lasso GAM. We implemented COSSO and adaptive COSSO with R package
\texttt{cosso 2.1-1}, generalized additive models with automatic
smoothing with R package \texttt{mgcv 1.8-31}, SB-GAM with R package
\texttt{sparseGAM 1.0}. COSSO models and SB-GAM do not provide
flexibility to define smoothing functions, and hence used the default
choices. Both mgcv and proposed models allow customized smoothing
functions and we chose the cubic regression spline. We controlled the
dimensionality of each smoothing function, 10 bases, for all different
choices of smoothing functions. 5-fold cross-validation were used for
COSSO models, SB-GAM and the proposed models for tuning parameter
selection based on the default selection criteria. 20 default candidates
of tuning parameters (\(s_0\) in BHAM, \(\lambda_0\) in SB-GAM) were
examined for SB-GAM and the proposed models that allow
user-specification of tuning candidates. All computation was conducted
on a high-performance 64-bit Linux platform with 48 cores of 2.70GHz
eight-core Intel Xeon E5-2680 processors and 24G of RAM per core and R
3.6.2 \citep{R}.

Other related methods for high-dimensional GAMs also exist, notably the
methods of sparse additive models by Ravikumar et al.
\citep{Ravikumar2009}, stochastic search term selection for GAM
\citep{Scheipl2012}. However, we exclude these methods from current
simulation study because of demonstrated inferior predictive performance
compared to mgcv and scalability issues with increased number of
predictors. \citep{Scheipl2013}

\hypertarget{monte-carlo-simulation-study}{%
\subsection{Monte Carlo Simulation
Study}\label{monte-carlo-simulation-study}}

We follow the data generating process described in Bai \citep{Bai2021}.
We first generated \(n=500\) training data points with
\(p=4, 10, 50, 100, 200\) predictors respectively, where the predictors
\(X\) are simulated from a multivariate normal distribution
\(\text{MVN}_{n\times p}(0, I_{P})\). We then simulated the outcome
\(y\) from two distributions, Gaussian and binomial with the identity
link and logit link \(g(x) = \log(\frac{x}{1-x})\) respectively. The
mean of each outcome were simulated via the following function \[
\mathbb{E}(Y) = g^{-1}(5 \sin(2\pi x_1) - 4 \cos(2\pi x_2 -0.5) + 6(x_3-0.5) - 5(x_4^2 -0.3))
\] for gaussian and binomial outcomes. Gaussian outcomes required
specification of dispersion, where we set the dispersion parameter to be
1. In this data generating process, we have \(x_1, x_2, x_3, x_4\) as
the active covariates, while the rest covariates are inactive,
i.e.~\(f_j(x_j) = 0\) for \(j = 4, \dots, p\). Another set of
independent sample of size \(n_{test}=1000\),(\(x_{new} ,y_{new}\)), are
created following the same data generating process, serving as the
testing data. We generated 50 independent pairs of training and testing
datasets to evaluate the prediction performance of the chosen models,
where training datasets are used to fit the models and testing datasets
used to calculate assessment measures.

To evaluate the predictive performance of the models, the statistics
calculated based on the testing dataset, \(R^2\) for Gaussian model,
area under the curve (AUC) for binomial model, are calculated averaging
across 50 simulations. The results are tabulated in Table
\ref{tab:gaus}, \ref{tab:bin_auc}.

\begin{center}
Table \ref{tab:gaus} here\\
Table \ref{tab:bin_auc} here\\
\end{center}

The predictive performances have a consistent pattern across the three
distributions of outcomes. Across all the scenarios, COSSO and adaptive
COSSO have the least favorable performance among the applicable methods
examined. To note, mgcv doesn't support high-dimensional analysis when
the number of coefficients are greater than the sample size. mgcv
predicts well when \(p\) is small or moderate (p = 4, 10), and
deteriorate when the number of predictors increase. Among the three
fast-computing Bayesian hierarchical models, the proposed models,
BHAM-IWLS and BHAM-CD predicts better than SB-GAM when the dimension of
are small, moderate, and high. However, in the hyper high dimensional
case, SB-GAM has better performance. Among the proposed methods,
BHAM-IWLS performs consistently better than BHAM-CD.

~

\hypertarget{metabolomics-data-analysis}{%
\section{Metabolomics Data Analysis}\label{metabolomics-data-analysis}}

\label{sec:real_data}

In this section, we apply the proposed models BHAM-SSL, fitted with the
EM-CD algorithm, to analyze two real-world metabolomics datasets where
the outcomes are binary and continuous respectively. We demonstrate the
improved prediction performance compared to the other Bayesian
hierarchical additive model, SB-GAM \citep{Bai2021}. Computation time
for cross-validation and optimal model fitting steps are recorded and
presented in Table \ref{tab:time_real_data}.

\begin{center}
Table \ref{tab:time_real_data} here
\end{center}

\hypertarget{emory-cardiovascular-biobank}{%
\subsection{Emory Cardiovascular
Biobank}\label{emory-cardiovascular-biobank}}

We use the proposed models BHAM-SSL to analyze a metabolic dataset from
a recently published research \citep{Mehta2020} studying plasma
metabolomic profile on the three-year all-cause mortality among patients
undergoing cardiac catheterization. The dataset is publicly available
via \textit{Dryad} \citep{Mehta2020_data}. It contains in total of 776
subjects from two cohorts. As there is a large number of non-overlapping
features among the two cohorts, we use the cohort with larger sample
size (N=454). There were initially 6796 features in the dataset, which
is too large to be practically meaningful to analyze. Hence, we
performed a univariate screening procedure on the features, via GAM
implemented in \texttt{mgcv}, and chose the the top 200 features with
smallest p-values. We used 5-knot spline additive models for binary
outcome using two different models, the proposed BHAM-SSL and the
SB-GAM. 10-Fold cross-validation(CV) are used to choose the optimal
tuning parameters of each framework with respect to the default
selection criterion implemented in the software. Out-of-bag samples were
used for prediction performance evaluation, where deviance, AUC, Brier
score defined as \(\frac{1}{n}\sum\limits^{n}_{i=1}(y_i - \hat y_i)^2\),
and misclassication error defined as
\(\frac{1}{n}\sum\limits^{n}_{i=1}I(|y_i - \hat y_i|>0.5)\) were
calculated. BHAM-CD obtained superior AUC, Brier score, and
misclassification error in the out-of-bag samples compared to SB-GAM
(Table \ref{tab:ECB_res}).

\begin{center}
Table \ref{tab:ECB_res} here
\end{center}

\hypertarget{weight-loss-maintenance-cohort}{%
\subsection{Weight Loss Maintenance
Cohort}\label{weight-loss-maintenance-cohort}}

We use the proposed models BHAM-SSL to analyze metabolomics data from a
recently published study \citep{Bihlmeyer2021} on the association
between metabolic biomarkers and weight loss, where the dataset is
publicly available \citep{Bihlmeyer2021_data}. In this analysis, we
primarily focused on the analysis of one of the three studies included,
weight loss maintenance (WLM) cohort \citep{Svetkey2008}, due to the
drastically different intervention effects. In the dataset, 765
metaboliltes in baseline plasma collected were profiled using liquid
chromatography mass spectrometry. Quality control and natural log
transformation were performed during metabolites data preparation. The
outcome of interest was standardized percent change in insulin
resistance, and hence modeled using a Gaussian model. After removing
missing datapoints and addressing outliers in the data, there were
\(p\)=237 features remaining in the analysis. 5-Knot spline additive
models for the gaussian outcome are constructed using two different
models, the proposed BHAM-SSL and the SB-GAM. 10-Fold
cross-validation(CV) are used to choose the optimal tuning parameters of
each framework with respect to the default selection criterion
implemented in the software. Out-of-bag samples were used for prediction
performance evaluation, where deviance, \(R^2\), mean squared error
(MSE) defined as \(\frac{1}{n}\sum\limits^{n}_{i=1}(y_i - \hat y_i)^2\),
and mean absolute error (MAE) defined as
\(\frac{1}{n}\sum\limits^{n}_{i=1}|y_i - \hat y_i|\) were calculated.
BHAM-CD obtained superior R2, MSE, and MAE in the out-of-bag samples
compared to SB-GAM (Table \ref{tab:WLM_res}).

\begin{center}
Table \ref{tab:WLM_res} here
\end{center}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\label{sec:concl} In the paper, we described a novel generalized
additive model using Bayesian hierarchical priors, particularly the
proposed spike-and-slab spline prior for bi-level functional selection.
Meanwhile, we introduced two optimization based algorithms for model
fitting. The algorithms can be easily scale up to address
high-dimensional data analysis in a computational efficient manner, but
also provide naive inferential supports for many commonly used
hierarchical priors, e.g.~t-distribution, double exponential
distribution, mixture t distribution, and mixture double exponential
distribution, in the analyses that the sparse assumption is weak or not
necessary. Via simulations, we demonstrated that the proposed model
provides as good, if not better, prediction performance compared to some
state-of-the-art non-linear modeling devices.

The proposed model shares many commonality with an high-dimensional
Bayesian GAM, SB-GAM \citep{Bai2021}, independently developed around the
same time of this work. Both frameworks emphasize computational
efficiency by deploying group spike-and-slab lasso type of priors and
optimization-based fast and scalable algorithms. Bai provided the
theoretical proof for the consistency of variable selection using group
spike-and-slab lasso prior. Nevertheless, SB-GAM fails to address the
bi-level functional selection and the model inference. The proposed
model provides solutions to these remaining questions while maintaining
the same level of prediction accuracy. Moreover, the proposed model
renders additional flexibility to model specification, allowing various
choices of smoothing functions and degrees of freedom.

For translational science purpose, we implemented the proposed model in
a R package \texttt{BHAM}, deposited at
\url{https://github.com/boyiguo1/BHAM}. To maximize the flexibility of
smoothing function specification, we deploy the same programming grammar
as in the state-of-the-art package \texttt{mgcv}, in contrast to
previous tools where smoothing functions are limited to the default
ones. Ancillary functions are provided for model specification in
high-dimensional settings, curve plotting and functional selection.

There are some improvements possible for the proposed models. First of
all, the proposed model achieves a bi-level selection via the two-part
spike-and-slab spline prior. Nevertheless, this set-up could result in a
situation that is not theoretically sound: the non-linear component is
selected, but the linear component is not. We currently address it
analytically by including the linear component in the model when
non-linear component is selected. Another possible solution is to impose
a dependent structure of \(\gamma_{j}^*\) on \(\gamma_{j^{0}}\),
i.e.~\(\gamma_j^*|\gamma_{j}^{0}, \theta_j\). Secondly, while the
proposed model addresses a great deal of analytic problem, analyzing the
time-to-event outcome remains unsolved. Our current effort resides on
expanding GAM to Cox proportional hazard models. An naive approach would
be convert a time-to-event outcome to a Poisson outcome following
Whitehead \citep{Whitehead1980}. However, it would be more efficient to
directly fit Cox models via penalized pseudo likelihood function
\citep{Simon2011}. Last but not least, we plan to expand the proposed
model to integrate external biology information. The main motivation for
the integrative model is that biologically informed grouping of weak
effects increases the power of detecting true associations between
features and the outcome \citep{Peterson2016}, and stabilizes the
analysis results for reproducibility purpose. Such integration can be
achieved by setting up a structural hyperprior on the inclusion
indicator of the smoothing function null space
\(\boldsymbol{\gamma}^0\). The similar strategy has been used in Ferrari
and Dunson \citep{Ferrari2020}.

\bibliography{bibfile.bib}

\newpage

\begin{figure}
\centering
\begin{tikzpicture} [
staticCompo/.style = {rectangle, minimum width=1cm, minimum height=1cm,text centered, draw=black, fill=blue!30},
outCome/.style={ellipse, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=blue!30},
mymatrix/.style={matrix of nodes, nodes=outCome, row sep=1em},
PriorBoarder/.style={rectangle, minimum width=5cm, minimum height=10cm, text centered, fill=lightgray!30},
background/.style={rectangle, fill=gray!10,inner sep=0.2cm, rounded corners=5mm}
]

\matrix (linearPrior) [matrix of nodes, column sep = 0mm, row sep = 0.7cm] {
  \node (linearGamma) [outCome] { $\gamma_j \sim Bin(1, \theta_j) $ };\\
  \node (linearBeta) [outCome] { $\beta_j \sim DE(0,(1-\gamma_{j}) s_0 + \gamma_{j} s_1)$};\\
};
\matrix (penPrior) [right = 2cm of linearPrior, matrix of nodes, column sep = 0mm, row sep = 0.7cm] {
  \node (penGamma) [outCome] { $\gamma_{j}^*\sim Bin(1, \theta_j)$ };\\
  \node (penBeta) [outCome] { $\beta_{jk}^*\sim  DE(0,(1-\gamma^*_{j}) s_0 + \gamma^*_{j} s_1)$};\\
};


\node (s) [staticCompo]  at ($(linearBeta)!0.5!(penBeta)$)  {($s_0, s_1$)};
\node (Beta) [staticCompo, below = 1cm of s] {$\boldsymbol{\beta }= (\beta_1, \boldsymbol{\beta}^*_1, \dots,\beta_j, \boldsymbol{\beta}^*_j , \dots,\beta_p, \boldsymbol{\beta}^*_p) $};
\node (Theta)[outCome, above = 2cm of s] {$\theta_{j} \sim Beta(a, b)$};
\node (ab)[staticCompo, above = 0.5cm of Theta] {$(a, b)$};
\node (Y) [outCome, below = 1cm of Beta] {$y_i \sim Expo. Fam. (g^{-1}(\boldsymbol{\beta}^T \boldsymbol{X}_i), \phi)$};

\draw[->] (Theta) -- (linearGamma);
\draw[->] (Theta) -- (penGamma);
\draw[->] (linearGamma) -- (linearBeta) ;
\draw[->] (penGamma) -- (penBeta);
\draw[->] (ab) -- (Theta);
\draw[->] (s) -- (linearBeta) ;
\draw[->] (s) -- (penBeta);
\draw[->] (linearBeta) -- (Beta);
\draw[->] (penBeta) -- (Beta);
\draw[->] (Beta) --  (Y);


\begin{pgfonlayer}{background}
  \node [background,
   fit=(linearGamma) (linearBeta),
   label=above:Linear Space:] {};
  \node [background,
    fit=(penGamma) (penBeta),
    label=above:Nonlinear Space:] {};
\end{pgfonlayer}

\end{tikzpicture}

\caption{Directed acyclic graph of BHAM-SSL model with parameter expansion. Elliposes are stochastic nodes, rectangles and are deterministic nodes. }
\label{fig:SSprior}
\end{figure}

\newpage

\begin{table}[ht]
\centering
\begin{tabular}{ccccccc}
  \hline
P & BHAM-IWLS & BHAM-CD & COSSO & Adaptive COSSO & mgcv & SB-GAM \\ 
  \hline
  4 & 0.90 (0.01) & 0.90 (0.01) & 0.75 (0.03) & 0.71 (0.13) & 0.90 (0.01) & 0.82 (0.04) \\ 
   10 & 0.90 (0.01) & 0.88 (0.01) & 0.67 (0.15) & 0.76 (0.03) & 0.90 (0.01) & 0.82 (0.04) \\ 
   50 & 0.88 (0.01) & 0.80 (0.04) & 0.43 (0.17) & 0.57 (0.19) & 0.86 (0.02) & 0.82 (0.04) \\ 
  100 & 0.80 (0.07) & 0.73 (0.06) & 0.41 (0.19) & 0.51 (0.22) & - & 0.81 (0.04) \\ 
  200 & 0.72 (0.10) & 0.77 (0.02) & 0.33 (0.15) & 0.44 (0.19) & - & 0.82 (0.04) \\ 
   \hline
\end{tabular}
\caption{The average and standard deviation of the out-of-sample $R^2$ measure for Gaussian outcomes over 50 iterations. The models of comparison include the proposed Bayesian hierarchical additive model (BHAM) fitted with Iterative Weighted Least Square (BHAM-IWLS) and Coordinate Descent (BHAM-CD) algorithms, component selection and smoothing operator (COSSO), adaptive COSSO, mgcv and sparse Bayesian generalized additive model (SB-GAM). mgcv doesn't provide estimation whe number of parameters exceeds sample size i.e. p = 100, 200.} 
\label{tab:gaus}
\end{table}

\newpage

\begin{table}[ht]
\centering
\begin{tabular}{rllllll}
  \hline
P & BHAM-IWLS & BHAM-CD & COSSO & Adaptive COSSO & mgcv & SB-GAM \\ 
  \hline
  4 & 0.94 (0.01) & 0.94 (0.02) & 0.90 (0.02) & 0.90 (0.01) & 0.94 (0.01) & 0.93 (0.01) \\ 
   10 & 0.93 (0.01) & 0.89 (0.02) & 0.85 (0.04) & 0.86 (0.03) & 0.92 (0.04) & 0.92 (0.01) \\ 
   50 & 0.92 (0.01) & 0.89 (0.01) & 0.83 (0.02) & 0.83 (0.02) & 0.76 (0.04) & 0.92 (0.01) \\ 
  100 & 0.89 (0.02) & 0.86 (0.02) & 0.83 (0.02) & 0.84 (0.02) & - & 0.92 (0.01) \\ 
  200 & 0.88 (0.01) & 0.86 (0.02) & 0.82 (0.05) & 0.81 (0.08) & - & 0.92 (0.01) \\ 
   \hline
\end{tabular}
\caption{The average and standard deviation of the out-of-sample area under the curve measures for binomial outcomes over 50 iterations. The models of comparison include the proposed Bayesian hierarchical additive model (BHAM) fitted with Iterative Weighted Least Square (BHAM-IWLS) and Coordinate Descent (BHAM-CD) algorithms, component selection and smoothing operator (COSSO), adaptive COSSO, mgcv and sparse Bayesian generalized additive model (SB-GAM). mgcv doesn't provide estimation whe number of parameters exceeds sample size i.e. p = 100, 200.} 
\label{tab:bin_auc}
\end{table}

\newpage
\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{2pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.75in}|p{0.75in}|p{0.75in}|p{0.75in}|p{0.75in}|p{0.75in}|p{0.75in}|p{0.75in}}

\caption{The average and standard deviation of computation time in seconds, including cross-validation and final model fitting, over 50 iterations. The models of comparison include the proposed Bayesian hierarchical additive model (BHAM) fitted with Iterative Weighted Least Square (BHAM-IWLS) and Coordinate Descent (BHAM-CD) algorithms, component selection and smoothing operator (COSSO), adaptive COSSO, mgcv and sparse Bayesian generalized additive model (SB-GAM). mgcv doesn't provide estimation whe number of parameters exceeds sample size i.e. p = 100, 200.\label{tab:time_sim}}\\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{Distribution}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{P}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{BHAM-IWLS}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{BHAM-CD}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{COSSO}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{Adaptive\ COSSO}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{mgcv}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{SB-GAM}}} \\

\noalign{\global\setlength{\arrayrulewidth}{2pt}}\arrayrulecolor[HTML]{666666}\cline{1-8}

\endfirsthead

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{Distribution}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{P}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{BHAM-IWLS}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{BHAM-CD}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{COSSO}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{Adaptive\ COSSO}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{mgcv}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{SB-GAM}}} \\

\noalign{\global\setlength{\arrayrulewidth}{2pt}}\arrayrulecolor[HTML]{666666}\cline{1-8}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{4}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{5.88\ (1.01)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{6.96\ (1.76)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{2.97\ (0.82)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{4.60\ (2.32)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{0.18\ (0.04)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{343.24\ (88.37)}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{10}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{14.01\ (1.86)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{7.65\ (1.25)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{7.95\ (2.70)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{10.01\ (3.56)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{3.51\ (12.03)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{543.13\ (133.51)}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{50}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{185.26\ (17.44)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{53.83\ (6.26)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{109.72\ (24.31)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{128.13\ (23.26)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{670.01\ (151.28)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{1630.42\ (193.61)}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{100}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{1106.93\ (356.73)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{48.41\ (5.84)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{715.15\ (170.55)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{718.63\ (193.53)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{-}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{2783.99\ (235.31)}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\multirow[c]{-5}{*}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{Binomial}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{200}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{6923.62\ (1911.59)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{99.98\ (7.05)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{5572.51\ (1295.18)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{4958.69\ (1970.46)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{-}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{4780.10\ (488.66)}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{4}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{4.22\ (1.51)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{2.51\ (0.20)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{0.79\ (0.11)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{0.67\ (0.07)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{0.05\ (0.00)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{36.02\ (3.02)}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{10}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{17.59\ (5.77)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{33.32\ (3.50)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{3.49\ (0.70)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{3.47\ (0.71)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{0.32\ (0.34)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{72.93\ (9.52)}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{50}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{329.83\ (44.57)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{334.53\ (22.33)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{35.54\ (9.15)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{35.44\ (9.02)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{73.33\ (71.24)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{373.42\ (34.93)}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{100}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{1706.19\ (225.92)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{446.98\ (79.31)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{150.58\ (46.72)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{152.82\ (45.07)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{-}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{684.05\ (67.80)}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\multirow[c]{-5}{*}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{Gaussian}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedleft}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{200}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{16379.27\ (4825.56)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{303.73\ (87.27)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{591.11\ (147.88)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{569.92\ (107.45)}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{-}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\raggedright}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{1314.12\ (137.85)}}} \\

\noalign{\global\setlength{\arrayrulewidth}{2pt}}\arrayrulecolor[HTML]{666666}\cline{1-8}



\end{longtable}

\newpage

\providecommand{\docline}[3]{\noalign{\global\setlength{\arrayrulewidth}{#1}}\arrayrulecolor[HTML]{#2}\cline{#3}}

\setlength{\tabcolsep}{2pt}

\renewcommand*{\arraystretch}{1.5}

\begin{longtable}[c]{|p{0.75in}|p{0.75in}|p{0.75in}|p{0.75in}|p{0.75in}|p{0.75in}|p{0.75in}}

\caption{Model fitting time in seconds for two metabolomics data analyses, from Emory Cardiovascular Biobank (ECB) and Weight Loss Maintenance Cohort (WLM). It tabulates the computation time for cross-validation step (CV) and optimal model fitting step (Final), and total computation time (Total) for the proposed model BHAM with EM-CD algorithm (BHAM-CD) and the model of comparison SB-GAM.\label{tab:time_real_data}}\\

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{} & \multicolumn{3}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 2.25in+4\tabcolsep+2\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{BHAM-CD}}} & \multicolumn{3}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 2.25in+4\tabcolsep+2\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{SB-GAM}}} \\

\noalign{\global\setlength{\arrayrulewidth}{1pt}}\arrayrulecolor[HTML]{666666}\cline{2-7}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\multirow[c]{-2}{*}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{Data}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{CV}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{Final}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{Total}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{CV}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{Final}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{Total}}} \\

\noalign{\global\setlength{\arrayrulewidth}{2pt}}\arrayrulecolor[HTML]{666666}\cline{1-7}

\endfirsthead

\hhline{>{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}->{\arrayrulecolor[HTML]{666666}\global\arrayrulewidth=2pt}-}

\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{} & \multicolumn{3}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 2.25in+4\tabcolsep+2\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{BHAM-CD}}} & \multicolumn{3}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 2.25in+4\tabcolsep+2\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{SB-GAM}}} \\

\noalign{\global\setlength{\arrayrulewidth}{1pt}}\arrayrulecolor[HTML]{666666}\cline{2-7}



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\multirow[c]{-2}{*}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{Data}}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{CV}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{Final}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{Total}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{CV}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{Final}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{Total}}} \\

\noalign{\global\setlength{\arrayrulewidth}{2pt}}\arrayrulecolor[HTML]{666666}\cline{1-7}\endhead



\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{ECB}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{225.2}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{3.0}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{228.2}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{3,506.4}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{34.4}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{3,540.7}}} \\





\multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{WLM}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{483.1}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{5.0}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{488.1}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{3,116.0}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{32.7}}} & \multicolumn{1}{!{\color[HTML]{000000}\vrule width 0pt}>{\centering}p{\dimexpr 0.75in+0\tabcolsep+0\arrayrulewidth}!{\color[HTML]{000000}\vrule width 0pt}}{\fontsize{11}{11}\selectfont{\textcolor[HTML]{000000}{3,148.7}}} \\

\noalign{\global\setlength{\arrayrulewidth}{2pt}}\arrayrulecolor[HTML]{666666}\cline{1-7}



\end{longtable}

\newpage

\begin{table}[ht]
\centering
\begin{tabular}{lrrrr}
  \hline
Methods & Deviance & AUC & Brier & Misclass \\ 
  \hline
BHAM-CD & 455.69 & 0.74 & 0.16 & 0.21 \\ 
  SB-GAM & 1230.06 & 0.71 & 0.21 & 0.24 \\ 
   \hline
\end{tabular}
\caption{Prediction performance of BHAM fitted with Cooridnate Descent algorithm (BHAM-CD) and SB-GAM models for Emory Cardiovascular Biobank by 10-fold cross-validation, including deviance, area under the curve (AUC), Brier score, and misclassification error (Misclass) where class labels are defined using threshold = 0.5.} 
\label{tab:ECB_res}
\end{table}

\newpage

\begin{table}[ht]
\centering
\begin{tabular}{lrrrr}
  \hline
Methods & Deviance & $R^2$ & MSE & MAE \\ 
  \hline
BHAM-CD & 665.63 & 0.07 & 0.94 & 0.75 \\ 
  SB-GAM & 666.83 & 0.03 & 0.98 & 0.77 \\ 
   \hline
\end{tabular}
\caption{Prediction performance of of BHAM fitted with Cooridnate Descent algorithm (BHAM-CD)  and SB-GAM models for Weight Loss Maintenance Cohort by 10-fold cross-validation, including deviance, $R^2$,  mean squared error (MSE), and mean absolute error (MAE).} 
\label{tab:WLM_res}
\end{table}



\end{document}
