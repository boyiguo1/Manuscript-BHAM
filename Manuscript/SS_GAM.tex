\documentclass[AMA,STIX1COL,]{WileyNJD-v2}


% Pandoc citation processing

\usepackage{tikz}
\usepackage{pgfplots}

\articletype{Research article}

\received{2021-01-01}

\revised{2021-02-01}

\accepted{2021-03-01}

\raggedbottom

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}

\title{Spike-and-Slab Generalized Additive Models and Fast Algorithms
for High-Dimensional Data}

\author[a]{Boyi Guo*}
\author[b]{Byron C. Jaeger}
\author[a]{AKM Fazlur Rahman}
\author[a]{D. Leann Long}
\author[a]{Nengjun Yi}

\authormark{Guo et al.}

\address[a]{Department of Biostatistics, University of Alabama at
Birmingham, Birmingham, USA}
\address[b]{Department of Biostatistics and Data Science, Wake Forest
University, Winston-Salem, USA}

\corres{*Corresponding author name, This is sample corresponding
address. \email{xxx@uab}}

\presentaddress{This is sample for present address text this is sample
for present address text}

\abstract{Spike-and-slab priors have been extensively studied for
variable selection, and recently been extended to function selection in
generalized additive models (GAMs). Most of the previous works on
fitting spike-and-slab GAM relies on Markov chain Monte Carlo (MCMC)
algorithms to identify the promising subsets of functions. The
computational burden of MCMC makes these models less feasible,
especially in high-dimensional setting. Besides, most of the current
spike-and-slab GAM consider an ``all-in-all-out'' approach for function
selection, rendering less flexibility to answer if non-linear effects
are necessary in the model. We propose Bayesian hierarchical generalized
additive models for function selection and prediction. The proposed
models employ spike-and-slab priors, mixture normal and mixture double
exponential, for smooth and sparse solution. Two EM based algorithms,
EM-IWLS and EM-Coordinate Descent, are developed for accelerated
computation. Simulation studies and real data analysis demonstrated
improved performance against the state-of-art models, mgcv and COSSO.
The software implementation of the proposed methods is freely available
via the R package BHAM.}

\keywords{Spike-and-Slab Priors; High-Dimensional Data; Generalized
Additive Models; EM-IWLS; EM-Coordinate Decent}

\maketitle

\usetikzlibrary{shapes.geometric, arrows, positioning, calc, matrix, backgrounds, fit}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\tp}{\text{pen}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\label{sec:intro}

Using additive models to interpolate nonlinear effects has a long
history. Early work of additive models can date back to 1979 when
Cleveland \citep{Cleveland1979} proposed local smoothing for bivariate
analysis. Since then, many mathematical and statistical models have been
proposed. \citep{Wood2020} With the recent advancement of computational
technology, many machine learning algorithms that are computationally
intensive start to gain much popularity in prognostic modeling and
functional selection, especially when complex signals, for example
nonlinear effects, are assumed. Such models generally have improved
prediction accuracy over classic statistical models. However, these
models have a fatal flaw, the lack of transparency and interpretability,
and hence are referred to as ``black box'' models \citep{Breiman2001}.
Given that the primary objective of biomedical studies is to understand
the etiology of diseases, ``Black box'' models are less helpful as they
can hardly explain how the change of biomarkers is associated with the
disease.

Semi-parametric regression models are a more appropriate alternative to
the ``black-box'' models, thanks to their balancing between model
flexibility and interpretability. Among those, generalized additive
models (GAMs), proposed in the seminal work of Hastie and Tibshirani
\citep{Hastie1987}, grew to be one of the most popular modeling tools.
In a GAM, the response variable, \(Y\), which is assumed to follow some
exponential family distribution \(EF(\mu, \phi)\) with mean \(\mu\) and
dispersion \(\phi\), can be expressed as the summation of smooth
functions, \(f_j, j = 1, \dots, p\), of a given \(p\)-dimensional vector
of covariates \(\boldsymbol{x}\), written as \[
 E(Y|\boldsymbol{x}) = g^{-1}(a + \sum\limits^p_{j=1}f_j(x_j)),
\] where \(g^{-1}\) is the inverse function of a smooth monotonic link
function. The smooth functions can take many forms and are estimated
using a pseudo-weighted version of the backfitting algorithm
\citep{Breiman1985}. Nevertheless, the classic GAMs cannot fulfill the
increasing analytic demands for high-dimensional data analysis in
biomedical studies.

Many modern biomedical research, e.g.~sequencing data analysis, electric
health record data analysis, require special treatment of
high-dimensionality, commonly known as \(p >> n\) problem. There is
extensive literature on high-dimensional linear models via penalized
models or Bayesian hierarchical models, see Mallick and Yi
\citep{Mallick2013} for review. These models assume sparsity among the
predictors, i.e.~most predictors have small or no effects on the
outcome. Further development focuses on addressing the grouping
structure among the predictors, where AMs and GAMs can be considered
special cases. Various penalties are imposed on the basis function
coefficients: Ravikumar et al. \citep{Ravikumar2009} extended the
grouped lasso \citep{Yuan2006} to additive models; Huang et al.
\citep{Huang2010} further developed adaptive grouped lasso for additive
models; Wang et al. \citep{Wang2007} and Xue \citep{Xue2009}
respectively applied grouped SCAD penalty \citep{Fan2001} to the
additive models. Recent Bayesian hierarchical models are also used in
the context of high-dimensional additive models. Various group
spike-and-slab priors combining with computationally intensive Markov
chain Monte Carlo (MCMC) algorithms \citep{Xu2015, Yang2020} are
proposed, where the application on AMs are by-products. Scheipl et al.
\citep{Scheipl2012} proposed an ad hoc spike-and-slab structure prior,
normal-mixture-of-inverse gamma, specifically for semi-parametric
regression using MCMC. The normal-mixture-of-inverse gamma prior is
mixture of t-distribution with an additional coefficients mixing vector,
where the vector follows a mixture of two Gaussian distribution. Bai et
al. \citep{Bai2020} was the first to apply group spike-and-slab lasso
prior to AMs using a fast optimization algorithm, and further
generalized the framework to GAMs\citep{Bai2021}. Regardless of the nice
statistical properties, a practical and pivotal question, if non-linear
effects exist, yet to be addressed by these methods. Besides Scheipl et
al. \citep{Scheipl2012}, most proposed methods adapt an `all-in-all-out'
strategy, i.e.~either including or excluding the variable completely,
rendering no space for within group selection. In addition, a common
criticism received by these methods is that the basis function
coefficients tend to be overly penalized due to the sparsity assumption
directly applied on the basis function coefficients.\citep{Scheipl2013}
It would be of special interest to develop a fast yet flexible
generalized additive model.

To address these challenges, we propose a novel Bayesian hierarchical
generalized additive model (BHAM) for high dimensional data analysis.
Specifically, we impose a new bi-part spike-and-slab spline prior on the
smooth functions to perform bilevel selection, where the linear and
nonlinear spaces of smooth functions can be selected separately. The
prior setup encourages a flexible solution, rending one of three
possibilities: no effect, only linear effect, or nonlinear effect of a
predictor. Additionally, two fast computing optimization-based
algorithms are developed for high and
ultrahigh-dimensional\citep{Fan2009} settings respectively. Meanwhile,
to avoid the over-penalization of the basis function coefficients,
smoothness penalties are incorporated in the model via
re-parameterization. Smoothness penalties are commonly implemented in
GAMs through smoothing and penalized regression splines. They are
quadratic norms of the coefficients and allow locally adaptive penalties
on each smoothing function. For the most common cases of cubic splines,
the smoothness penalty conditioning on a smoothness penalty parameter
\(\lambda_j\) is a function of the integration of the second derivative
of the spline function, expressed mathematically as
\begin{equation}\label{eq:smoothpen}
  \text{pen}\left[f_j(x)\right] = \lambda_j \int f^{"}_j(x)dx = \lambda_j \boldsymbol{\beta}_j^T \boldsymbol{S}_j \boldsymbol{\beta}_j ,
\end{equation} where \(\boldsymbol{S}_j\) is a known smoothness penalty
matrix, and \(\boldsymbol{\beta}_j\) is the basis function coefficients.
Smoothness penalties are also previously used in the spike-and-slab GAM
\citep{Scheipl2012} and the sparsity-smoothness penalty
\citep{Meier2009}. Moreover, incorporating the smoothness penalty allows
the separation of the linear space of the smooth function, also know as
the null space, from the nonlinear space. Marra and Wood
\citep{Marra2011} introduced a re-parameterization of the basis function
matrix based on the decomposition of the smoothness penalty matrix
\(\boldsymbol{S}\). The re-parameterization allows different penalties
imposed on the null and nonlinear spaces separately, and further allows
the sparsity in linear space and smoothness in nonlinear effect
interpolation. In addition, the priors of the linear and nonlinear
effects are independent, creating computational convenience and further
improving the scalability of the model.

The proposed BHAM uses a spike-and-slab prior for function selections,
but it substantially differs from previous spike-and-slab based GAMs,
i.e.~the spike-and-slab GAM \citep{Scheipl2012} and the SB GAM
\citep{Bai2021} in many ways. First of all, the proposed spike-and-slab
spline prior is a spike-and-slab lasso type prior using independent
mixture double exponential distribution, compared to spike-and-slab GAM.
Spike-and-slab lasso priors provide computational convenience during
model fitting through optimization algorithms versus computational
intensive sampling algorithms. They make fitting high- and
ultrahigh-dimensional models more feasible without sacrificing
performance in prediction and variable selection. Secondly, the
optimization algorithms introduced in this article include EM-Coordinate
Descent algorithm and EM-Iterative weighted least square algorithm for
different utilities. The applications of the two aforementioned
algorithm are fruitful in Bayesian hierarchical models in
high-dimensional data analysis.
\citep{Yi2012, Rockova2014a, Rockova2018} Even though SB-GAM with a
group spike-and-slab lasso prior also uses EM-CD for model fitting, the
proposed EM-IWLS provides uncertainty measures that could be further
used for confidence bound calculation and hypothesis testing. EM-IWLS
complements the lack of inference potential from EM-CD in a median and
high dimensional setting. Furthermore, the proposed model addresses the
incapability of bilevel selection in SB-GAM. Lastly, the R
implementation of the Bayesian hierarchical additive model is available
publicly via BHAM at \url{https://github.com/boyiguo1/BHAM}, make
transnational science more accessible.

The rest of the paper are arranged as the followings: in Chapter 2 we
demonstrate the Bayesian hierarchical generalized additive model, where
we provide the setup of the proposed model and describe the two
fast-fitting algorithms. In Chapter 3, we compare the proposed via monte
carlo simulation. .Application to real-world datasets demonstrates the
usefulness of the proposed methodologies in Chpater 4. Conclusion and
discussions are made in Chapter 5. \footnote{TODO: rewrite this
  sentence.}

\hypertarget{bayesian-hierarchical-additive-models}{%
\section{Bayesian Hierarchical Additive
Models}\label{bayesian-hierarchical-additive-models}}

\label{sec:BHAM}

Following the GLM notation introduced in Section \ref{sec:intro}, we
have a generalized additive model and its matrix form,
\begin{equation}\label{eq:gam}
E(Y|\boldsymbol{x}) = g^{-1}(a + \sum\limits^p_{j=1}f_j(x_j)) = g^{-1}(a + \sum\limits^p_{j=1} \boldsymbol{\beta}_j^T \boldsymbol{X}_j),
\end{equation} with the smoothing function \(f_j(x_j)\) of the variable
\(x_j, j = 1, \dots, p.\) The basis function matrix, i.e.~the design
matrix derived from the smoothing function \(f_j(x_j)\), is denoted
\(\boldsymbol{X}_j\) for the variable \(x_j\). The dimension of the
design matrix depends on the choice of the smoothing function. For
example, a cubic spline function with \(k\) knots produces a \(k+2\)
elements column vector. Its corresponding smoothness penalty matrix is
denoted as \(\boldsymbol{S}_j\). \(\boldsymbol{\beta}_j\) denotes the
basis function coefficients for the \(j\)th variable, and the matrix
transposing operation is denoted with a superscript \(^T\). To note, the
proposed model can include parametric forms of variables in the model,
treating the parametric function as a special case of the smoothing
function, e.g.~\(f_j(x_j) = x_j\) with the smoothness penalty matrix
defined as \(\boldsymbol{S}_j = \begin{bmatrix}0\end{bmatrix}\).

To encourage proper smoothness of the smoothing functions, we adopt the
idea of smoothing penalties from smoothing spline models. The basic idea
is to set up a smoothing penalty, depending on the smoothness penalty
matrix via Equation \ref{eq:smoothpen}, in the posterior density
function, or least squared and likelihood function in the penalized
framework. However, the direct integration of smoothness penalties into
sparsity penalties is not obvious. Given the smoothness penalty matrix
is most often symmetric and positive semi-definite \footnote{ we limit
  the discussion to univariate smoothing function. However, the penalty
  matrix could possibly contain multiple matrices, e.g.~tensor product
  smoother.}, it can be integrated into the design matrix
\(\boldsymbol{X}\) via a re-parameterization. The re-parameterization
follows Marra and Wood \citep{Marra2011}, in which eigen-decomposed the
penalty matrix
\(\boldsymbol{S} = \boldsymbol{U} \boldsymbol{D} \boldsymbol{U}^T\),
where the matrix \(\boldsymbol{D}\) is diagonal with the eigenvalues
arranged in the ascending order. To note, \(\boldsymbol{D}\) can contain
\(0\) elements on the diagonal, associated with the null space of the
smooth function. We further write the orthonormal matrix
\(\boldsymbol{U}\), containing the eigenvectors as column, as an (row)
augmentation of two sub-matrices
\(\boldsymbol{U} \equiv \begin{bmatrix}\boldsymbol{U}^0 : \boldsymbol{U}^{\text{pen}}\end{bmatrix}\),
where \(\boldsymbol{U}^{\text{pen}}\) contains the eigenvectors (as
columns) for the non-zero eigenvalues, i.e.~the penalized space, and
\(\boldsymbol{U}^0\) contains the eigenvectors for the null space. We
multiply the basis function matrix \(\boldsymbol{X}\) with the
orthonormal matrix \(\boldsymbol{U}\) for the new design matrix
\(\boldsymbol{X}^\ast = \boldsymbol{X} \boldsymbol{U} \equiv \begin{bmatrix} \boldsymbol{X}^0 : \boldsymbol{X}^{\text{pen}} \end{bmatrix}\).
An additional scaling step is imposed on \(\boldsymbol{X}^{\text{pen}}\)
by the non-zero eigenvalues of \(\boldsymbol{D}\) such that the new
basis function matrix \(\boldsymbol{X}^\ast\) is scale-invariant
\footnote{ I know this work for tensor spline (see \citep{Wood2006}),
  but I am not sure if it can be generalized to all smoothing functions}.
For notation convenience, we denote
\(\boldsymbol{X}_j \equiv \begin{bmatrix} \boldsymbol{X}_j^0 : \boldsymbol{X}_j^{\text{pen}} \end{bmatrix}\)
as the after re-parameterized basis function matrix for the \(j\)th
variable hereafter. Similarly, the spline function can be expressed in
the matrix form \[
f_j(x_j) = f_j^0(x_j) + f_j^\text{pen}(x_j) = \boldsymbol{\beta_j^0}^T \boldsymbol{X}_j^0 + \boldsymbol{\beta_j^\text{pen}}^T \boldsymbol{X}_j^\text{pen},
\] and the generalized additive model in Equation \ref{eq:gam} be
\begin{equation}\label{eq:gam-repa}
E(Y|\boldsymbol{x}) =  g^{-1}(a + \sum\limits^p_{j=1} \boldsymbol{\beta}_j^T \boldsymbol{X}_j) = g^{-1}\left[a + \sum\limits^p_{j=1} ({\beta_j^0}^T \boldsymbol{X}_j^0 + {\beta_j^\text{pen}}^T \boldsymbol{X}_j^\text{pen})\right].
\end{equation}

The re-parameterization sets up the foundation of the proposed model,
which has three-fold benefits. First of all, as previously mentioned,
the re-parameterization integrated the smoothness penalty into the
design matrix, and hence the penalty can be expressed in the posterior
density function implicitly in addition to the sparse penalty for
variable and function selection. Secondly, the eigendecomposition of the
smooth penalty isolates the linear space of a smooth function from the
non-linear space, improving the feasibility of bi-level
variable/function selection and estimation inference. The
eigendecomposition also introduced the linearly independent column
spaces of the smooth function matrix, which allows imposing independent
priors on the coefficients. This reduces the computational complexity
that multivariate priors induces, and greatly broadens the choices of
priors and further model choices. Last but not least, the scaling step
of \(\boldsymbol{X}^\text{pen}\) further simplifies the choice of
penalty priors, from column-specific penalties to a unified penalty.
This step is also necessary for the proposed spike-and-slab spline
prior. Conventionally, the spike-and-slab priors require scaling of
predictors \footnote{TODO: insert a citation.}

\hypertarget{spike-and-slab-smooth-priors}{%
\subsection{Spike-and-Slab Smooth
Priors}\label{spike-and-slab-smooth-priors}}

The family of spike-and-slab (SS) priors regression models dominates
Bayesian high-dimensional analysis for its utility in outcome prediction
and variable selection. We defer to Bai et al. \citep{Bai2021Review} for
an in-depth introduction to spike-and-slab priors. To summary,
spike-and-slab priors are a family of mixture distributions that
comprises a skinny spike density \(f_{spike}(\cdot)\) for weak signals
and a flat slab density \(f_{slab}(\cdot)\) for large signals,
mathematically \[
 \beta|\gamma \sim (1-\gamma)f_{spike}(\beta) + \gamma f_{slab}(\beta).
\] The most distinct feature of the SS priors is that it is conditioned
on a latent binary variable \(\gamma\) that indicates whether the
variable \(x\) is included in the model. There are various
spike-and-slab priors depending on the choice for the spike density
\(f_{spike}(\cdot)\)\footnote{TODO: should I change to notation of \(f\)
  to not confound with the smooth functions?} and the slab density
\(f_{slab}(\cdot)\), see George and McCulloch
\citep{George1993, George1997}; Chipman \citep{Chipman1996} for grouped
variables; Brown et al. \citep{Brown1998} for multivariate outcomes;
Ishwaran and Rao \citep{Ishwaran2005}; Clyde and George
\citep{Clyde2004} and reference therein.

The major criticism of early spike-and-slab priors receives is being
computationally prohibitive. \citep{Bai2021Review} Since then, many
studies focus on alleviating the computational burden that sampling
algorithms bear, which include EMVS \citep{Rockova2014a} and
Spike-and-slab Lasso \citep{Rockova2018b, Rockova2018}. Particularly,
the development of Spike-and-slab Lasso substantially improves the
scalability of SS models, setting up the theoretical foundation for
generalized models in -omics data analysis
\citep{Tang2017a, Tang2017, Tang2018, Tang2019}. The spike-and-slab
lasso prior is composed of two double exponential distributions with
mean 0 and different dispersion parameters, \(0 < s_0 < s_1\),
mathematically, \begin{equation} \label{eq:ssl}
\beta | \gamma \sim (1-\gamma)DE(0, s_0) + \gamma DE(0, s_1), 0 < s_0 < s_1.
\end{equation} The SSL also mitigated the problem of EMVS where the weak
signals are not shrink to zero, favored in high-dimensional data
analysis. We noticed that Bai \citep{Bai2021} introduces a
spike-and-slab group prior in the GAM framework, where the densities of
the spike and slab components take the group lasso density
\citep{Xu2015}. Bai \citep{Bai2021} takes an ``all-in-all-out'' strategy
for function selection.

\hypertarget{spike-and-slab-lasso-prior}{%
\subsubsection{Spike-and-Slab Lasso
Prior}\label{spike-and-slab-lasso-prior}}

We introduce a novel prior for GAMs, particularly for high-dimensional
nonlinear effect modeling with bi-level selection. The proposed prior
extends from the spike-and-slab lasso prior described in Equation
\ref{eq:ssl}. Given the reparameterized deign matrix
\(\boldsymbol{X}_j = \begin{bmatrix} \boldsymbol{X}_j^0 : \boldsymbol{X}_j^\text{pen}\end{bmatrix}^T\)
for the \(j\)th variable, we impose a two-part SSL prior to the
coefficients \(\boldsymbol{\beta}_j\). Specifically, we impose
independent group priors on the null space coefficients and on the
penalized space coefficients respectively,

\begin{align}\label{eq:bham_ssl}
  \beta^0_{j} |\gamma^0_{j},s_0,s_1 &\sim DE(0,(1-\gamma^0_{j}) s_0 + \gamma^0_{j} s_1)\\
  \beta^\text{pen}_{jk} | \gamma^\text{pen}_{j},s_0,s_1 &\sim DE(0,(1-\gamma^\text{pen}_{j}) s_0 + \gamma^\text{pen}_{j} s_1), 
\end{align}

where \(\gamma^0_{j}\in\{0,1\}\) and
\(\gamma^\text{pen}_{j}\in \{0,1\}^{K_j}\) are latent indicator
variables, indicating if the model includes the linear effect and the
non-linear effect of the \(j\)th variable respectively. ; \(s_0\) and
\(s_1\) are scale parameters, assuming \(0 < s_0 < s_1\) and given.
These scale parameters \(s_0\) and \(s_1\) can be treated as tuning
parameters and optimized via cross-validation. A discussion of how to
choose the scale parameters comes in Section \ref{sec:tune}. To note,
this prior differs from previous group spike-and-slab lasso priors
\citep{Tang2018, Tang2019}, where the \(\boldsymbol{\beta}^0_j\) and
\(\boldsymbol{\beta}^\text{pen}_j\) shares the same indicator variables
\(\gamma_j^0\), \(\gamma_j^\text{pen}\) respectively. It is possible to
relax the assumption such that each coefficient \(\beta\) to have its
own latent indicator \(\gamma\), but at the cost ofcomplicating the
bi-level function selection. Conversely, it is possible to add a more
restrictive assumption on the priors, assuming that one indicator
variable decides the inclusion of both the linear effect and non-linear
effect, i.e.~\(\gamma_j = \gamma^0_j = \gamma^\text{pen}_j\). This
converges\footnote{Is ``becomes'' better?} to the SB-GAM
\citep{Bai2021}.

The reparameterization introduced in Section \ref{sec:BHAM} grants the
validity of the proposed prior. First of all, the smooth function bases
are linear dependent to each other based on the definitions, and ask for
extra attention. The eigeondecomposition remedies the problem. Hence our
prior can be set to be conditionally independent. Secondly, the
eigenvalue scaling provides a panacea to allow unified scale parameters
for all bases of all smoothing functions of variables, where scaling is
mandate.

The rest of the hierarchical prior follows the traditional
spike-and-slab lasso piror: we set up the hyper-prior of
\(\boldsymbol{\gamma}\), to allow local adaption of the shrinkage, using
a bernoulli distribution. The two indicators of the \(j\)th predictor,
\(\gamma^{0}_j\) and \(\gamma^\text{pen}_j\), shares the same
probability parameter, \[
\begin{aligned}
\gamma_{j}^{0} | \theta_j &\sim Bin(\gamma^{0}_{j}|1, \theta_j)\\
\gamma_{j}^\text{pen}| \theta_j &\sim Bin(\gamma^\text{pen}_{j}|1, \theta_j).
\end{aligned}
\]

This is to leverage the fact that the probability of selecting the bases
of a smooth function should be similar, while allowing different penalty
on the null space and penalty space of the spline function. The hyper
prior of \(\gamma_{j}^{0}\) decides the sparsity of the model at the
function selection level, while that of \(\gamma_{j}^\text{pen}\)
decides the smoothness of the spline function at basis function level.
Meanwhile, we specify that \(\gamma_{j}^0\) and
\(\gamma_{j}^\text{pen}\) are independently distributed for analytic
simplicity. We further specify the parameter \(\theta_j\) follows a beta
distribution with given shape parameters \(a\) and \(b\), \[
\theta_j \sim Beta(a, b).
\] The beta distribution is a conjugate prior for the binomial
distribution and hence provides some computation convenience. For
simplicity, we focus on a special case of beta distribution, uniform
(0,1). When the variable have large effects in any of the bases, the
parameter \(\theta_j\) will be estimated large, which in turn encourages
the model to include the rest of bases, achieving the local adaption
among spline bases. Hereafter, we refer Bayesian hierarchical
generalized additive models with the spike-and-slab lasso prior as the
BHAM-SSL. \textbackslash{}

\begin{tikzpicture} [
staticCompo/.style = {rectangle, minimum width=1cm, minimum height=1cm,text centered, draw=black, fill=blue!30},
outCome/.style={ellipse, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=blue!30},
mymatrix/.style={matrix of nodes, nodes=outCome, row sep=1em},
PriorBoarder/.style={rectangle, minimum width=5cm, minimum height=10cm, text centered, fill=lightgray!30},
background/.style={rectangle, fill=gray!10,inner sep=0.2cm, rounded corners=5mm}
] 

\matrix (linearPrior) [matrix of nodes, column sep = 0mm, row sep = 0.7cm] {
&\node (linearTheta) [outCome] { $\theta_j^{0}\sim Beta(a, b)$ };\\
&\node (linearGamma) [outCome] { $\gamma_j^0 \sim Bin(1, \theta_j^0) $ };\\
&\node (linearBeta) [outCome] { $\beta_j^0 \sim DE(0,(1-\gamma^0_{j}) s_0 + \gamma^0_{j} s_1)$};\\
};

\matrix (penPrior) [right = 2cm of linearPrior, matrix of nodes, column sep = 0mm, row sep = 0.7cm] {
\node (penTheta) [outCome] { $\theta_{j}^\text{pen}\sim Beta(a, b)$ };\\
\node (penGamma) [outCome] { $\gamma_{j}^\text{pen}\sim Bin(1, \theta_j^\text{pen})$ };\\
\node (penBeta) [outCome] { $\beta_{jk}^\text{pen}\sim  DE(0,(1-\gamma^0_{j}) s_0 + \gamma^0_{j} s_1)$};\\
};

\node (ab)[staticCompo] at ($(linearTheta)!0.5!(penTheta)$) {$(a, b)$};
%\node (PenaltyPrior) [PriorBoarder,  right = 1cm of ab] {part 2};
\node (s) [staticCompo]  at ($(linearBeta)!0.5!(penBeta)$)  {($s_0, s_1$)};

\node (Beta) [staticCompo, below = 1cm of s] {$\boldsymbol{\beta }= (\beta^0_1, \boldsymbol{\beta}^\text{pen}_1, \dots,\beta^0_j, \boldsymbol{\beta}^\text{pen}_j , \dots,\beta^0_p, \boldsymbol{\beta}^\text{pen}_p) $};
\node (Y) [outCome, below = 1cm of Beta] {$y_i \sim Expo. Fam. (g(\eta_i))$};

%Lines

\draw[->] (linearTheta) -- (linearGamma);
\draw[->] (penTheta) -- (penGamma);
\draw[->] (linearGamma) -- (linearBeta) ;
\draw[->] (penGamma) -- (penBeta);
\draw[->] (ab) -- (linearTheta);
\draw[->] (ab) -- (penTheta);
\draw[->] (s) -- (linearBeta) ;
\draw[->] (s) -- (penBeta);
\draw[->] (linearBeta) -- (Beta);
\draw[->] (penBeta) -- (Beta);
\draw[->] (Beta) --  (Y);


% Now that the diagram has been drawn, background rectangles
% can be fitted to its elements. This requires the TikZ
% libraries "fit" and "background".
% Control input and measurement are labeled. These labels have
% not been translated to English as "Measurement" instead of
% "Messung" would not look good due to it being too long a word.
\begin{pgfonlayer}{background}
    \node [background,
                fit=(linearTheta) (linearBeta),
                label=above:Null Space:] {};
    \node [background,
                 fit=(penTheta) (penBeta),
                label=above:Penalized Space:] {};

\end{pgfonlayer}
\end{tikzpicture}

\hypertarget{other-priors}{%
\subsubsection{Other Priors}\label{other-priors}}

With the re-parameterization step of the basis function matrix \(X\), it
is possible to generalized the SSL prior to other priors, for example
normal priors for ridge-type regularization and mixture normal prior for
spike-and-slab regularization. These priors would work better in low and
medium dimensional settings where the sparse assumption is not
necessary. Here we focus on the mixture normal prior as a demonstration
of applying continous spike-and-slab prior in additive models.
\footnote{TODO: enter a sentence introducing EMVS and spike-and-slab
  mixture normal prior}.

A spike-and-slab mixture normal prior in additive models can be
expressed as \[
\begin{aligned}
  \beta^0_{j} |\gamma^0_{j},s_0,s_1 &\sim N(0,(1-\gamma^0_{j}) s_0 + \gamma^0_{j} s_1)\nonumber\\
  \beta^\text{pen}_{jk} | \gamma^\text{pen}_{j},s_0,s_1 &\sim N(0,(1-\gamma^\text{pen}_{j}) s_0 + \gamma^\text{pen}_{j} s_1).
\end{aligned}
\] Similar to the BHAM-SSL prior in Equation \ref{eq:bham_ssl},
\(0 < s_0 < s_1\) are tuning parameters and can be optimized via
cross-validation. One of the critics received by the spike-and-slab
mixture normal prior is that the tails of a normal distribution
diminishes to zero too fast, which renders problem when estimating the
large effects. Distributions with heavier tails can be used as an
alternative, for example mixture \(t\) distribution with small degree of
freedom.

\hypertarget{algorithms-for-fitting-bhams}{%
\subsection{Algorithms for Fitting
BHAMs}\label{algorithms-for-fitting-bhams}}

Given the proposed model specifications, these models can be easily
fitted with MCMC algorithms. Nevertheless,the computational burden MCMC
algorithms bear creates scalability issues. George and
McCulloch\citep{George1997} examined the computation speed for various
MCMC algorithms with continous spike-and-slab priors, and suggested MCMC
algorithms works well for medium size of predictors (\(p\)=25). However,
it is not feasible to high-dimensionality data analysis where the number
of predictors easily exceeds 100. Sepcific to additive models, each
predictor would expand to mutliple new ``predictors'' via basis
functions, creating greater computational demands. Scheipl et al.
\citep{Scheipl2013} demonstrated the computational demands of MCMC
algorithm for fitting spike-and-slab additive models grow exponentially
as \(p\) increases modestly via monte carlo simulation studies. Hence,
we feel compelled to develop scalable and parsimonious algorithms for
fitting Bayesian hierarchical additive models in high-dimensional
settings.

Alternative to the sampling algorithms for fitting Bayesian models,
optimization algorithms focus on the maximum a posteriori (MAP)
estimates and speed up the process at the cost of uncertainty measures.
The earliest work for fitting spike-and-slab models using optimization
algorithm is EMVS \citep{Rockova2014a}. Ro\{\v{c}\}kov\{'\{a\}\} and
George\citep{Rockova2014a} proposed an expectation-maximization (EM)
based algorithm to fit models that use continuous mixture normal priors.
In the E step, the latent variables \(\gamma\) are treated as the
missing data, and calculate the posterior mean of the density function;
in the M step, a ridge estimator was used to update the coefficients,
and \(\sigma^2, \theta\) are updated accordingly. The same authors
\citep{Rockova2018b, Rockova2018} further combined the EM algorithm with
coordinate descent (CD) algorithm for fitting SSL models. Yi and his
group incorporated the SSL prior in the EM-Iterative Weighted Least
Square(IWLS) algorithm and proposed another EM algorithm, EM-cyclic
coordinate descent, based on the Lasso penalty SSL exhibit. Such
applications were used in generalized linear models\citep{Tang2017a},
cox models \citep{Tang2017} and their grouped
counterparts\citep{Tang2018, Tang2019}. Both EM based algorithms provide
deterministic solutions, which becomes a more popular property as
reproducible results. Bai et al. \citep{Bai2020} provides an up-to-date
summary of SSL methods and theoretical discussion.

In the follow section, we expand the two EM-based algorithms, EM-CD and
EM-IWLS algorithms, to fit BHAMs. To note, the two proposed algorithm
provides different utilities. The EM-CD algorithm is specifically for
fitting BHAM-SSL with an expedited performance, recommending to use in
high and ultra-high dimensional setting. SB-GAM\citep{Bai2020, Bai2021}
also used EM-CD algorithm. The main difference between proposed EM-CD
algorithm and SB-GAM is that SB-GAM uses a blocked CD algorithm for
their group prior. In comparison, our prior is pairwise independent
requiring no special treatment. A specific concern of EM-CD algorithm is
that it provides no information for inference. In the contrast, the
EM-IWLS provides a complement to the problem. The EM-IWLS can easily
extract the variance-covariance matrix due to the IWLS algorithm, and
used to construct Wald type inference. Moreover, the EM-IWLS is a more
general model fitting algorithm that can be used for fitting not only
SSL and continuous SS priors but also Bayesian ridge and lasso priors.

\hypertarget{em-algorithms}{%
\subsubsection{EM algorithms}\label{em-algorithms}}

EM algorithm is an iterative algorithm to find the maximum likelihood
estimates or the Bayesian counterpart MAP estimates. It is commonly used
when some necessary data to establish the likelihood function are
missing. Instead, the algorithm maximizes the expectation, in respect to
the ``missing'' data, of the likelihood function.

The recursive algorithm consists of two steps:

\begin{itemize}
\tightlist
\item
  E-step: to calculate the expectation of the likelihood function
  conditioning on some ``missing'' data
\item
  M-step: to maximize the spectated likelihood function to calculate the
  parameter of interest
\end{itemize}

For BHAMs, we define the parameters of interest as
\(\Theta = {\boldsymbol{\beta}, \boldsymbol{\theta}, \phi}\) and
consider the latent binary indicators \(\boldsymbol{\gamma}\) as
nuisance parameters of the model. Our objective is to find the
parameters \(\Theta\) that maximize the posterior density function, or
equivalently, the logarithm of the density function, \[
\begin{aligned}
& \text{argmax}_{\Theta}
\log p(\Theta, \boldsymbol{\gamma}| \textbf{y}, \textbf{X}) \\
&= \log p(\textbf{y}|\boldsymbol{\beta}, \phi) + \log p(\phi) + \sum\limits_{j=1}^p\left[\log p(\beta^0_j|\gamma^0_j)+\sum\limits_{k=1}^{K_j} \log p(\beta^{pen}_{jk}|\gamma^{pen}_{jk})\right]\\
& +\sum\limits_{j=1}^{p} \left[ (\gamma^0_j+\gamma_{j}^{pen})\log \theta_j + (2-\gamma^0_j-\gamma_{j}^{pen}) \log (1-\theta_j)\right] +  \sum\limits_{j=1}^{p}\log p(\theta_j)
\end{aligned}
\]\\
For simplicity, we consider the null space to be 1-dimension.

We use the EM algorithm to find the MAP estimate of \(\Theta\). Since
the latent binary indicators \(\boldsymbol{\gamma}\) are not of our
primary interest, we treat them as the ``missing data'' in the EM
algorithms. Naturally In the E-step, we calculate the expectation of
posterior density function of
\(\log p(\Theta, \boldsymbol{\gamma}| \textbf{y}, \textbf{X})\) with
respect to the latent indicators \(\boldsymbol{\gamma}\) conditioning on
the values from previous iteration \(\Theta^{t-1}\), \[
E_{\boldsymbol{\gamma}|\Theta^{t-1}}\log p(\Theta, \boldsymbol{\gamma}| \textbf{y}, \textbf{X}) .
\] Hereafter, we use the short hand notation
\(E(\cdot)\equiv E_{\boldsymbol{\gamma}|\Theta^{t-1}}(\cdot)\). In the
M-step, we find the \(\Theta^{t}\) that maximize
\(E\log p(\Theta, \boldsymbol{\gamma}| \textbf{y}, \textbf{X})\). To
note here, the log-posterior density of BHAMs (up to additive constants)
can be written as a two-part equation

\[ \log p(\Theta, \boldsymbol{\gamma}| \textbf{y}, \textbf{X}) = Q_1(\beta, \phi) + Q_2 (\gamma,\theta),\]
Where
\[ Q_1(\boldsymbol{\beta}, \phi) = \log p(\textbf{y}|\boldsymbol{\beta}, \phi) + \log p(\phi) + \sum\limits_{j=1}^p\left[\log p(\beta^0_j|\gamma^0_j)+\sum\limits_{k=1}^{K_j} \log p(\beta^{pen}_{jk}|\gamma^{pen}_{jk})\right]\]
and \[
Q_2(\boldsymbol{\gamma},\boldsymbol{\theta}) = \sum\limits_{j=1}^{p} \left[ (\gamma^0_j+\gamma_{j}^{pen})\log \theta_j + (2-\gamma^0_j-\gamma_{j}^{pen}) \log (1-\theta_j)\right] +  \sum\limits_{j=1}^{p}\log p(\theta_j) .\]
\(Q_1\) and \(Q_2\) are, respectively, the log-likelihoods of model
coefficients and the dispersion parameter \(\boldsymbol{\beta}, \phi\),
and latent indicators \(\boldsymbol{\gamma}, \theta\). Conditioning on
\(\gamma_{jk}\), \(Q_1\) and \(Q_2\) are independent and can be
maximized separately for \(\boldsymbol{\beta}, \phi\) and
\(\boldsymbol{\theta}\).

The E- and M- steps are iterated until the algorithm converge. Depending
on the choice of the mixture priors, the maximization in the M-step can
take different algorithms, CD for faster computation and IWLS for
uncertainty measures.

\hypertarget{em-coordinate-descent}{%
\subsubsection{EM-Coordinate Descent}\label{em-coordinate-descent}}

When the prior distribution is set to mixture double exponential,
coordinate descent algorithm can be used to estimate
\(\boldsymbol{b}eta\), replacing IRLS, in the M-step. Coordinate descent
is an optimization algorithm that offers extreme computational
advantage, and famous for its application in optimizing the \(l_1\)
penalized likelihood function.

Recall, the density function of spike-and-slab mixture double
exponential prior can be written as \[
f(\beta|\gamma, s_0, s_1) = \frac{1}{(1-\gamma)s_0 + \gamma s_1}\exp(-\frac{|\beta|}{(1-\gamma)s_0 + \gamma s_1}),
\] and \(E(Q_1)\) can be expressed as a likelihood function with \(l_1\)
penalty \begin{equation}\label{eq:Q1_CD}
E(Q_1) = \log p(\textbf{y}|\boldsymbol{\beta}, \phi) + \log p(\phi) + \sum\limits_{j=1}^p\left[E({S^0_j}^{-1})|\beta^0_j|+\sum\limits_{k=1}^{K_j}E(S^{-1}_{j})|\beta_{jk}|\right],
\end{equation} where
\(S_{j}^0 = (1-\gamma^{0}_{j}) s_0 + \gamma^{0}_{j} s_1\) and
\(S_{j} = (1-\gamma^\text{pen}_{j}) s_0 + \gamma^\text{pen}_{j} s_1\).
To calculate two unknown quantities \(E({S_j^0}^{-1})\) and
\(E(S^{-1}_j)\), the posterior probability
\(p^0_{j} \equiv p(\gamma^{0}_{j}=1|\Theta^{t-1})\) and
\(p_{j} \equiv p(\gamma^\text{pen}_{j}=1|\Theta^{t-1})\) are necessary,
which can be derived via Bayes' theorem. The calculation for \(p_j\) is
slightly different from that of \(p^0_j\), as \(p_j\) depends on the
value of \(\beta_{jk}\) for \(k=1, \dots, K_j\) and \(p^0_j\) only
depends on \(\beta_j^0\). The calculation follows the equations below.

\[
\begin{aligned}
p_{j}^0 &= \frac{Pr(\gamma_{j}^0 = 1|\theta_j)f(\beta_{j}^0|\gamma_{j}^0=1, s_1) }{Pr(\gamma_{j}^0 = 1|\theta_j)f(\beta_{j}^0|\gamma_{j}^0=1, s_1) + Pr(\gamma_{j}^0 = 0|\theta_j)f(\beta^0_{j}|\gamma^0_{j}=0, s_0)}\\
p_{j} &= \frac{Pr(\gamma^{pen}_{j} = 1|\theta_j)\prod\limits_{k=1}^{K_j}f(\beta_{jk}|\gamma^{pen}_{j}=1, s_1) }{Pr(\gamma^{pen}_{j} = 1|\theta_j)\prod\limits_{k=1}^{K_j}f(\beta_{jk}|\gamma^{pen}_{j}=1, s_1) + Pr(\gamma^{pen}_{j} = 0|\theta_j)\prod\limits_{k=1}^{K_j}f(\beta_{jk}|\gamma^{pen}_{j}=0, s_0)}
\end{aligned}
\]

and
\(Pr(\gamma_{j}^{0} = 1|\theta_j) = Pr(\gamma_{j}^\text{pen}= 1|\theta_j) = \theta_j\),
\(Pr(\gamma_{j}^{0} = 0|\theta_j) = Pr(\gamma_{j}^\text{pen}= 0|\theta_j) = 1-\theta_j\),
\(f(\beta|\gamma=1, s_1) = \text{DE}(\beta|0 , s_1)\),
\(f(\beta|\gamma=0, s_0) = \text{DE}(\beta|0 , s_0)\). It is trivial to
show \[
\begin{aligned}
&E(\gamma^0_{j})  = p^0_{j} & &E(\gamma^{pen}_{j}) = p_{j}\\
&E({S^0}^{-1}_{j}) = \frac{1-p^{0}_{j}}{s_0} + \frac{p^{0}_{j}}{s_1} & &E(S^{-1}_{j}) = \frac{1-p_{j}}{s_0} + \frac{p_{j}}{s_1}.
\end{aligned}
\]

As the formulation of \(E(Q_1)\) can be seen as a \(l_1\) penalized
likelihood function, it can be optimized via coordinate descent
algorithm. After the \(\boldsymbol{\beta}\) are updated with the
coordinate descent algorithm, we update the hypor parameter
\(\boldsymbol{\theta}\). The remaining parameters of interest
\(\boldsymbol{\theta}\) can be updated by maximizing \(E(Q_2)\). As the
beta distribution is a conjugate prior for Bernoulli distribution,
\(\boldsymbol{\theta}\) can be easily updated with a closed form
equation:

\begin{equation}\label{eq:update_theta}
\theta_j = \frac{p^0_j + p_{j} + a - 1 }{a + b}.
\end{equation}

Totally, the framework of the proposed EM-coordinate descent algorithm
was summarized as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Choose a starting value \(\boldsymbol{\beta}^{(0)}\) and
  \(\boldsymbol{\theta}^{(0)}\) for \(\boldsymbol{\beta}\) and
  \(\boldsymbol{\theta}\). For example, we can initialize
  \(\boldsymbol{\beta}^{(0)} = \boldsymbol{0}\) and
  \(\boldsymbol{\theta}^{(0)} = \boldsymbol{0}.5\)
\item
  Iterate over the E-step and M-step until convergence
\end{enumerate}

E-step: calculate \(E(\gamma^0_{j})\), \(E(\gamma^{pen}_{j})\)and
\(E({S^0}^{-1}_{j})\) , \(E({S}^{-1}_{j})\)with estimates of
\(\Theta^{(t-1)}\) from previous iteration

M-step:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  Update \(\boldsymbol{\beta}^{(t)}\) using the coordinate descent
  algorithm
\item
  Update \(\boldsymbol{\theta}^{(t)}\) using Equation
  \ref{eq:update_theta}
\item
  Update the dispersion parameter \(\phi\) if exists.
\end{enumerate}

We assess convergence by the criterion:
\(|d^{(t)}-d^{(t-1)}|/(0.1+|d^{(t)}|)<\epsilon\), where
\(d^{(t)} = -2l(\beta^{(t)},\phi^{(t)})\) is the estimate of deviance at
the \(t\)th iteration, and \(\epsilon\) is a small value (say
\(10^{-5}\)).

\hypertarget{em-irls}{%
\subsubsection{EM-IRLS}\label{em-irls}}

Similar to the EM-CD algorithm, the EM-IRLS algorithm is an iterative
EM-based algorithm where iterative weighted least square algorithm are
used to find the estimate \(\boldsymbol{\beta}, \phi\) that maximizes
\(E(Q_1)\), instead of the coordinate descent algorithm. It can fit the
models with a wider range of priors, e.g.~mixture \(t\) prior, including
mixture normal prior as an special case, and mixture double exponential
prior. The idea is that the mixture priors can be expressed as a
hierarchical normal distribution with various forms of disperssion
distributions. \citep{Yi2012} Given the hierarchical normal prior of the
coefficients, a generalized model can be fitted using linear
approximation and least square algorithm. For simplicity, we demonstrate
using EM-IWLS algorithm to fit mixture normal priors, with the potential
of generalization following Yi and Ma \citep{Yi2012}.

Recall the prior set-up of mixture normal prior, we have the prior
densities of \(\boldsymbol{\beta}\) conditioning on
\(\boldsymbol{\gamma}\)

\[
\begin{aligned}
p(\boldsymbol{\beta }| \boldsymbol{\gamma}) &\propto \prod\limits_{j=1}^{p}\left[{S^0_j}^{-1/2}\exp(-1/2({\beta^0_{j}}^2/S^0_{j}))\prod\limits_{k=1}^{K_j}S_{j}^{-1/2}\exp(-1/2(\beta_{jk}^2/S_{j}))\right],
\end{aligned}
\] where \(S_{j}^0\) and \(S_{j}\) are defined the same as in Equation
\ref{eq:Q1_CD}. Similarly, \(E({S_j^0}^{-1})\) and \(E(S^{-1}_j)\) are
derived via Bayes' theorem, conditioning on \(p^0_j\) and \(p_j\). We
re-write \(E(Q_1)\) as a \(l_2\) penalized likelihood function, and
maximize it using the IWLS algorithm. The likelihood function can be
approximate by a weighted normal likelihood: \[
  p(\textbf{y}|\boldsymbol{\beta}, \phi) \approx Normal(\textbf{z}|X\beta, \phi\Sigma)
\] where the `normal response' \(z_i\) and `weight' \(w_i\) are called
the pseudo-response and pseudo-weight, respectively.The pseudo data and
the pseudo-weight are calculated by \[
\begin{aligned}
z_i &= \hat\eta_i - \frac{L^{'}((y_i|\hat\eta_i))}{L^{''}((y_i|\hat\eta_i))}& w_i &= - L^{''}((y_i|\hat\eta_i)),
\end{aligned}
\]

To note, as the EM-IWLS algorithm is based on the IWLS algorithm, an
Hessian matrix is yielded as a by-product of the model fitting. The
Hessian matrix can be further used to construct variance-covariacne
matrix of coefficient estimates, and in turn used for statistical
inferences.

\hypertarget{selecting-optimal-scale-values}{%
\subsection{Selecting Optimal Scale
Values}\label{selecting-optimal-scale-values}}

\label{sec:tune} Our proposed models, BHAM, require two preset scale
parameters (\(s_0\), \(s_1\)). Hence, we would construct a two
dimensional grid, consists of different pairs of (\(s_0\), \(s_1\))
value. However, previous research on the topic\footnote{TODO: add
  citations} suggested the value of slab scale \(s_1\) have less impact
on the final model and is recommended to be set as a generally large
value of \(s_1 = 1\) that provides no or weak shrinkage. More attention
is focused on examining different values of spike scale \(s_0\). Instead
of the 2-D grid, We consider a sequence of \(L\) decreasing values
\(\{s_0^l\}: 0 < s_0^1 < s_0^2 < \dots < s_0^L < s_1\). Increasing the
spike scale s\_0 tends to include more non-zero coefficients in the
model. An measure of preference, e.g.~area under the curve (AUC), mean
squared error, can be used to facilitate the selection of a final model.
The procedure is similar to the lasso implemented in the widely used R
package \texttt{glmnet}, which quickly fits the lasso model over a list
of values of \(\lambda\) covering the entire range, giving a sequence of
models for users to choose from.

\hypertarget{simulation-study}{%
\section{Simulation Study}\label{simulation-study}}

In this section, we compare the performance of the proposed models to
four alternative models: component selection and smoothing operator
(COSSO) \citep{Zhang2006GAM}, adaptive COSSO \citep{Storlie2011},
generalized additive models with automatic smoothing \citep{Wood2011},
SB-GAM \citep{Bai2021}. COSSO is one of the earliest smoothing spline
models that consider sparsity-smoothness penalty. Adaptive COSSO
improved upon COSSO by using adaptive weight for penalties such that the
penalty of each functional component are different for extra
flexibility. Generalized additive models with automatic smoothing,
hereafter \textit{mgcv}, is one of the most popular models for
non-linear effect interpolation and prediction. SB-GAM is the first
spike-and-slab lasso GAM. We implemented COSSO and adaptive COSSO with R
package \texttt{cosso 2.1-1}, generalized additive models with automatic
smoothing with R package \texttt{mgcv 1.8-31}, SB-GAM with R package
\text{sparseGAM 1.0}. COSSO models and SB-GAM do not provide flexibility
to define smoothing functions, and hence used the default choices. Both
mgcv and proposed models allow customized smoothing functions and we
chose the cubic regression spline. We controlled the dimensionality of
each smoothing function, 10 bases, for all different choices of
smoothing functions. 5-fold cross-validation were used for COSSO models,
SB-GAM and the proposed models for tuning parameter selection based on
the default selection criteria. 20 default candidates were examined for
SB-GAM and the proposed models which allow user-specification of tuning
candidates. All computation was conducted on a high-performance 64-bit
Linux platform with 48 cores of 2.70GHz eight-core Intel Xeon E5-2680
processors and 24G of RAM per core and R 3.6.2 \citep{R}.

Other related methods for high-dimensional GAMs also exist, notably the
methods of sparse additive models by Ravikumar et al.
\citep{Ravikumar2009}, stochastic search term selection for GAM
\citep{Scheipl2012}. However, we exclude these methods from current
simulation study because of demonstrated inferior predictive performance
compared to mgcv and scalability issues with increasing number of
predictors. \citep{Scheipl2013}

\hypertarget{monte-carlo-simulation-study}{%
\subsection{Monte Carlo Simulation
Study}\label{monte-carlo-simulation-study}}

We follow the data generating process described in Bai \citep{Bai2021}.
We first generated \(n=500\) training data points with
\(p=4, 10, 50, 200\) predictors respectively, where the predictors \(X\)
are simulated from a multivariate normal distribution
\(\text{MVN}_{n\times p}(0, I_{P})\). We then simulated the outcome
\(y\) from three distribution distributions, gaussian, binomial, and
poisson with the identity link, logit link
\(g(x) = \log(\frac{x}{1-x})\), and log link \(g(x) = log(x)\). The mean
of each outcome were simulated via the following function \[
\mathbb{E}(Y) = g^{-1}(5 \sin(2\pi x_1) - 4 \cos(2\pi x_2 -0.5) + 6(x_3-0.5) - 5(x_4^2 -0.3))
\] for gaussian and binomial outcomes and shrinked effects

\[
\mathbb{E}(Y) = g^{-1}(\sin(2\pi x_1) - 2\cos(2\pi x_2 -0.5) + 0.4(x_3-0.5) - 2(x_4^2 -0.3))
\] for poisson outcomes. Gaussian outcomes required specification of
dispersion, where we set the dispersion parameter to be 1. In this data
generating process, we have \(x_1, x_2, x_3, x_4\) as the active
covariates, while the rest covariates are inactive,
i.e.~\(f_j(x_j) = 0\) for \(j = 4, \dots, p\). Another set of
independent sample of size \(n_{test}=1000\),(\(x_{new} ,y_{new}\)), are
created following the same data generating process, serving as the
testing data. The dimensionality of the smooth functions are controlled
as k=10. Hence, the corresponding number of coefficient parameters to be
estimated is 37, 82, 451 and 1801.

To evaluate the performance of the models, mean squared error, mean
absolute error for gaussian model, AUC and misclassifcation rate are
calculated for binomial model, and deviance are calculated for Poisson
model, averaging across 50 simulations. The results are tabulated in
Table 1, 2, 3.

\includegraphics{SS_GAM_files/figure-latex/unnamed-chunk-7-1.pdf}

\includegraphics{SS_GAM_files/figure-latex/unnamed-chunk-8-1.pdf}

\includegraphics{SS_GAM_files/figure-latex/unnamed-chunk-9-1.pdf}

For the binomial outcome shown in Figure 1, we can see that the proposed
models are consistently performed better than COSSO and adaptive COSSO.
Mgcv slightly outperforms the proposed models in low dimension
settings(p = 4, 10), but suffers from overfitting in medium dimension
setting (p=50). In addition, in the high-dimension setting (p=200), mgcv
could not make prediction due to the p \textgreater{} n problem. Among
the proposed models, we see that there is modest difference between the
models that shares the same indicator variable('\_spline' models) and or
not sharing indicator variable (modles), where the not sharing indicator
variable models have slightly edge over indicator sharing model.
Moreover, the mixture double exponential model and lasso model perform
substaintially better in the medium and high settings. For the remaining
two outcomes, the results shows similar trends, where mixture double
exponential model and ss lasso model are recommended for use.

\%are calculated to assess out-of-sample estimation accuracy,

\%We also calculated the area under the curve (AUC) of the receiver
operating characteristic curve for \(y_{new}\). We will also visually
display the estimated smooth function with the smooth function of truth
to compare if the models can infer the smooth function correctly,
despite the prediction accuracy.

\%When constructing the smooth function of each variables, we constructs
the natural cubic spline with degree of freedom 25. In other words, the
design matrix of each smooth function will have 26 columns. The total
number of columns for \(p=4, 10, 50\) is \(105, 261, 1301\)
respectively, where intercept is kept in the model when fitting. The
\(p=50\) can mimic the high dimensional setting as the design matrix are
in high-dimensional setting (\(p >> n\))

\%We benchmark the performance of the proposed methods with the state of
art GAM package \texttt{mgcv}{[}TODO: add citation{]}. In the future
steps, we will compare our proposed model with other high dimensional
GAM applications.

~

\hypertarget{simulation-results}{%
\subsubsection{Simulation Results}\label{simulation-results}}

Overall, the proposed priors outperforms the state-of-art GAM model
\texttt{mgcv} when the dimensionality is low, median, and high. The
proposed priors predicts better than \texttt{mgcv} and infers better of
the underlying smooth function. Most importantly, \texttt{mgcv} is
infeasible in the high dimensional setting, i.e.~when \(p=50\) where the
number of parameters are greater than the sample size.

\hypertarget{prediction-performace}{%
\paragraph{Prediction Performace}\label{prediction-performace}}

The prediction performance is tabulated in the table below for AUC and
Supplementary tables for MSE and misclassification rate. In summary, the
proposed Bayesian models outperforms the benchmark model \texttt{mgcv}
regardless the choice of prior or fitting algorithm. The models with
IRLS fitting algorithm give slightly better performance than the the
model with cyclic coordinate descent algorithm. The advantage diminishes
in the high dimensional setting (p=50). This is possible because the SS
normal spline prior provides a more smooth solution, which would work
well in a low to median dimension setting. However, in the high
dimensional setting, there are not enough sparsity in the model, i.e.~SS
normal prior will provides a more complex model than necessary. This is
confirmed with the visualization in the following section.

\hypertarget{visualization}{%
\paragraph{Visualization}\label{visualization}}

We plot the estimated smooth functions of the four active variables for
the 100 simulation iterations, along with the `true' smooth function.
Across different settings of predictors, the patterns look similar.
First of all, in all the settings, \texttt{mgcv} tends to provide more
extreme models, where in the graphs the smooth function have much higher
bound than the true smooth function. Rarely the estimated smooth
function from \texttt{mgcv} matches with the true smooth function. In
comparison, the proposed method can interpolate the shape of the true
smooth function. However, different priors and fitting algorithms have
different advantages. For example, in Figure 1, we see that the SS
normal distribution have problem with the tail of the smooth function,
especially obvious in the estimations of the variable \emph{x4}.
However, it would have less sparse solutions, which is more likely to
accurately interpolate the smooth function especially when the shape is
complicated (see variable \emph{x2}). On the contrary, the SS double
exponential prior model encourages sparse solutions, which can sometime
omit active predictors.

\hypertarget{real-data-analysis}{%
\section{Real Data Analysis}\label{real-data-analysis}}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

In the paper, we described a novel generalized additive model using
Bayesian hierarchical priors, particularly the proposed spike-and-slab
spline prior for bi-level functional selection. Meanwhile, we introduced
two optimization based algorithms for model fitting. The algorithms can
be easily scale up to address high-dimensional data analysis in a
computational efficient manner, but also provide naive inferential
supports for many commonly used hierarchical priors,
e.g.~t-distribution, double exponential distribution, mixture t
distribution, and mixture double exponential distribution, in the
analyses that the sparse assumption is weak or not necessary. Via
simulations, we demonstrated that the proposed model provides as good,
if not better, prediction performance compared to some state-of-the-art
non-linear modeling devices.

The proposed model shares many commonality with an high-dimensional
Bayesian GAM, SB-GAM \citep{Bai2021}, independently developed around the
same time of this work. Both frameworks emphasize computational
efficiency by deploying group spike-and-slab lasso type of priors and
optimization-based fast and scalable algorithms. Bai provided the
theoretical proof for the consistency of variable selection using group
spike-and-slab lasso prior. Nevertheless, SB-GAM fails to address the
bi-level functional selection and the model inference. The proposed
model provides solutions to these remaining questions while maintaining
the same level of prediction accuracy. Moreover, the proposed model
renders additional flexibility to model specification, allowing various
choices of smoothing functions and degrees of freedom.

For transnational science purpose, we implemented the proposed model in
a R package \texttt{BHAM}, deposited at
\url{https://github.com/boyiguo1/BHAM}. To maximize the flexibility of
smoothing function specification, we deploy the same programming grammar
as in the state-of-the-art package \texttt{mgcv}, in contrast to
previous tools where smoothing functions are limited to the default
ones. Ancillary functions are provided for model specification in
high-dimensional settings, curve plotting and functional selection.

There are some improvements possible for the proposed models. First of
all, the proposed model achieves a bi-level selection via the two-part
spike-and-slab spline prior. Nevertheless, this set-up could result in a
situation that is not theoretically sound: the non-linear component is
selected, but the linear component is not. We currently address it
analytically by forcing to include the linear component when non-linear
component is included. Another possible solution is to impose an
dependent structure of \(\gamma_{j}^{pen}\) on \(\gamma_{j^{0}}\),
i.e.~\(\gamma_j^{pen}|\gamma_{j}^{0}, \theta_j\). Secondly, while the
proposed model addresses a great deal of analytic problem, analyzing the
time-to-event outcome remains unsolved. Our current effort resides on
expanding GAM to Cox proportional hazard models. An naive approach would
be convert an time-to-event outcome to a poisson outcome following
Whitehead \citep{Whitehead1980}. However, it would be more efficient to
directly fitting Cox models via penalized pseudo likelihood function.
Last but not least, we plan to expand the proposed model to integrate
external biology information. The main motivation for the integrative
model is that biologically informed groupping of weak effects increases
the power of detecting true associations between features and the
outcome \citep{Peterson2016}, and stabilizes the analysis results for
reproducibility purpose. Such integration can be achieved by setting up
a structural hyperprior on the inclusion indicator of the smoothing
function null space \(\boldsymbol{\gamma}^0\). The similar strategy has
been used in Ferrari and Dunson \citep{Ferrari2020}.

\bibliography{bibfile.bib}


\end{document}
