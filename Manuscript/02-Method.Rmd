\section{Bayesian Hierarchical Additive Models (BHAM)}
\label{sec:BHAM}

\bg{We assume the response variable, $Y$, follows an exponential family distribution with density function $f(y)$, mean $\mu$ and dispersion parameter $\phi$. The mean of the response variable can be modeled as the summation of smoothing functions, $B_j(\cdot), j = 1, \dots, p$, of a given $p$-dimensional vector of covariates $\bs x$, written as 
$$
 E(Y|\bs x) = g^{-1}(\beta_0 + \sum\limits^p_{j=1}B_j(x_j)) = g^{-1}(\beta_0 + \sum\limits^p_{j=1} \bs \beta_j^T \bs X_j),
$$
where $g^{-1}(\cdot)$ is the inverse of a monotonic link function. Given $n$ data points $\{y_i, \bs x_i\}^n_{i=1}$,} the data distribution is expressed as
\begin{equation}
f(\bs Y = \bs y| \bs \beta, \phi) = \prod\limits^n_{i=1}f( Y = y_i|\bs \beta, \phi).\nonumber
\end{equation}

The basis function matrix, i.e. the design matrix derived from the smoothing function $B_j(x_j)$, is denoted $\bs X_j$ for the variable $x_j$. The dimension of the design matrix depends on the choice of the smoothing function, and is denoted as $K_j$ for $x_j$.  $\bs \beta_j$ denotes the basis function coefficients for the $j$th variable such that $B_j(x_j) = \bs \beta_j^T \bs X_j$. With slight abuse of notation, we denote vectors and matrices in bold fonts $\bs \beta, \bs X$ with conformable dimensions, where scalar and random variables are denoted in unbold fonts $\beta, X$. The matrix transposing operation is denoted with a superscript $^T$. \bg{To note, the proposed model can include parametric forms of variables in the model, and hence considers general linear models and semiparametric regression models as special cases.}

\subsection{Smoothing Function Reparameterization} 

To encourage proper smoothing of each additive function, we adopt the smoothing penalty from smoothing spline models [TODOO: add Wood citation]. A smoothing penalty, a function of the integration of the second derivative of the spline function.  is quadratic norms of the coefficients \bg{of what?} and allow locally adaptive penalties on \bg{each smoothing function[TODO: not accurate, bases]}, mathematically
\begin{equation}\label{eq:smoothpen}
  \text{pen}\left[B_j(x)\right] = \lambda_j \int B^{\prime\prime}_j(x)dx = \lambda_j \bs \beta_j^T \bs S_j \bs \beta_j ,
\end{equation}
where $\bs S_j$ is a known smoothing penalty matrix and $\lambda_j$ denotes a smoothing parameter. A linear function can be modeled as $B_j(x_j) = x_j$ with the smoothing penalty matrix defined as $\bs S_j = \begin{bmatrix}0\end{bmatrix}$. \bg{Unlike previous methods that embeds smoothing penalty in the sparse penalty which leads to a more restrictive solution, we considers an additional mechanism via the propsed prior (discussed in Section xxx) to address sparsity in signal. such that the locally adpative nature of the smoothing penalty retains.}

Marra and  Wood [@Marra2011] proposed a re-parameterization procedure \bg{to factor the smoothing penalty into the design matrix[TODO: vague]}. Given the smoothing penalty matrix $\bs S_j$ is symmetric and positive semi-definite for univariate smoothing functions, we eigendecompose the penalty matrix $\bs S = \bs U \bs D \bs U^T$ , where the matrix $\bs D$ is diagonal with the eigenvalues arranged in the ascending order. To note, $\bs D$ can contain elements of zeros on the diagonal, where the zeros are associated with the linear space of the smoothing function. For the most popular smoothing function, cubic splines, the dimension of the linear space is one. Hereafter, we focus on discussing a uni-dimensional linear space for simplicity; however, it generalizes easily to the cases where the linear space is multidimensional. We further write the orthonormal matrix $\bs U \equiv \begin{bmatrix} \bs U^0 : \bs U^{\tp}\end{bmatrix}$ containing the eigenvectors as columns in the corresponding order to $\bs D$. That is, $\bs U$ contains the eigenvectors $U^0$ with zero eigenvalues for the linear space and $\bs U^{\tp}$ contains the eigenvectors (as columns) for the non-zero eigenvalues, i.e. the non-linear space. We multiply the basis function matrix $\bs X$ with the orthonormal matrix $\bs U$ for the new design matrix ${\bs X}^\repa = \bs X \bs U \equiv \begin{bmatrix} X^0 : \bs X^{\tp} \end{bmatrix}$. An additional scaling step is imposed on $\bs X^{\tp}$ by the non-zero eigenvalues of $\bs D$ such that the new basis function matrix $\bs X^\ast$ can receive uniform penalty on each of its dimensions. With slight abuse of the notation, we drop the superscript $^\repa$ and denote $\bs X_j \equiv \begin{bmatrix} X_j^0 : \bs X_j^{\tp} \end{bmatrix}$ as the basis function matrix for the $j$th variable after the re-parameterization. A spline function can be expressed in the matrix form
$$
B_j(x_j) = B_j^0(x_j) + B_j^\tp (x_j) = \beta_j X^0_j + \bs {\beta_j^\tp}^T \bs X_j^\tp ,
$$
and the generalized additive model in Equation (\ref{eq:gam}) now is
\begin{equation}\label{eq:gam-repa}
E(Y|\bs x) = g^{-1}(\beta_0 + \sum\limits^p_{j=1} B_j(x_j)) = g^{-1}(\beta_0 + \sum\limits^p_{j=1} \bs \beta_j^T \bs X_j) = g^{-1}\left[\beta_0 + \sum\limits^p_{j=1} (\beta_j X^0_j + {\bs \beta_j^\tp}^T \bs X_j^\tp)\right],
\end{equation}
where the coefficients $\bs \beta_j \equiv \begin{bmatrix} \beta_j : \bs \beta^\tp_j \end{bmatrix}$ is an augmentation of the coefficient scalar $\beta_j$ of linear space and the coefficient vector $\bs \beta^\tp_j$ of non-linear space.

To summarize, the re-parameterization step provides three benefits. First of all, the re-parameterization integrates the smoothing penalty into the design matrix, and encourages models to properly smooth the nonlinear function in addition to the sparse penalty for functional selection. Secondly, the eigendecomposition of the smoothing penalty allows the isolation of the linear space from the nonlinear space, improving the feasibility of bi-level functional selection. The eigendecomposition facilitates the construction of orthonormal design matrix, which makes imposing independent priors on the coefficients possible. This reduces the computational complexity compared to using a multivariate priors, and greatly broadens the choices of priors and further model choices. Last but not least, the scaling step of $\bs X^\tp$ further simplifies the choice of priors, from column-specific priors to a unified prior. 

\subsection{Two-part Spike-and-Slab Lasso Prior for Smoothing Fucntions}

The family of spike-and-slab (SS) regression models is one of most commonly used models in high-dimensional data analysis for its utility in outcome prediction and variable selection. \bg{Among all the spike-and-slab priors, the spike-and-slab Lasso (SSL) prior [@Rockova2018b; @Rockova2018] is one of the most popular choices. The SSL prior is composed of two double exponential distributions with mean 0 and different dispersion parameters, $0 < s_0 < s_1$, mathematically,
\begin{equation} 
\beta | \gamma \sim (1-\gamma)DE(0, s_0) + \gamma DE(0, s_1), 0 < s_0 < s_1.\nonumber
\end{equation}
}
The latent binary variable $\gamma \in \{0,1\}$ indicates whether a variable $x$ is included in the model, while the dispersion parameters $s_0$ and $s_1$ controls the shrinkage of the coefficient. Given that both double exponential distributions have a mean of 0 and the latent indicator $\gamma$ can only take the value of 0 or 1, the mixture double exponential distribution can be formulated as one single double exponential density,
\begin{equation} \label{eq:ssl}
\beta | \gamma \sim DE(0, (1-\gamma)s_0 + \gamma s_1), 0 < s_0 < s_1.
\end{equation}
Compared to other priors for high-dimensional data analysis, SSL has the following advantages.First of all, the spike-and-slab double exponential prior provides a locally adaptive shrinkage when estimating the coefficients. Hence, the smoothing functions can be estimated more accurately. Secondly, the spike-and-slab exponential prior encourages a sparse solution, making variable selection straight forward. Thirdly, the spike-and-slab double exponential prior motivates a scalable algorithm, the EM-CD algorithm, for model fitting, and hence is more feasible for high-dimensional data analysis. We defer to Bai et al. [@Bai2021Review] for an detailed discussion. Particularly, the development of SSL model substantially improves the scalability of SS models, setting up the theoretical foundation for generalized models in -omics data analysis [@Tang2017a; @Tang2017; @Tang2018; @Tang2019]. We notice that Bai[TODO: add Bai arxiv paper] is the first to apply spike-and-slab lasso prior in the GAM framework, where the densities of the spike and slab components take the group lasso density [TODO: add Xu Ghosh (2015)] and limits to an “all-in-all-out” strategy for functional selection.

We introduce a novel SSL-based prior for smoothing functions in GAMs. Given the re-parameterized design matrix $\bs X_j = \begin{bmatrix} X^0_j : \bs X_j^\tp \end{bmatrix}$ for the $j$th variable, we impose a two-part SSL prior to the coefficients $\bs \beta_j = \begin{bmatrix} \beta_j : \bs \beta_j^\tp \end{bmatrix}$. Specifically, the linear space coefficient has a SSL prior and the nonlinear space coefficients shares a group SSL prior,
\begin{align}\label{eq:bham_ssl}
  \beta_{j} | \gamma_{j},s_0,s_1 &\sim DE(0,(1-\gamma_{j}) s_0 + \gamma_{j} s_1) \nonumber \\
  \beta^\tp_{jk} | \gamma^\tp_{j},s_0,s_1 &\simiid DE(0,(1-\gamma^\tp_{j}) s_0 + \gamma^\tp_{j} s_1), k=1,\dots, K_j
\end{align}
where $\gamma_{j}\in\{0,1\}$ and $\gamma^\tp_{j}\in \{0,1\}$ are two latent indicator variables, indicating if the model includes the linear effect and the nonlinear effect of the $j$th variable respectively.$s_0$ and $s_1$ are scale parameters, assuming $0 < s_0 < s_1$ and given. These scale parameters $s_0$ and $s_1$ can be treated as tuning parameters and optimized via cross-validation, discussed in Section \ref{sec:tune}. 

\bg{The group SSL prior of the nonlinear space coefficients differs from previous group SSL priors [@Tang2018; @Tang2019], as effect hierarchy are preferred when selecting linear effect and non-linear effect. Effect hierarchy refers to the principle that lower-order effects are more likely to be active than higher-order effects. [TODO: add chipman paper citation.] To implement, we consider the hyper prior of the shared latent indicator $\gamma^\tp_j$} depends on the value of the linear space latent indicator $\gamma_j$. Both latent indicators $\gamma_j and \gamma^\tp_j$ follows a Bernoulli distribution, written as binomial distribution of one trial. While the probability of including the linear effect is $\theta_j$, a probability parameter, the probability of including the nonlinear effect is $\bg{\gamma_{j}}\theta_j$.}
$$
\begin{aligned}
&\gamma_{j} | \theta_j \sim Bin(1, \theta_j) & & 
&\gamma_{j}^\tp | \bg{\gamma_{j}, } \theta_j \sim Bin(1, \bg{\gamma_{j}}\theta_j).
\end{aligned}
$$
\bg{This is, when the linear effect is not selected, the probability of including the nonlinear effect drops from $\theta_j$ to 0. For the computational convenience in the proposed EM-CD algorithm (see Section [TODO: add section ref]), we analytically integrate $\theta_j$ out such that $\gamma_{j}^\tp | \theta_j \sim Bin(1, \theta_j^2)$.
}

<!-- To note, this prior differs from previous group SSL priors [@Tang2018; @Tang2019], as the $\beta_j$ and $\bs \beta^\tp_j$ have different indicator variables $\gamma_j$, $\gamma_j^\tp$ respectively. It is possible to add a more restrictive assumption on the priors, assuming that one indicator variable decides the inclusion of both the linear effect and nonlinear effect, i.e. $\gamma_j = \gamma^\tp_j$.  This converges to the SB-GAM [@Bai2021]. Conversely, it is also possible to relax the assumption such that each coefficient $\beta_{jk} \in \bs \beta^\tp_j$ has its own latent indicator $\gamma_{jk}$, but at the cost of complicating the bi-level functional selection. This reduces the proposed prior to the classic SSL prior. -->

<!-- The re-parameterization introduced in Section \ref{sec:BHAM} grants the validity of the proposed prior. First of all, the smoothing function bases are linear dependent and necessitate extra attention. The eigeondecomposition remedies the problem and hence our prior can be set to be conditionally independent. Secondly, the eigenvalue scaling provides a panacea to allow unified scale parameters for all bases of all smoothing functions.  -->
 
<!-- The rest of the hierarchical prior follows the traditional SSL prior: we set up hyper-priors on $\gamma_j, \gamma^\tp_j$ to allow local adaption of the shrinkage using a Bernoulli distribution, written as binomial distribution of one trial. The two indicators of the $j$th predictor, $\gamma_j$ and $\gamma^\tp_j$, shares the same probability parameter $\theta_j$, -->



<!-- This is to leverage the fact that the probability of selecting the bases of a smoothing function should be similar, while allowing different penalty on the linear space and non-linear space of the smoothing function. The hyper prior of $\gamma_{j}$ decides the sparsity of the model at the functional selection level, while that of $\gamma_{j}^\tp$ decides the smoothness of the spline function at basis function level.  -->

\bg{To allow the shrinkage to self-adapt to the sparsity and smoothing pattern of the data, }we further specify the parameter $\theta_j$ follows a beta distribution with given shape parameters $a$ and $b$,
$$
\theta_j \sim Beta(a, b).
$$
The beta distribution is a conjugate prior for the binomial distribution and hence provides some computation convenience. For simplicity, we focus on a special case of beta distribution, uniform (0,1), i.e. $a = 1, b = 1$. \bg{This enables the proposed prior to inherit the selective shrinkage property and self-adaptivity [TODO: add Bai review] from the classical SSL prior. In other words, when a smoothing function is significant, the coefficients of the smoothing function will escape the overall shrinkage and produce a more accurate estimate, particular with the smoothing penalty addressed implicitly via the reparameterization. Meanwhile, automatic adjustment for different level of sparsity are feasible via the information borrowing cross coordinates.}
<!-- This informaiton borrowing helps the model to find adequate shrinkage for the linear coefficient and coefficients of the nonlinear part. Not that, the reparameterazation step factor the smoothing penalty matrix into the design matrix, and we don't have to further explicitly arrange a multivariate piror to consider appropriate shrinkage of those coefficient.} -->
<!-- When the variable has large effect in any of the bases, the parameter $\theta_j$ will be estimated large, which in turn encourages the model to include the rest of bases.  -->
Hereafter, we refer Bayesian hierarchical generalized additive models with the two-part spike-and-slab LASSO prior as the BHAM, and visually presented in Figure \ref{fig:SSprior}.
\begin{center}
Figure \ref{fig:SSprior} here
\end{center}


\subsection{Scalable EM-Coordinate Descent Algorithm}

\bg{Despite the advantage to estimate posterior densities and derive uncertainty inference, using MCMC algorithms to fit the proposed model is computational prohibited and not feasible for high-dimensional data. Previous research shows the computation performance of MCMC algorithms for spike-and-slab models is bottlenecked for medium size data ($p$=25) [@George1997], and substaintially slows as $p$ increases modestly in hte GAM context [@Scheipl2013]. Hence, we considers optimizaiton algorithms that focus on the maximum a posteriori estimates at the cost of posterior inference. Optimizaiton algorithms greatly speed up the model fitting process, and improve the scalability of BHAM.}

In this section, we extend the EM-Coordinate Descent (EM-CD) algorithm to fit BHAMs. \bg{Specifically for the SSL priors, Rochova and co-authors [@Rockova2018b; @Rockova2018] further integrate the EM algorithm and coordinate descent algorithm for fast computation.
Yi and his group independently developed the EM-Iterative Weighted Least Square and EM-cyclic coordinate descent algorithms to fit models with broader class of priors, SSL included.[@Yi2019] These algorithms were implemented for generalized linear models[@Tang2017a], Cox proportional hazards models [@Tang2017] and their grouped counterparts[@Tang2018; @Tang2019]. Both EM based algorithms provide deterministic solutions, which becomes a popular property for reproducible research.} To note, SB-GAM[@Bai2020; @Bai2021] also used an EM-CD algorithm. The main difference between proposed EM-CD algorithm and that in SB-GAM is that SB-GAM uses a block CD algorithm for their group prior, while the proposed prior is pairwise independent requiring no special treatment in the CD algorithm. 

<!-- The proposed models can be fitted with MCMC algorithms. Nevertheless, the computational burden of MCMC algorithms creates scalability issues. George and McCulloch examined the computation speed for various MCMC algorithms with spike-and-slab mixture normal priors, and suggested MCMC algorithms works well for medium size ($p$=25) of predictors with only linear effects. However, it is not feasible for high-dimensional data analysis where the number of predictors easily exceeds 100. Specific to additive models, each predictor would expand to multiple new "predictors" via basis functions, creating greater computational demands. Scheipl et al. [@Scheipl2013] demonstrated the computational demands of a MCMC algorithm for fitting spike-and-slab GAM grow exponentially as $p$  via simulation studies. Hence, we feel compelled to develop scalable algorithms for fitting Bayesian hierarchical additive models in high-dimensional settings.  -->

<!-- As an alternative to sampling algorithms for fitting Bayesian models, optimizatio algorithms focus on the maximum a posteriori (MAP) estimates and speed up the model fitting process at the cost of posterior inference. The earlier work for fitting SS models using optimization algorithms includes EMVS [@Rockova2014a]. Rockova and George[@Rockova2014a] proposed an expectation-maximization (EM) based algorithm to fit models that use continuous mixture normal priors.  -->
<!-- The coefficient $\beta_j$ independently follows a mixture normal distribution $N(0, \sigma^2((1-\gamma_i)v_0 + \gamma_i v_1)$ for $0 \leq v_0 < v_1$. The parameter $\sigma^2$ follows a inverse gamma prior, $IG(v/2, v\lambda /2)$. Like in the spike-and slab family, the latent binary variables $\bs \gamma$ follows a Bernoulli prior with a hyper prior $\theta$ follows a beta distribution, or simpler the uniform (0,1).  -->
<!-- In the E step, the latent binary indicators $\gamma$s are treated as the missing data, and the posterior means are calculated conditioning on the current value of other parameters; in the M step, a ridge estimator was used to update the coefficients, followed by updating $\phi, \theta$.  -->


<!-- EM algorithm is an iterative algorithm to find MAP estimates or the maximum likelihood estimates. It is commonly used when some necessary data to establish the likelihood function are missing. Instead of maximizing the the likelihood function, the algorithm maximizes the expectation of the likelihood function with respect to the "missing" data.  -->

<!-- The recursive algorithm consists of two steps:  -->

<!-- * E-step: to calculate the expectation of the posterior density function with respect to some "missing" data -->
<!-- * M-step: to maximize the expectation derived in the E-step and update parameters of interest -->

For BHAMs, we define the parameters of interest as $\Theta = \{\bs \beta, \bs \theta, \phi\}$ and consider the latent binary indicators $\bs \gamma$ as nuisance parameters of the model, in other words the "missing" data\bg{} for the EM algorithm}. Our objective is to find the parameters $\Theta$ that maximize the posterior density function, or equivalently, the logarithm of the density function, 
$$
\begin{aligned}
& \text{argmax}_{\Theta}
\log f(\Theta, \bs \gamma| \textbf{y}, \textbf{X}) \\
&= \log f(\textbf{y}|\bs \beta, \phi) + \sum\limits_{j=1}^p\left[\log f(\beta_j|\gamma_j)+\sum\limits_{k=1}^{K_j} \log f(\beta^{\tp}_{jk}|\gamma^{\tp}_{j})\right]\\
& +\sum\limits_{j=1}^{p} \left[ (\gamma_j+\gamma_{j}^{\tp})\log \theta_j + (2-\gamma_j-\gamma_{j}^{\tp}) \log (1-\theta_j)\right] +  \sum\limits_{j=1}^{p}\log f(\theta_j),
\end{aligned}
$$  
where $f(\textbf{y}|\bs \beta, \phi)$ is the data distribution and $f(\theta)$ is the Beta(\bg{a},\bg{b}) density. We choose non-informative prior for the intercept $\beta_0$ and the dispersion parameter $\phi$; for example, $f(\beta_0|\tau_0^2)=N(0,\tau_0^2)$ with $\tau^2_0$ set to a large value and $f(\log \phi) \propto 1$.

We use the EM algorithm to find the the maximum a posteriori estimate of $\Theta$. This is, in the E-step, we calculate the expectation of posterior density function of $\log f(\Theta, \bs \gamma| \textbf{y}, \textbf{X})$ with respect to the latent indicators $\bs \gamma$ conditioning on the values from previous iteration $\Theta^{(t-1)}$, 
$$
E_{\bs \gamma|\Theta^{(t-1)}}\log f(\Theta, \bs \gamma| \textbf{y}, \textbf{X}) .
$$
Hereafter, we use the shorthand notation $E(\cdot)\equiv E_{\bs \gamma|\Theta^{(t-1)}}(\cdot)$. In the M-step, we find the $\Theta^{(t)}$ that maximize $E\log f(\Theta, \bs \gamma| \textbf{y}, \textbf{X})$. \bg{The parenthesized subscription denotes the parameter estimation at the $t$th iteration.} The E- and M- steps are iterated until the algorithm converge.

To note here, the log-posterior density of BHAMs (up to additive constants) can be written as a two-part equation
$$ \log f(\Theta, \bs \gamma| \textbf{y}, \textbf{X}) = Q_1(\bs \beta, \phi) + Q_2 (\bs \gamma,\bs \theta),$$
where
$$ Q_1\equiv Q_1(\bs \beta, \phi) = \log f(\textbf{y}|\bs \beta, \phi) + \sum\limits_{j=1}^p\left[\log f(\beta_j|\gamma_j)+\sum\limits_{k=1}^{K_j} \log f(\beta^{\tp}_{jk}|\gamma^{\tp}_{jk})\right]$$ and 
$$Q_2 \equiv Q_2(\bs\gamma,\bs\theta) = \sum\limits_{j=1}^{p} \left[ (\gamma_j+\gamma_{j}^{\tp})\log \theta_j + (2-\gamma_j-\gamma_{j}^{\tp}) \log (1-\theta_j)\right] +  \sum\limits_{j=1}^{p}\log f(\theta_j).$$
$Q_1$ and $Q_2$ are respectively the log posterior density of the coefficients $\bs \beta$ and the log posterior density of the probability parameters $\bs \theta$ conditioning on $\bs \gamma$. Meanwhile, conditioning on $\bs \gamma$, $Q_1$ and $Q_2$ are independent and can be maximized separately for $\bs \beta, \phi$ and $\bs \theta$. With the proposed two-part spike-and-slab lasso prior, $Q_1$ can be treated as penalized likelihood function and maximization of $E(Q_1)$ can be solved via the Coordinate Descent algorithm in each iteration. Coordinate descent is an optimization algorithm that offers extreme computational advantage, and famous for its application in optimizing the $l_1$ penalized likelihood function. Maximization of $E(Q_2)$ can be solved via closed form equations following the beta-binomial conjugate relationship.

The density function of the mixture double exponential prior of coefficient $\beta$ can be written as
$$
f(\beta|\gamma, s_0, s_1) = \frac{1}{2\left[(1-\gamma)s_0 + \gamma s_1\right]}\exp(-\frac{|\beta|}{(1-\gamma)s_0 + \gamma s_1}),
$$
and $E(Q_1)$ can be expressed as a log-likelihood function with $l_1$ penalty
\begin{equation}\label{eq:Q1_CD}
E(Q_1) = \log f(\textbf{y}|\bs \beta, \phi) - \sum\limits_{j=1}^p\left[E({S_j}^{-1})|\beta_j|+\sum\limits_{k=1}^{K_j}E({S^{\tp}}^{-1}_{j})|\beta_{jk}|\right],
\end{equation}
where $S_{j} = (1-\gamma_{j}) s_0 + \gamma_{j} s_1$ and $S^\tp_{j} = (1-\gamma^\tp_{j}) s_0 + \gamma^\tp_{j} s_1$. To calculate two unknown quantities $E({S_j}^{-1})$ and $E({S^\tp}^{-1}_j)$, the posterior probability $p_{j} \equiv \pr(\gamma_{j}=1|\Theta^{(t-1)})$ and $p_{j}^\tp \equiv \pr(\gamma^\tp_{j}=1|\Theta^{(t-1)})$ are necessary, which can be derived via Bayes' theorem. The calculation of $p_j^\tp$ is slightly different from that of $p_j$, as $p_j^\tp$ depends on the value of the vector $\bs \beta^\tp_{j}$ and $p_j$ only depends on the scalar $\beta_j$. The calculation follows the equations below.
\begin{align*}
p_{j} &= \frac{\pr(\gamma_{j} = 1|\theta_j)f(\beta_{j}|\gamma_{j}=1, s_1) }{\pr(\gamma_{j} = 1|\theta_j)f(\beta_{j}|\gamma_{j}=1, s_1) + \pr(\gamma_{j} = 0|\theta_j)f(\beta_{j}|\gamma_{j}=0, s_0)}\\
p^\tp_{j} &= \frac{\pr(\gamma^{\tp}_{j} = 1|\theta_j)\prod\limits_{k=1}^{K_j}f(\beta_{jk}|\gamma^{\tp}_{j}=1, s_1) }{\pr(\gamma^{\tp}_{j} = 1|\theta_j)\prod\limits_{k=1}^{K_j}f(\beta_{jk}|\gamma^{\tp}_{j}=1, s_1) + \pr(\gamma^{\tp}_{j} = 0|\theta_j)\prod\limits_{k=1}^{K_j}f(\beta_{jk}|\gamma^{\tp}_{j}=0, s_0)}
\end{align*}
and $\pr(\gamma_{j} = 1|\theta_j) = \theta_j$, $\pr(\gamma_{j} = 0|\theta_j) = 1-\theta_j$, $\bg{\pr(\gamma_{j}^\tp = 1|\theta_j) = \theta_j^2}$, $\bg{\pr(\gamma_{j}^\tp = 0|\theta_j) = 1-\theta^2_j}$, $f(\beta|\gamma=1, s_1) = \text{DE}(\beta|0 , s_1)$, $f(\beta|\gamma=0, s_0) = \text{DE}(\beta|0 , s_0)$. It is trivial to show
\begin{align}\label{eq:exp_scale}
&E(\gamma_{j})  = p_{j} & &E(\gamma^{\tp}_{j}) = p_{j}^{\tp}\nonumber\\
&E({S}^{-1}_{j}) = \frac{1-p_{j}}{s_0} + \frac{p_{j}}{s_1} & &E({S^\tp}^{-1}_{j}) = \frac{1-p_{j}^{\tp}}{s_0} + \frac{p_{j}^{\tp}}{s_1}.
\end{align}
After replacing withe calculated quantities, $E(Q_1)$ can be seen as a $l_1$ penalized likelihood function with the regularization parameter $\lambda = E(S^{-1})$, and hence be optimized via coordinate descent algorithm [@Friedman2010]. Independently, the remaining parameters of interest $\bs \theta$ can be updated by maximizing $E(Q_2)$. As the beta distribution is a conjugate prior for Bernoulli distribution, $\bs \theta$ can be easily updated with a closed form equation,
\begin{equation}\label{eq:update_theta}
\theta_j = \frac{p_j + p^\tp_{j} + a - 1 }{a + b}.
\end{equation}

Totally, the proposed EM-coordinate descent algorithm is summarized as follows:

1) Choose a starting value $\bs \beta^{(0)}$ and $\bs \theta^{(0)}$ for $\bs \beta$ and $\bs \theta$. For example, we can initialize $\bs \beta^{(0)} = \bs 0$ and $\bs \theta^{(0)} = \bs 0.5$

2) Iterate over the E-step and M-step until convergence

    E-step: calculate $E(\gamma_{j})$, $E(\gamma^\tp_{j})$ and $E({S}^{-1}_{j})$, $E({S^\tp}^{-1}_{j})$ with estimates of $\Theta^{(t-1)}$ from previous iteration

    M-step:

    a) Update $\bs \beta^{(t)}$, and the dispersion parameter $\phi^{(t)}$ if exists, using the coordinate descent algorithm with the penalized likelihood function in Equation (\ref{eq:Q1_CD})

    b) Update $\bs \theta^{(t)}$ using Equation (\ref{eq:update_theta})

We assess convergence by the criterion: $|d^{(t)}-d^{(t-1)}|/(0.1+|d^{(t)}|)<\epsilon$, where $d^{(t)} = -2\log f(\textbf{y}| \textbf{X}, \bs\beta^{(t)},\phi^{(t)})$ is the estimate of deviance at the $t$th iteration, and $\epsilon$ is a small value (say $10^{-5}$).


    



\subsection{Selecting Optimal Scale Values}
\label{sec:tune}
Our proposed models, BHAM, require two preset scale parameters ($s_0$, $s_1$). Hence, we need to find the optimal values for the scale parameters such that the model reaches its best prediction performance regarding a criteria of preference. This would be achieved by constructing a two-dimensional grid, consists of different pairs of ($s_0$, $s_1$) value. However, previous research suggested the value of slab scale $s_1$ have less impact on the final model and is recommended to be set as a generally large value, e.g. $s_1 = 1$, that provides no or weak shrinkage. [@Rockova2018] As a result, we focus on examining different values of spike scale $s_0$. Instead of the two-dimensional grid, we consider a sequence of $L$ decreasing values $\{s_0^l\}: 0 < s_0^1 < s_0^2 < \dots < s_0^L < s_1$. Increasing the spike scale $s_0$ tends to include more non-zero coefficients in the model. A measure of preference calculated with cross-validations (CV), e.g. deviance, area under the curve (AUC), mean squared error, can be used to facilitate the selection of a final model. The procedure is similar to the Lasso implemented in the widely used R package `glmnet`, which quickly fits Lasso models over a list of values of regularization parameters $\lambda$ , giving a sequence of models for users to choose from.