---
title: Spike-and-Slab Generalized Additive Models and Scalable Algorithms for High-Dimensional Data
subtitle: "Supporting Infromation"
author: Boyi Guo, Byron C. Jaeger, AKM Fazlur Rahman, D. Leann Long, Nengjun Yi
output: 
  pdf_document:
    keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


```
\newcommand{\tp}{*}
\newcommand{\pr}{\text{Pr}}
\newcommand{\bs}[1]{\boldsymbol{#1}}

\subsection{Supplementary Information 1: Marginal Distribution of $\gamma_j^\tp$}
Given that $\gamma_{j}^\tp | \gamma_{j}, \theta_j \sim Bin(1, \gamma_{j}\theta_j)$ where $\gamma_{j} | \theta_j \sim Bin(1, \theta_j)$, we can derive the the marginal distribution of $\gamma_{j}^\tp$ with the following manipulation.
\begin{align*}
& \pr(\gamma_j^\tp = 1 | \theta_j)  = \pr(\gamma_j^\tp = 1, \gamma_j = 1 | \theta_j) + \pr(\gamma_j^\tp = 1 , \gamma_j = 0| \theta_j)\\
= & \pr(\gamma_j^\tp = 1, \gamma_j = 1 | \theta_j) + 0 \qquad \text{[hierarchical structure between }\gamma^\tp \text{ and }\gamma \text{.]}\\
= & \pr(\gamma_j^\tp = 1| \gamma_j = 1,  \theta_j)\pr(\gamma_j = 1| \theta_j)\\
= & \theta_j^2\\
& \pr(\gamma_j^\tp = 0 | \theta_j)  = \pr(\gamma_j^\tp = 0, \gamma_j = 1 | \theta_j) + \pr(\gamma_j^\tp = 0 , \gamma_j = 0| \theta_j)\\
= & \pr(\gamma_j^\tp = 0, \gamma_j = 1 | \theta_j) + \pr(\gamma_j^\tp = 0, \gamma_j = 0 | \theta_j)\\
= & \pr(\gamma_j^\tp = 0| \gamma_j = 1,  \theta_j)\pr(\gamma_j = 1| \theta_j) + \pr(\gamma_j^\tp = 0| \gamma_j = 0,  \theta_j)\pr(\gamma_j = 0| \theta_j)\\
= & (1-\theta_j)\theta_j + 1(1-\theta_j) = 1-\theta_j^2
\end{align*}

\clearpage

\subsection{Supplementary Information 2: EM-IWLS Algorithm for Fitting Bayesian Hierarchical Additive Models}

Similar to the EM-CD algorithm, the EM-IWLS algorithm is an iterative EM-based algorithm where the iterative weighted least squares algorithm is used to find the estimate of $\bs \beta, \phi$ that maximizes $E(Q_1)$. The iterative weighted least squares algorithm was originally proposed to fit the classical generalized linear models, and generalized to fit some Bayesian hierarchical models.[@Gelman2013] Yi and Ma [@Yi2012] formulated Student's t-distribution and double exponential distribution as hierarchical normal distributions such that generalized linear models with shrinkage priors can be easily fitted using IWLS in combination with EM algorithm. In this work, we adapt the EM-IWLS paradigm to fit BHAM with spike-and-slab spline prior
<!-- , with the full potential to adopt Student's t, double exponential and mixture Student's t priors when sparse assumption is not necessary -->
.

<!-- The primary purpose of the EM algorithm here is mitigate the computational burden associate with the hierarchical normal formulation.  -->

A double exponential prior, $\beta|S \sim DE(0, S)$ can be formulated as a hierarchical normal prior with unknown variance $\tau^2$ integrated out:
\begin{align*}
  \beta|\tau^2 &\sim N(0, \tau^2)\\
  \tau^2|S & \sim Gamma(1, 1/(2S^2)), 
\end{align*}
For the mixture double exponential priors, we can define the scale parameter $S = (1-\gamma)s_0 + \gamma s_1$ following Equation (\ref{eq:ssl}). The change in the prior formulation in turn leads to the change in the log posterior density function, as $Q_1$ needs to account for the hyperprior of $\tau^2$: 
\begin{equation}\label{eq:Q1_IWLS}
Q_1(\bs \beta, \phi) = \log f(\textbf{y}|\bs \beta, \phi) + \sum\limits_{j=1}^p\left[\log f(\beta_j|{\tau}^2_j) + \log f({\tau}^2_j| S_j)+\sum\limits_{k=1}^{K_j} \{\log f(\beta^{\tp}_{jk}|{\tau^{\tp}}^2_{jk})+\log f({\tau^\tp}^2_{jk}| S^\tp_j)\}\right].
\end{equation}
Since $\bs \tau^2$ are not of our primary interest, we treat them as the "missing" data in addition to the latent indicators $\bs \gamma$, and hence construct the expectation $E_{\bs \gamma, \bs \tau^2|\Theta^{(t-1)}}(Q_1)$ in the E-step. To note, unlike the same latent indicator $\gamma^\tp_j$ which is shared by the coefficients of the non-linear terms $\beta^\tp_{jk}$ for $k = 1, \dots, K_j$ , $\tau^2_{jk}$ is coefficient specific for $\beta^\tp_{jk}$. $E({S_j}^{-1}|\beta_j, s_0, s_1), E({S^\tp}^{-1}_j|\bs \beta_j^\tp, s_0, s_1), E({\tau}^2_{j}|S_j, \beta_j) \text{ and } E({\tau^\tp}^2_{jk}|S_j^\tp, \beta^\tp_{jk})$ needs to be calculated to formulate $E(Q_1)$. As neither $E({S_j}^{-1}|\beta_j, s_0, s_1)$ nor $E({S^\tp}^{-1}_j|\bs \beta_j^\tp, s_0, s_1)$ depends on $\tau^2$s, they can be derived using Equation (\ref{eq:exp_scale}). On the other hand, $\tau^{2}$, following gamma distributions, is a conjugate prior for the normal variance, and the conditional posterior density of $\tau^{-2}$ is an inverse Gaussian distribution. $E({\tau}^{-2}_{j})$ and $E({\tau^\tp}^{-2}_{jk})$ are calculated using the closed form equation
\begin{align*}
 E({\tau}^{-2}_{j}|S_j, \beta_j) ={S_j}^{-1}/|\beta_j| \qquad E({\tau^\tp}^{-2}_{jk}|S_j^\tp, \beta^\tp_{jk})={S_j^\tp}^{-1}/|\beta^\tp_{jk}|,
\end{align*}
where $S_j$ and $S_j^\tp$ are replaced by the expectation and $\beta$s are replaced with $\beta^{(t-1)}$. With simplification (up to constant additive terms), we have 
\begin{equation}\label{eq:EQ1_IWLS}
E(Q_1) = \log f(\textbf{y}|\bs \beta, \phi) - \sum\limits_{j=1}^p\left[ {2E({\tau_j}^{-2})}{\beta_j}^2 +\sum\limits_{k=1}^{K_j} {2E({\tau_{jk}^\tp}^{-2})}{\beta_{jk}^\tp}^2\right].
\end{equation}
<!-- $E(Q_1)$ can be seen as a $l_2$ penalized likelihood function where the regularization parameter $\lambda = 2E({\tau}^{-2})$. Equivalently, we have  -->
$2E({\tau}^{-2})\beta^2$ can be seen as the kernel of a normal density with mean 0 and variance $E(\tau^{2})$, and we can formulate the coefficients $\bs \beta$ as a multivariate normal distribution with means $\bs 0$ and variance covariance matrix $\bs \Sigma_{\tau^2}$, where $\bs \Sigma_{\tau^2}$ is a diagonal matrix with $E(\tau^2)$s on the diagonal,
$$
\bs \beta \sim \text{MVN}(0, \bs \Sigma_{\tau^2}).
$$

Meanwhile, following the classical IWLS, we can approximate the generalized model likelihood at each iteration with a weighted normal likelihood:
$$
f(\textbf{y}|\bs \beta, \phi) \approx \text{MVN}(\textbf{z}|\bs X \bs \beta, \phi\bs \Sigma )
$$
where the ‘normal response’ $z_i$ and ‘weight’ $w_i$ are called the pseudo-response and pseudo-weight respectively. The pseudo-response and the pseudo-weight are calculated by
$$
\begin{aligned}
z_i &= \hat\eta_i - \frac{L^{'}(y_i|\hat\eta_i)}{L^{''}(y_i|\hat\eta_i)}& w_i &= - L^{''}(y_i|\hat\eta_i),
\end{aligned}
$$
where $\hat\eta_i = (\bs X {\hat{\bs\beta}})_i$, $L^{'}(y_i|\hat\eta_i, \hat \phi)$ and $L^{''}(y_i|\hat\eta_i, \hat \phi)$ are the first and second derivative of the log density, $\log f(\textbf{y}_i|\bs \beta, \phi)$ with respect to $\eta_i$. 

With $\bs z\sim \text{MVN}(\bs X \bs \beta, \phi \bs \Sigma)$ and $\bs \beta \sim \text{MVN}(0, \phi \bs \Sigma_{\tau^2})$, we can augment the two multivariate normal distributions and update the estimates for $\bs \beta$ and $\phi$ via least squares in each iteration of the EM algorithm. We create the augmented response, augmented data, and augmented variance-covariance matrix following \begin{align*}
& \bs z_* = \begin{bmatrix} \bs z\\ \bs 0\end{bmatrix} &&
  \bs X_* = \begin{bmatrix} \bs X \\ \bs I \end{bmatrix} &&
  \bs \Sigma_* = \begin{bmatrix} \bs \Sigma & \bs 0  \\ \bs 0 & \bs \Sigma_{\tau^2}/\phi \end{bmatrix}, &
\end{align*}
 such that 
$$
\bs z_* \sim \text{MVN}(\bs X_* \bs \beta , \phi \Sigma_*).
$$
Using the least squares estimators to update $\bs\beta$ and $\phi$, we have 
\begin{align*}
& \bs \beta^{(t)} = (\bs X_*^T \bs \Sigma^{-1} \bs X_*)^{-1}\bs X_*^T \bs \Sigma^{-1} \bs z_* && \phi^{(t)} = \frac{1}{n}(\bs z_*-X_*\bs \beta^{(t)})^T\bs \Sigma^{-1}(\bs z_*-X_*\bs \beta^{(t)}).&
\end{align*}
To note, the variance-covariance matrix of the coefficient estimates variance-covariance matrix can be derived in the EM-IWLS algorithm and in turn can be used for statistical inferences,
$$
  \text{Var}(\bs\beta^{(t)}) = (\bs X_*^T\bs \Sigma^{-1} \bs X_*)^{-1}\phi^{(t)}.
$$

Totally, the proposed EM-IWLS algorithm is summarized as follows:

1) Choose a starting value $\bs \beta^{(0)}$ and $\bs \theta^{(0)}$ for $\bs \beta$ and $\bs \theta$. For example, we can initialize $\bs \beta^{(0)} = \bs 0$ and $\bs \theta^{(0)} = \bs 0.5$

2) Iterate over the E-step and M-step until convergence

    E-step: calculate $E(\gamma_{j})$, $E(\gamma^\tp_{j})$ and $E(\tau^{-2}_{j})$, $E({\tau^\tp}^{-2}_{jk})$ with the estimates $\Theta^{(t-1)}$ from the previous iteration

    M-step:

    a) Based on the current value of $\beta$, calculate the pseudo-data $z_i^{(t)}$ and the pseudo-weights $w_i^{(t)}$
    b) Update $\bs \beta^{(t)}$ by runing the augmented weighted least squared
    c) If $\phi$ is present, update $\phi$

Similar to EM-CD, we assess convergence by the criterion, $|d^{(t)}-d^{(t-1)}|/(0.1+|d^{(t)}|)<\epsilon$, where $\epsilon$ is a small value (say $10^{-5}$).


<!--     where $\eta_i = \sum\limits_{j=1}^p \hat f_j^{(t)}(x_{ij})$ is the estimated linear predictor using current estimated value of $\bs \beta^{}$, $L^{'}$ and $L^{''}$ are the first derivative and second derivative of the log-likelihood function. The $l_2$ penalty, $\sum\limits_{j=1}^{p}\left[E({S_j}^{-1}){\beta_j}^2+\sum\limits_{k=1}^{K_j}E(S^{-1}_{j})\beta_{jk}^2\right]$ can be seems as a multivariate normal distribution of mean $\textbf{0}$ and variance-covariance matrix $\Sigma(E({S}_1^{-1}), E(S_1^{-1}), \dots, E({S_p}^{-1}), E(S_p^{-1})$, conditioning on the value of $E({S_1}^{-1}), \dots E({S_p}^{-1})$. An augmentation step combines the normal approximation and multivariate normal for $\bs \beta$, similarly to Ridge regression model fitting. The estimates of $\bs \beta$ and $\phi$ can be quickly updated in the IRLS algorithm. -->


<!-- From Equation^[TODO: add equation number], we can see that the estimates of $\gamma_jk, S^{-1}_{jk}$ are larger for larger coefficients $\beta_{jk}$, leading to different shrinkage for different coefficients. Moreover, to note that, we have different shrinkage $S^{-1}_{jk}$for the coefficients $\beta_{jk}$ of the variable $x_j$, and hence, we can penalize the null space of the spline differently and allow local adaption. --> 


<!-- Totally, the framework of the proposed EM IRLS algorithm was summarized as follows: -->

<!-- 1) Choose a starting value $\bs \beta^{(0)}$ and $\bs \theta^{(0)}$ [for $\bs \beta$ and $\bs \theta$. For example, we can initialize $\bs \beta^{(0)} = \bs 0$ and $\bs \theta^{(0)} = \bs 0.5$ -->

<!-- 2) Iterate over the E-step and M-step until convergence -->

<!-- E-step: calculate $E(\gamma_{j})$, $E(\gamma^{pen}_{j})$and $E({S}^{-1}_{j})$  , $E({S}^{-1}_{j})$with estimates of $\Theta^{(t-1)}$ from previous iteration -->

<!-- M-step: -->

<!-- a) Update $\bs \beta^{(t)}$ using the IRLS algorithm -->

<!-- b) Update $\bs \theta^{(t)}$ using Equation \ref{eq:update_theta} -->

<!-- We assess convergence by the criterion: -->
<!-- $|d^{(t)}-d^{(t-1)}|/(0.1+|d^{(t)}|)<\epsilon$, where -->
<!-- $d^{(t)} = -2l(\beta^{(t)},\phi^{(t)})$ is the estimate of deviance at -->
<!-- the $t$th iteration, and $\epsilon$ is a small value (say -->
<!-- $10^{-5}$). -->

\subsubsection{Other Priors}

With the re-parameterization step of the basis function matrix $\bs X$, it is possible to generalized the SSL prior to other priors, for example normal priors for ridge-type regularization and mixture normal prior for spike-and-slab regularization. These priors would work better in low and medium dimensional settings where the sparse assumption is not necessary. Here we elaborate the mixture normal prior as a demonstration of applying continuous spike-and-slab prior in BHAM.

A spike-and-slab mixture normal spline prior can be expressed as 
\begin{align*}
  \beta_{j} |\gamma_{j},s_0,s_1 &\sim N(0,(1-\gamma_{j}) s_0 + \gamma_{j} s_1)\\
  \beta^\tp_{jk} | \gamma^\tp_{j},s_0,s_1 &\simiid N(0,(1-\gamma^\tp_{j}) s_0 + \gamma^\tp_{j} s_1), k=1,\dots, K_j.
\end{align*}
Similar to the spike-and-slab spline prior in Equation (\ref{eq:bham_ssl}), $0 < s_0 < s_1$ are tuning parameters and can be optimized via cross-validation. One of the critics received by the spike-and-slab mixture normal prior is that the tails of a normal distribution diminishes to zero too fast, which causes problems when estimating the large effects. Distributions with heavier tails can be used as an alternative, for example mixture Student's $t$ distribution with small degree of freedom. 

\clearpage

\subsection{Supplementary Information 3: Predictive Performance of Linear Simulations}
\input{Tabs/sim_lnr_gaus_tab.tex}
\input{Tabs/sim_lnr_binom_tab.tex}

\clearpage

\subsection{Supplementary Information 4: Variable Selection Performance of Simulations}
\input{Tabs/sim_binom_var_slct_tab.tex}
\input{Tabs/sim_lnr_gaus_var_slct_tab.tex}
\input{Tabs/sim_lnr_binom_var_slct_tab.tex}


