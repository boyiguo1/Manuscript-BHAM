\section{Simulation Study}
\label{sec:sim}

In this section, we compare the performance of the proposed models to four alternative models: component selection and smoothing operator (COSSO) [@Zhang2006GAM], adaptive COSSO [@Storlie2011], generalized additive models with automatic smoothing [@Wood2011], SB-GAM [@Bai2021]\bg{, linear lasso model and spikeSlabGAM [TODO: add citation here].[TODO: explain about linear lasso model and spikeSlabGAM model]}. COSSO is one of the earliest smoothing spline models that consider sparsity-smoothness penalty. Adaptive COSSO improved upon COSSO by using adaptive weight for penalties such that the penalty of each functional component are different for extra flexibility. Generalized additive models with automatic smoothing, hereafter \textit{mgcv}, is one of the most popular models for nonlinear effect interpolation and prediction. \bg{mgcv doesn't support analyses where the number of parameters exceeds the sample size.} SB-GAM is the first spike-and-slab lasso GAM. We implement COSSO and adaptive COSSO with R package \texttt{cosso 2.1-1}, generalized additive models with automatic smoothing with R package \texttt{mgcv 1.8-31}, SB-GAM with R package \texttt{sparseGAM 1.0}. COSSO models and SB-GAM do not provide flexibility to define smoothing functions, and hence use the default choices. Both mgcv and proposed models allow customized smoothing functions and we choose the cubic regression spline. We controll the dimensionality of each smoothing function, 10 bases, for all different choices of smoothing functions. We use 5-fold CV with the default selection criteria to select the final model for COSSO models, SB-GAM and the proposed models. 20 default candidates of tuning parameters ($s_0$ in BHAM, $\lambda_0$ in SB-GAM) are examined for SB-GAM and the proposed models which allow user-specification of tuning candidates. All computation was conducted on a high-performance 64-bit Linux platform with 48 cores of 2.70GHz eight-core Intel Xeon E5-2680 processors and 24G of RAM per core and R 3.6.2 [@R].

Other related methods for high-dimensional GAMs also exist, notably the methods of sparse additive models by Ravikumar et al. [@Ravikumar2009] and stochastic search term selection for GAM [@Scheipl2012]. However, we exclude these methods from current simulation study because of demonstrated inferior predictive performance compared to mgcv and scalability issues with increased number of predictors. [@Scheipl2013]


\subsection{Monte Carlo Simulation Study}
We follow the data generating process described in Bai [@Bai2021]. We first generate $n=500$ training data points with $p=4, 10, 50, 100, 200$ predictors respectively, where the predictors $X$ are simulated from a multivariate normal distribution $\text{MVN}_{n\times p}(0, I_{P})$. We then simulate the outcome $y$ from two distributions, Gaussian and binomial with the identity link and logit link $g(x) = \log(\frac{x}{1-x})$ respectively. The mean of each outcome were simulated via the following function
$$
\mathbb{E}(Y) = g^{-1}(5 \sin(2\pi x_1) - 4 \cos(2\pi x_2 -0.5) + 6(x_3-0.5) - 5(x_4^2 -0.3))
$$
for Gaussian and binomial outcomes. Gaussian outcomes requires specification of dispersion, where we set the dispersion parameter to be 1. In this data generating process, we have $x_1, x_2, x_3, x_4$ as the active covariates, while the rest covariates are inactive, i.e. $f_j(x_j) = 0$ for $j = 4, \dots, p$. Another set of independent sample of size $n_{test}=1000$, are created following the same data generating process, serving as the testing data. We generate 50 independent pairs of training and testing datasets to evaluate the prediction performance of the chosen models, where training datasets are used to fit the models and testing datasets used to calculate assessment measures. \bg{In addition, we considered the data generating process where the all the functional forms of the predictors are linear while keeping the rest of simulation parameters keep the same. These additional sets of simulations are designed to investigate the flexibility of the proposed model when non-linear assumptions are not met.}

To evaluate the predictive performance of the models, the statistics, $R^2$ for Gaussian model and AUC for binomial model calculated based on the testing dataset, are averaged <!--and deviance are calculated for Poisson model,--> across 50 simulations. \bg{To evaluate the variable selection performance of the models, we record the set of variables each model selects and calculate the averaged false discover rate (FDR), false negative rate(FNR), and Matthews correlation coefficient (MCC). For method that doesn't automatically achieve variable selection, we set alpha level at 0.05 for mgcv that relies on hypothesis testing for variable selection, and soft-threshold at 0.5 for spikeSlabGAM given the marginal inclusion probabilities. For the two methods, BHAM and spikeSlabGAM, that are capable of bi-level selection, we record the probability that the linear and nonlinear components of each predictors are selected in the models.}
$$
\bg{MCC = \frac{TP\times TN - FP \times FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}},}
$$
\bg{where TP, TN, FP, and FN are true positives, true negatives, false positives, and false negatives, respectively.}
\subsection{\bg{Simulation Results}}
\subsubsection{\bg{Prediction Performance}}
\bg{Among the main set of simulations where the functional forms of the predictors are mostly non-linear,} the predictive performances have a consistent pattern across the two distributions of outcomes. (See Tables \ref{tab:gaus} and \ref{tab:bin_auc}) \bg{The proposed model, BHAM, predicts as good as, if not better than, other high dimensional spline models. Specifically, BHAM shows greate improvement over COSSO methods [TODO: add statsitics] and the spikeSlabGAM model [TODO: add statsitics]. When comparing to SB-GAM, BHAM performs better in lower dimensional cases ($p=4,10$), and equally good or slightly worse [TODO: re-evaluate after tuning the models] in high-dimensional cases ($p=50,100, 200$). As previously hypothesized, the linear lasso model predicts less accurate compared to other flexibly models across all scenario; mgcv performs extremely well in low-dimensional case ($p = 4, 10$), and deteriorates as the dimensionality increases until not applicable. To note, mgcv still fit models with increasing in large dimension case ($p=50$). But the algorithm fails to converge within the default number of iterations, and hence leads to bad prediction performance.} In high dimensional cases where we mimic the situation the signals are extremely sparse, SB-GAM has better performance than the proposed method. However, the BHAM-CD has extreme computational advantage over SB-GAM (see Table \ref{tab:time_sim}) without sacrificing much of the prediction accuracy. 

\bg{We also examine the prediction performance when the functional form of predictors are linear. (See support information Table 1, and 2 [TODO: add hyperlink to the github repo]) The proposed model, BHAM, has similar performance as the linear lasso model regardless of the type outcome. This observation demonstrates that BHAM is a flexible model, and has good prediction performance regardless the underlying functional form of predictors. spikeSlabGAM have similar prediction performance to BHAM (numeralize the advantage). Surprisingly, SB-GAM doesn't perform well in high-dimensional Gaussian outcome scenarios. (numeralize the advantage)}


\bg{\subsubsection{Variable Selection Performance}
While the main utility of the proposed work is prediction, the proposed model can also serve as variable selection/functional selection as a by-product. Hence We evaluate the variable selection performance. }

\begin{itemize}
\item \bg{If we use mcc as the measure, SBGAM has the best performance among all the method of comparison, followed by the proposed model BHAM. The pattern is consitent regardless of the type of outcomes. To note here, mgcv is not included in the performance table, as the mgcv isn't compatable with hign-dimensitonal data. Nevertheless, when the dimensionality is low, mgcv perfoms while for variable selection.}
\item \bg{If we look at the precision of the variable selection, BHAM performs better than SBGAM model particularaly when the dimensionality increases. While the recall is lower than SBGAM. This translate to the selection of BHAM is more conservative that SBGAM, such that among the selected variables, many of them are signals, however, due to the conversative selection, there are few variables are selected.}
\item \bg{We also want to high light that in the set of linear simulation, when the outcome is cgaussina, the proposed model performed extremely while. On the contrary the SBGAM is not so well.}
\end{itemize}




\bg{\subsubsection{Bi-level Selection}
Among the methods of comparison, there are two methods is possible for bi-level selection, i.e. the proposed BHAM and spikeSlabGAM. Hence, in this section, we further investigate the bi-level selection of the proposed models. } 



<!-- #### Visualization -->
<!-- We plot the estimated smooth functions of the four active variables for the 100 simulation iterations, along with the 'true' smooth function. Across different settings of predictors, the patterns look similar. First of all, in all the settings, `mgcv` tends to provide more  extreme models, where in the graphs the smooth function have much higher bound than the true smooth function. Rarely the estimated smooth function from `mgcv` matches with the true smooth function. In comparison, the proposed method can interpolate the shape of the true smooth function. However, different priors and fitting algorithms have different advantages. For example, in Figure 1, we see that the SS normal distribution have problem with the tail of the smooth function, especially obvious in the estimations of the variable _x4_. However, it would have less sparse solutions, which is more likely to accurately interpolate the smooth function especially when the shape is complicated (see variable _x2_). On the contrary, the SS double exponential prior model encourages sparse solutions, which can sometime omit active predictors. -->

