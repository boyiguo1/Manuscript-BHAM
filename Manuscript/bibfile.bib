@article{Wood2020,
  title={Inference and computation with generalized additive models and their extensions},
  author={Wood, Simon N},
  journal={Test},
  volume={29},
  number={2},
  pages={307--339},
  year={2020},
  publisher={Springer}
}


@article{Cleveland1979,
abstract = {The visual information on a scatterplot can be greatly enhanced, with little additional cost, by computing and plotting smoothed points. Robust locally weighted regression is a method for smoothing a scatterplot, (xi, yi), i = 1, {\ldots}, n, in which the fitted value at zkis the value of a polynomial fit to the data using weighted least squares, where the weight for (xi, yi) is large if xiis close to xkand small if it is not. A robust fitting procedure is used that guards against deviant points distorting the smoothed points. Visual, computational, and statistical issues of robust locally weighted regression are discussed. Several examples, including data on lead intoxication, are used to illustrate the methodology. {\textcopyright} 1979, Taylor {\&} Francis Group, LLC.},
author = {Cleveland, William S.},
doi = {10.1080/01621459.1979.10481038},
file = {:C$\backslash$:/Users/boyiguo1/Downloads/2286407.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Graphics,Nonparametric regression,Robust estimation,Scatterplots,Smoothing},
number = {368},
pages = {829--836},
title = {{Robust locally weighted regression and smoothing scatterplots}},
volume = {74},
year = {1979}
}

@article{Breiman2001,
  title={Statistical modeling: The two cultures (with comments and a rejoinder by the author)},
  author={Breiman, Leo},
  journal={Statistical science},
  volume={16},
  number={3},
  pages={199--231},
  year={2001},
  publisher={Institute of Mathematical Statistics}
}


@article{Hastie1987,
abstract = {Generalized additive models have the form $\eta$(x) = $\alpha$ + $\sigma$ fj(xj), where $\eta$ might be the regression function in a multiple regression or the logistic transformation of the posterior probability Pr(y = 1 x) in a logistic regression. In fact, these models generalize the whole family of generalized linear models $\eta$(x) = $\beta$′x, where $\eta$(x) = g($\mu$(x)) is some transformation of the regression function. We use the local scoring algorithm to estimate the functions fj(xj) nonparametrically, using a scatterplot smoother as a building block. We demonstrate the models in two different analyses: a nonparametric analysis of covariance and a logistic regression. The procedure can be used as a diagnostic tool for identifying parametric transformations of the covariates in a standard linear analysis. A variety of inferential tools have been developed to aid the analyst in assessing the relevance and significance of the estimated functions: these include confidence curves, degrees of freedom estimates, and approximate hypothesis tests. The local scoring algorithm is analogous to the iterative reweighted least squares algorithm for solving likelihood and nonlinear regression equations. At each iteration, an adjusted dependent variable is formed and an additive regression model is fit using the backfitting algorithm. The backfitting algorithm cycles through the variables and estimates each coordinated function by smoothing the partial residuals. {\textcopyright} 1976 Taylor {\&} Francis Group, LLC.},
author = {Hastie, Trevor and Tibshirani, Robert},
doi = {10.1080/01621459.1987.10478440},
file = {:C$\backslash$:/Users/boyiguo1/Downloads/2289439.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Generalized linear model,Logistic regression,Nonparametric regression,Smooth},
number = {398},
pages = {371--386},
title = {{Generalized additive models: Some applications}},
volume = {82},
year = {1987}
}




@article{Breiman1985,
  title={Estimating optimal transformations for multiple regression and correlation},
  author={Breiman, Leo and Friedman, Jerome H},
  journal={Journal of the American statistical Association},
  volume={80},
  number={391},
  pages={580--598},
  year={1985},
  publisher={Taylor \& Francis Group}
}

@article{Mallick2013,
author = {Mallick, Himel and Yi, Nengjun},
doi = {10.4172/2155-6180.S1-005},
file = {:C$\backslash$:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Manuscript - 2014 - NIH Public Access.pdf:pdf},
issn = {21556180},
journal = {Journal of Biometrics {\&} Biostatistics},
keywords = {bayesian,bayesian hierarchical models,bayesian model selection,bayesian subset regression,consistency,high dimensional linear models,mcmc,nonlocal priors,penalized regression,posterior,regularization,shrinkage methods,variable selection},
number = {205},
pages = {1--27},
title = {{Bayesian Methods for High Dimensional Linear Models}},
url = {https://www.omicsonline.org/bayesian-methods-for-high-dimensional-linear-models-2155-6180.S1-005.php?aid=15156},
year = {2013}
}


@article{Ravikumar2009,
abstract = {We present a new class of methods for high dimensional non-parametric regression and classification called sparse additive models. Our methods combine ideas from sparse linear modelling and additive non-parametric regression. We derive an algorithm for fitting the models that is practical and effective even when the number of covariates is larger than the sample size. Sparse additive models are essentially a functional version of the grouped lasso of Yuan and Lin. They are also closely related to the COSSO model of Lin and Zhang but decouple smoothing and sparsity, enabling the use of arbitrary non-parametric smoothers. We give an analysis of the theoretical properties of sparse additive models and present empirical results on synthetic and real data, showing that they can be effective in fitting sparse non-parametric models in high dimensional data. {\textcopyright} 2009 Royal Statistical Society.},
annote = {Ravikumar, Lafferty, Lue and Wasserman, extended the orginal backfitting algorithm to accommadate the high-dimensional setting. The sparsity and smoothness was handled saparately with a softthreshold. The authors also deomonstrated its similarity to Group Lasso},
archivePrefix = {arXiv},
arxivId = {0711.4555},
author = {Ravikumar, Pradeep and Lafferty, John and Liu, Han and Wasserman, Larry},
doi = {10.1111/j.1467-9868.2009.00718.x},
eprint = {0711.4555},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ravikumar et al. - 2009 - Sparse additive models.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Additive models,Lasso,Non-parametric regression,Sparsity},
month = {nov},
number = {5},
pages = {1009--1030},
title = {{Sparse additive models}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2009.00718.x},
volume = {71},
year = {2009}
}

@article{Yuan2006,
  title={Model selection and estimation in regression with grouped variables},
  author={Yuan, Ming and Lin, Yi},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={68},
  number={1},
  pages={49--67},
  year={2006},
  publisher={Wiley Online Library}
}


@article{Huang2010,
  title={Variable selection in nonparametric additive models},
  author={Huang, Jian and Horowitz, Joel L and Wei, Fengrong},
  journal={Annals of statistics},
  volume={38},
  number={4},
  pages={2282},
  year={2010},
  publisher={NIH Public Access}
}

@article{Xue2009,
  title={Consistent variable selection in additive models},
  author={Xue, Lan},
  journal={Statistica Sinica},
  pages={1281--1296},
  year={2009},
  publisher={JSTOR}
}

@article{Wang2007,
  title={Group SCAD regression analysis for microarray time course gene expression data},
  author={Wang, Lifeng and Chen, Guang and Li, Hongzhe},
  journal={Bioinformatics},
  volume={23},
  number={12},
  pages={1486--1494},
  year={2007},
  publisher={Oxford University Press}
}

@article{Fan2001,
  title={Variable selection via nonconcave penalized likelihood and its oracle properties},
  author={Fan, Jianqing and Li, Runze},
  journal={Journal of the American statistical Association},
  volume={96},
  number={456},
  pages={1348--1360},
  year={2001},
  publisher={Taylor \& Francis}
}

@article{Bai2020,
  title={Spike-and-slab group lassos for grouped regression and sparse generalized additive models},
  author={Bai, Ray and Moran, Gemma E and Antonelli, Joseph L and Chen, Yong and Boland, Mary R},
  journal={Journal of the American Statistical Association},
  pages={1--14},
  year={2020},
  publisher={Taylor \& Francis}
}

@article{Bai2021,
      title={Spike-and-Slab Group Lasso for Consistent Estimation and Variable Selection in Non-Gaussian Generalized Additive Models}, 
      author={Ray Bai},
      journal={arXiv:2007.07021v5.},
      year={Preprint posted online June 5, 2021. https://arxiv.org/abs/2007.07021}

}

@article{Xu2015,
  title={Bayesian variable selection and estimation for group lasso},
  author={Xu, Xiaofan and Ghosh, Malay and others},
  journal={Bayesian Analysis},
  volume={10},
  number={4},
  pages={909--936},
  year={2015},
  publisher={International Society for Bayesian Analysis}
}

@article{Yang2020,
  title={Consistent group selection with Bayesian high dimensional modeling},
  author={Yang, Xinming and Narisetty, Naveen N and others},
  journal={Bayesian Analysis},
  volume={15},
  number={3},
  pages={909--935},
  year={2020},
  publisher={International Society for Bayesian Analysis}
}

@article{Scheipl2013,
  title={Penalized likelihood and Bayesian function selection in regression models},
  author={Scheipl, Fabian and Kneib, Thomas and Fahrmeir, Ludwig},
  journal={AStA Advances in Statistical Analysis},
  volume={97},
  number={4},
  pages={349--385},
  year={2013},
  publisher={Springer}
}

@article{Scheipl2012,
abstract = {Structured additive regression (STAR) provides a general framework for complex Gaussian and non-Gaussian regression models, with predictors comprising arbitrary combinations of nonlinear functions and surfaces, spatial effects, varying coefficients, random effects, and further regression terms. The large flexibility of STAR makes function selection a challenging and important task, aiming at (1) selecting the relevant covariates, (2) choosing an appropriate and parsimonious representation of the impact of covariates on the predictor, and (3) determining the required interactions. We propose a spike-and-slab prior structure for function selection that allows to include or exclude single coefficients as well as blocks of coefficients representing specific model terms. A novel multiplicative parameter expansion is required to obtain good mixing and convergence properties in a Markov chain Monte Carlo simulation approach and is shown to induce desirable shrinkage properties. In simulation studies and with (real) benchmark classification data, we investigate sensitivity to hyperparameter settings and compare performance to competitors. The flexibility and applicability of our approach are demonstrated in an additive piecewise exponential model with time-varying effects for right-censored survival times of intensive care patients with sepsis. Geoadditive and additive mixed logit model applications are discussed in an extensive online supplement. {\textcopyright} 2012 American Statistical Association.},
annote = {semiparametric regression model can be solved when treating as mixed models. Scheipl, Fahrmeir, Kneib (2012) proposed an new spike-and-slab structure prior, normal-mixture -of-inverse gamma (NMIG) for semiparmtric regression using MCMC. The NMIG prior is mixtrue of t-distribution with an additional coerfficients mixing vector, where the vector follows a mixtureof two Gaussian distribution.},
archivePrefix = {arXiv},
arxivId = {1105.5250},
author = {Scheipl, Fabian and Fahrmeir, Ludwig and Kneib, Thomas},
doi = {10.1080/01621459.2012.737742},
eprint = {1105.5250},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Scheipl, Fahrmeir, Kneib - 2012 - Spike-and-slab priors for function selection in structured additive regression models.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Generalized additive mixed models,Parameter expansion,Penalized splines,Spatial regression,Stochastic search variable selection},
number = {500},
pages = {1518--1532},
title = {{Spike-and-slab priors for function selection in structured additive regression models}},
volume = {107},
year = {2012}
}

@article{Fan2009,
  title={Ultrahigh dimensional feature selection: beyond the linear model},
  author={Fan, Jianqing and Samworth, Richard and Wu, Yichao},
  journal={The Journal of Machine Learning Research},
  volume={10},
  pages={2013--2038},
  year={2009},
  publisher={JMLR. org}
}


@article{Meier2009,
abstract = {We propose a new sparsity-smoothness penalty for high-dimensional generalized additive models. The combination of sparsity and smoothness is crucial for mathematical theory as well as performance for finite-sample data. We present a computationally efficient algorithm, with provable numerical convergence properties, for optimizing the penalized likelihood. Furthermore, we provide oracle results which yield asymptotic optimality of our estimator for high dimensional but sparse additive models. Finally, an adaptive version of our sparsity-smoothness penalized approach yields large additional performance gains. {\textcopyright} Institute of Mathematical Statistics, 2009.},
annote = {Meier, Van De Geer and BNuhlmann (2009) proposed an a spartsity-smoothness penalty,
$$
J(f_j) = \lambda_1 \sqrt{||f_j||^2_n + \lambda \int(f^"_j(x))^2dx},},
archivePrefix = {arXiv},
arxivId = {0806.4115},
author = {Meier, Lukas and {Van De Geer}, Sara and B{\"{u}}hlmann, Peter},
doi = {10.1214/09-AOS692},
eprint = {0806.4115},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Meier, Van De Geer, B{\"{u}}hlmann - 2009 - High-dimensional additive modeling.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Group lasso,Model selection,Nonparametric regression,Oracle inequality,Penalized likelihood,Sparsity},
number = {6 B},
pages = {3779--3821},
title = {{High-dimensional additive modeling}},
volume = {37},
year = {2009}
}

@article{Marra2011,
  title={Practical variable selection for generalized additive models},
  author={Marra, Giampiero and Wood, Simon N},
  journal={Computational Statistics \& Data Analysis},
  volume={55},
  number={7},
  pages={2372--2387},
  year={2011},
  publisher={Elsevier}
}

@article{Bai2021Review,
      title={Spike-and-Slab Meets LASSO: A Review of the Spike-and-Slab LASSO}, 
      author={Ray Bai and Veronika Rockova and Edward I. George},
      journal={arXiv:2010.06451.},
      year={Preprint posted online July 1, 2021. https://arxiv.org/abs/2010.06451}
}


@article{George1993,
author = {George, Edward I. and McCulloch, Robert E.},
doi = {10.1080/01621459.1993.10476353},
file = {:C$\backslash$:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/George, McCulloch - 1993 - Variable Selection via Gibbs Sampling.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = {sep},
number = {423},
pages = {881--889},
title = {{Variable Selection via Gibbs Sampling}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476353},
volume = {88},
year = {1993}
}

@article{George1997,
abstract = {This paper describes and compares various hierarchical mixture prior formulations of variable selection uncertainty in normal linear regression models. These include the nonconjugate SSVS formulation of George andMcCulloch (1993), as well as conjugate formulations which allow for analytical simplification. Hyperpa- rameter settings which base selection on practical significance, and the implications of using mixtures with point priors are discussed. Computational methods for pos- terior evaluation and exploration are considered. Rapid updating methods are seen to provide feasible methods for exhaustive evaluation using Gray Code sequencing in moderately sized problems, and fast Markov Chain Monte Carlo exploration in large problems. Estimation of normalization constants is seen to provide improved posterior estimates of individual model probabilities and the total visited probabil- ity. Various procedures are illustrated on simulated sample problems and on a real problem concerning the construction of financial index tracking portfolios.},
author = {George, Edward I. and McCulloch, Robert E.},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/George, McCulloch - 1997 - Approaches for Bayesian variable selection.pdf:pdf},
journal = {Statistica Sinica},
keywords = {Gibbs sampling,Gray code,conjugate prior,hierar},
number = {2},
pages = {339--373},
title = {{Approaches for Bayesian variable selection.}},
volume = {7},
year = {1997}
}

@article{Brown1998,
abstract = {The multivariate regression model is considered with p regressors. A latent vector with p binary entries serves to identify one of two types of regression coefficients: those close to 0 and those not. Specializing our general distributional setting to the linear model with Gaussian errors and using natural conjugate prior distributions, we derive the marginal posterior distribution of the binary latent vector. Fast algorithms aid its direct computation, and in high dimensions these are supplemented by a Markov chain Monte Carlo approach to sampling from the known posterior distribution. Problems with hundreds of regressor variables become quite feasible. We give a simple method of assigning the hyperparameters of the prior distribution. The posterior predictive distribution is derived and the approach illustrated on compositional analysis of data involving three sugars with 160 near infra-red absorbances as regressors. {\textcopyright} 1998 Royal Statistical Society.},
author = {Brown, P. J. and Vannucci, M. and Fearn, T.},
doi = {10.1111/1467-9868.00144},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brown, Vannucci, Fearn - 1998 - Multivariate Bayesian variable selection and prediction.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Bayesian selection,Conjugate distributions,Latent variables,Markov chain Monte Carlo method,Model averaging,Multivariate regression,Prediction},
number = {3},
pages = {627--641},
title = {{Multivariate Bayesian variable selection and prediction}},
volume = {60},
year = {1998}
}

@article{Chipman1996,
abstract = {In data sets with many predictors, algorithms for identifying a good subset of predictors are often used. Most such algorithms do not allow for any relationships between predictors. For example, stepwise regression might select a model containing an interaction AB but neither main effect A or B. This paper develops mathematical representations of this and other relations between predictors, which may then be incorporated in a model selection procedure. A Bayesian approach that goes beyond the standard independence prior for variable selection is adopted, and preference for certain models is interpreted as prior information. Priors relevant to arbitrary interactions and polynomials, dummy variables for categorical factors, competing predictors, and restrictions on the size of the models are developed. Since the relations developed are for priors, they may be incorporated in any Bayesian variable selection algorithm for any type of linear model. The application of the methods here is illustrated via the stochastic search variable selection algorithm of George and McCulloch (1993), which is modified to utilize the new priors. The performance of the approach is illustrated with two constructed examples and a computer performance dataset.},
author = {Chipman, Hugh},
doi = {10.2307/3315687},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chipman - 1996 - Bayesian variable selection with related predictors.pdf:pdf},
issn = {03195724},
journal = {Canadian Journal of Statistics},
keywords = {ams 1991 subject classification,and phrases,dummy variable,gibbs sampler,interaction,primary 62105,regression},
month = {mar},
number = {1},
pages = {17--36},
title = {{Bayesian variable selection with related predictors}},
url = {http://doi.wiley.com/10.2307/3315687},
volume = {24},
year = {1996}
}

@article{Ishwaran2005,
abstract = {Variable selection in the linear regression model takes many apparent faces from both frequentist and Bayesian standpoints. In this paper we introduce a variable selection method referred to as a rescaled spike and slab model. We study the importance of prior hierarchical specifications and draw connections to frequentist generalized ridge regression estimation. Specifically, we study the usefulness of continuous bimodal priors to model hypervariance parameters, and the effect scaling has on the posterior mean through its relationship to penalization. Several model selection strategies, some frequentist and some Bayesian in nature, are developed and studied theoretically. We demonstrate the importance of selective shrinkage for effective variable selection in terms of risk misclassification, and show this is achieved using the posterior from a rescaled spike and slab model. We also show how to verify a procedure's ability to reduce model uncertainty in finite samples using a specialized forward selection strategy. Using this tool, we illustrate the effectiveness of rescaled spike and slab models in reducing model uncertainty.},
archivePrefix = {arXiv},
arxivId = {arXiv:math/0505633v1},
author = {Ishwaran, Hemant and Rao, J. Sunil},
doi = {10.1214/009053604000001147},
eprint = {0505633v1},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ishwaran, Rao - 2005 - Spike and slab variable selection Frequentist and Bayesian strategies.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {Generalized ridge regression,Hypervariance,Model averaging,Model uncertainty,Ordinary least squares,Penalization,Rescaling,Shrinkage,Stochastic variable selection,Zcut},
month = {apr},
number = {2},
pages = {730--773},
primaryClass = {arXiv:math},
title = {{Spike and slab variable selection: Frequentist and Bayesian strategies}},
url = {http://projecteuclid.org/euclid.aos/1117114335},
volume = {33},
year = {2005}
}

@article{Clyde2004,
abstract = {The evolution of Bayesian approaches for model uncertainty over the past decade has been remarkable. Catalyzed by advances in methods and technology for posterior computation, the scope of these methods has widened substantially. Major thrusts of these developments have included new methods for semiautomatic prior specification and posterior exploration. To illustrate key aspects of this evolution, the highlights of some of these developments are described.},
author = {Clyde, Merlise and George, Edward I.},
doi = {10.1214/088342304000000035},
file = {:C\:/Users/boyiguo1/Desktop/4144374.pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
keywords = {Bayes factors,Classification arid regression trees,Linear and nonparametric regression,Model averaging,Objective prior distributions,Reversible jump Markov chain Monte Carlo,Variable selection},
number = {1},
pages = {81--94},
title = {{Model uncertainty}},
volume = {19},
year = {2004}
}

@article{Rockova2014a,
abstract = {Despite rapid developments in stochastic search algorithms, the practicality of Bayesian variable selection methods has continued to pose challenges. High-dimensional data are now routinely analyzed, typically with many more covariates than observations. To broaden the applicability of Bayesian variable selection for such high-dimensional linear regression contexts, we propose {EMVS}, a deterministic alternative to stochastic search based on an {EM} algorithm which exploits a conjugate mixture prior formulation to quickly find posterior modes. Combining a spike-and-slab regularization diagram for the discovery of active predictor sets with subsequent rigorous evaluation of posterior model probabilities, {EMVS} rapidly identifies promising sparse high posterior probability submodels. External structural information such as likely covariate groupings or network topologies is easily incorporated into the {EMVS} framework. Deterministic annealing variants are seen to improve the effectiveness of our algorithms by mitigating the posterior multi-modality associated with variable selection priors. The usefulness the {EMVS} approach is demonstrated on real high-dimensional data, where computational complexity renders stochastic search to be less practical.},
annote = {Rockova and George (2014) proposed an EM-based idea for the spike-and-slab Gaussian mixture prior, EMVS. The coefficent $\beta_j$ independently follows a mixture normal distribution $N(0, \sigma^2((1-\gamma_i)v_0 + \gamma_i v_1)$ for $0 \leq v_0 < v_1$. The parameter $\sigma^2$ follows a inverse gamma prior, $IG(v/2, v\lambda /2)$. Like in the spike-and slab family, the latent binary variables $\bm \gamma$ follows a bernoulli prior with a hyper prior $\theta$ follows a beta distribution, or simplifier uniform (0,1). In the E step, the latent variable are treated as the missing data, and calculate the prosterior mean of $\gamma$; in the M step, a ridge estimator was used to update the coefficients, and $\sigma^2, \theta$ are updated accordingly. As the ridge regression creates small but nonzero coeffcients, a further step is needed to select the varibale: EMVS further suggests to threshold the inclusion probabilty for variable selection or generate regularization plot with different values of the spike scale parameter. EMVS is also compatible with structured prior when dealling with grouped variables. The group structre are imposed on the prior of the latent binary variable $\gamma$ via either logistic regression or markov random field.},
author = {Ro{\v{c}}kov{\'{a}}, Veronika and George, Edward I.},
doi = {10.1080/01621459.2013.869223},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ro{\v{c}}kov{\'{a}}, George - 2014 - EMVS The EM approach to Bayesian variable selection(2).pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Dynamic posterior exploration,High dimensionality,Regularization plots,SSVS,Sparsity},
number = {506},
pages = {828--846},
title = {{EMVS: The EM approach to Bayesian variable selection}},
volume = {109},
year = {2014}
}

@article{Rockova2018,
abstract = {Grounded theory analysis was applied to qualitative interviews with 25 communication professionals concerning cultural influences on crisis. This approach yielded several findings. First, public relations practitioners had difficulties in defining multiculturalism, often equating cultural diversity with communicating with Latinos. Second, interviewees saw cultural differences as just one aspect of diversity, emphasizing that age, religion, and education differences also affect corporate discourse. Third, although professionals considered culture a key element of crisis management, they did not feel prepared to handle the challenges of a multicultural crisis, nor did they report that they used culturally adjusted crisis strategies often. By integrating cultural competence and crisis management frameworks, this study provides the foundation for an in-depth understanding of crises, where scholars and professionals can pair crisis strategies with audiences' cultural expectations. Training initiatives focused on increasing levels of cultural competence can make organizations and communication professionals ready to the challenges of a global market. [ABSTRACT FROM PUBLISHER]},
author = {Ro{\v{c}}kov{\'{a}}, Veronika and George, Edward I.},
doi = {10.1080/01621459.2016.1260469},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ro{\v{c}}kov{\'{a}}, George - 2018 - The Spike-and-Slab LASSO.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {High-dimensional regression,LASSO,Penalized likelihood,Posterior concentration,Spike-and-Slab,Variable selection},
number = {521},
pages = {431--444},
publisher = {Taylor & Francis},
title = {{The Spike-and-Slab LASSO}},
url = {https://doi.org/10.1080/01621459.2016.1260469},
volume = {113},
year = {2018}
}

@article{Rockova2018b,
author = {Ro{\v{c}}kov{\'{a}}, Veronika},
doi = {10.1214/17-AOS1554},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ro{\v{c}}kov{\'{a}} - 2018 - Bayesian estimation of sparse signals with a continuous spike-and-slab prior.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {feb},
number = {1},
pages = {401--437},
title = {{Bayesian estimation of sparse signals with a continuous spike-and-slab prior}},
url = {https://projecteuclid.org/euclid.aos/1519268435},
volume = {46},
year = {2018}
}

@article{Yi2012,
abstract = {Genetic and other scientific studies routinely generate very many predictor variables, which can be naturally grouped, with predictors in the same groups being highly correlated. It is desirable to incorporate the hierarchical structure of the predictor variables into generalized linear models for simultaneous variable selection and coefficient estimation. We propose two prior distributions: hierarchical Cauchy and double-exponential distributions, on coefficients in generalized linear models. The hierarchical priors include both variable-specific and group-specific tuning parameters, thereby not only adopting different shrinkage for different coefficients and different groups but also providing a way to pool the information within groups. We fit generalized linear models with the proposed hierarchical priors by incorporating flexible expectation-maximization (EM) algorithms into the standard iteratively weighted least squares as implemented in the general statistical package R. The methods are illustrated with data from an experiment to identify genetic polymorphisms for survival of mice following infection with Listeria monocytogenes. The performance of the proposed procedures is further assessed via simulation studies. The methods are implemented in a freely available R package BhGLM (http://www.ssg.uab.edu/bhglm/). {\textcopyright} 2012 De Gruyter. All rights reserved.},
annote = {To incorporate the grouping structure, another level of prior was imposed on $s^2$ and $s$ respectively. For the hierarchical t prior, $s^2$ follows another gamma distribution Gamma($a, b_{k[j]}$, where $a$ is an arbitrary constant (for example, 0.5), and $b_{k[j]}$ is a unknown random variable with density $p(\log(b_k))\propto 1$. The subscription $k[j]$ denotes the variable $j$ belongs to the $k$th group. For the hierarchical double exponential prior, $s$ follows the same gamma distribution Gamma($a, b_{k[j]}$. The hypor-prior imposing on $s$ instead of $s^2$ is to facilitate computation. The hierarchical prior of $\beta_j$ can be simplified as $t_v(0,s_j^2)$ and $DE(0, s_j)$. Hence, we have $s_j^2$ and $s_j$, respectively, controls the shrinkage of individual variables while the hyper-parameter $b_{k[j]}$ balances the shrinkage of individual variables within a group.

While the hierarchical prior seems complicated, it provides a flexible and parsimonious solutaion. While there is rarely closed form solutaion to hierarchy model, most of the solution is derived from Monte Carlo Markov Chain which is infamously computation intensive, especially when the number of variables are large. The EM-IWLS provides an parsimonious alternative. Following the EM algorithm, $\tau^2$, $s_j$ and $b_{k[j]}$ are treatming as missing data. In the E step, the posterior mean of the "missing" parameters are calculated; in the M step, IWLS algorithm was used to update the coefficients conditioning on the posterior means.},
author = {Yi, Nengjun and Ma, Shuangge},
doi = {10.1515/1544-6115.1803},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yi, Ma - 2012 - Hierarchical Shrinkage Priors and Model Fitting for High-dimensional Generalized Linear Models.pdf:pdf},
issn = {1544-6115},
journal = {Statistical Applications in Genetics and Molecular Biology},
keywords = {Adaptive lasso,Bayesian inference,Generalized linear model,Genetic polymorphisms,Grouped variables,Hierarchical model,High-dimensional data,Shrinkage prior},
month = {jan},
number = {6},
pmid = {23192052},
title = {{Hierarchical Shrinkage Priors and Model Fitting for High-dimensional Generalized Linear Models}},
url = {https://www.degruyter.com/view/j/sagmb.2012.11.issue-6/1544-6115.1803/1544-6115.1803.xml},
volume = {11},
year = {2012}
}


@article{Tang2017,
abstract = {{\textcopyright} 2017 The Author. Motivation: Large-scale molecular profiling data have offered extraordinary opportunities to improve survival prediction of cancers and other diseases and to detect disease associated genes. However, there are considerable challenges in analyzing large-scale molecular data. Results: We propose new Bayesian hierarchical Cox proportional hazards models, called the spikeand- slab lasso Cox, for predicting survival outcomes and detecting associated genes. We also develop an efficient algorithm to fit the proposed models by incorporating Expectation-Maximization steps into the extremely fast cyclic coordinate descent algorithm. The performance of the proposed method is assessed via extensive simulations and compared with the lasso Cox regression. We demonstrate the proposed procedure on two cancer datasets with censored survival outcomes and thousands of molecular features. Our analyses suggest that the proposed procedure can generate powerful prognostic models for predicting cancer survival and can detect associated genes. Availability and implementation: The methods have been implemented in a freely available R package BhGLM (http://www.ssg.uab.edu/bhglm/).},
author = {Tang, Zaixiang and Shen, Yueping and Zhang, Xinyan and Yi, Nengjun},
doi = {10.1093/bioinformatics/btx300},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang et al. - 2017 - The spike-and-slab lasso Cox model for survival prediction and associated genes detection.pdf:pdf},
issn = {14602059},
journal = {Bioinformatics},
number = {18},
pages = {2799--2807},
title = {{The spike-and-slab lasso Cox model for survival prediction and associated genes detection}},
volume = {33},
year = {2017}
}
@article{Tang2017a,
abstract = {{\textcopyright} 2017 by the Genetics Society of America. Large-scale “omics” data have been increasingly used as an important resource for prognostic prediction of diseases and detection of associated genes. However, there are considerable challenges in analyzing high-dimensional molecular data, including the large number of potential molecular predictors, limited number of samples, and small effect of each predictor. We propose new Bayesian hierarchical generalized linear models, called spike-and-slab lasso GLMs, for prognostic prediction and detection of associated genes using large-scale molecular data. The proposed model employs a spike-and-slab mixture double-exponential prior for coefficients that can induce weak shrinkage on large coefficients, and strong shrinkage on irrelevant coefficients. We have developed a fast and stable algorithm to fit large-scale hierarchal GLMs by incorporating expectation-maximization (EM) steps into the fast cyclic coordinate descent algorithm. The proposed approach integrates nice features of two popular methods, i.e., penalized lasso and Bayesian spike-and-slab variable selection. The performance of the proposed method is assessed via extensive simulation studies. The results show that the proposed approach can provide not only more accurate estimates of the parameters, but also better prediction. We demonstrate the proposed procedure on two cancer data sets: a well-known breast cancer data set consisting of 295 tumors, and expression data of 4919 genes; and the ovarian cancer data set from TCGA with 362 tumors, and expression data of 5336 genes. Our analyses show that the proposed procedure can generate powerful models for predicting outcomes and detecting associated genes. The methods have been implemented in a freely available R package BhGLM (http://www.ssg.uab.edu/bhglm/).},
author = {Tang, Zaixiang and Shen, Yueping and Zhang, Xinyan and Yi, Nengjun},
doi = {10.1534/genetics.116.192195},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang et al. - 2017 - The spike-and-slab lasso generalized linear models for prediction and associated genes detection.pdf:pdf},
issn = {19432631},
journal = {Genetics},
keywords = {Cancer,Double-exponential distribution,Generalized linear model,Outcome prediction,Spike-and-slab lasso},
number = {1},
pages = {77--88},
title = {{The spike-and-slab lasso generalized linear models for prediction and associated genes detection}},
volume = {205},
year = {2017}
}
@article{Tang2019,
abstract = {Group structures among genes encoded in functional relationships or biological pathways are valuable and unique features in large-scale molecular data for survival analysis. However, most of previous approaches for molecular data analysis ignore such group structures. It is desirable to develop powerful analytic methods for incorporating valuable pathway information for predicting disease survival outcomes and detecting associated genes. We here propose a Bayesian hierarchical Cox survival model, called the group spike-and-slab lasso Cox (gsslasso Cox), for predicting disease survival outcomes and detecting associated genes by incorporating group structures of biological pathways. Our hierarchical model employs a novel prior on the coefficients of genes, i.e., the group spike-and-slab double-exponential distribution, to integrate group structures and to adaptively shrink the effects of genes. We have developed a fast and stable deterministic algorithm to fit the proposed models. We performed extensive simulation studies to assess the model fitting properties and the prognostic performance of the proposed method, and also applied our method to analyze three cancer data sets. Both the theoretical and empirical studies show that the proposed method can induce weaker shrinkage on predictors in an active pathway, thereby incorporating the biological similarity of genes within a same pathway into the hierarchical modeling. Compared with several existing methods, the proposed method can more accurately estimate gene effects and can better predict survival outcomes. For the three cancer data sets, the results show that the proposed method generates more powerful models for survival prediction and detecting associated genes. The method has been implemented in a freely available R package BhGLM at 
                    https://github.com/nyiuab/BhGLM
                    
                  .},
author = {Tang, Zaixiang and Lei, Shufeng and Zhang, Xinyan and Yi, Zixuan and Guo, Boyi and Chen, Jake Y. and Shen, Yueping and Yi, Nengjun},
doi = {10.1186/s12859-019-2656-1},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang et al. - 2019 - Gsslasso Cox A Bayesian hierarchical model for predicting survival and detecting associated genes by incorporating.pdf:pdf},
issn = {14712105},
journal = {BMC Bioinformatics},
keywords = {Cox survival models,Grouped predictors,Hierarchical modeling,Lasso,Pathway,Spike-and-slab prior},
number = {1},
pages = {1--15},
publisher = {BMC Bioinformatics},
title = {{Gsslasso Cox: A Bayesian hierarchical model for predicting survival and detecting associated genes by incorporating pathway information}},
volume = {20},
year = {2019}
}
@article{Tang2018,
abstract = {MotivationLarge-scale molecular data have been increasingly used as an important resource for prognostic prediction of diseases and detection of associated genes. However, standard approaches for omics data analysis ignore the group structure among genes encoded in functional relationships or pathway information.ResultsWe propose new Bayesian hierarchical generalized linear models, called group spike-and-slab lasso GLMs, for predicting disease outcomes and detecting associated genes by incorporating large-scale molecular data and group structures. The proposed model employs a mixture double-exponential prior for coefficients that induces self-adaptive shrinkage amount on different coefficients. The group information is incorporated into the model by setting group-specific parameters. We have developed a fast and stable deterministic algorithm to fit the proposed hierarchal GLMs, which can perform variable selection within groups. We assess the performance of the proposed method on several simulated scenarios, by varying the overlap among groups, group size, number of non-null groups, and the correlation within group. Compared with existing methods, the proposed method provides not only more accurate estimates of the parameters but also better prediction. We further demonstrate the application of the proposed procedure on three cancer data sets by utilizing pathway structures of genes. Our results show that the proposed method generates powerful models for predicting disease outcomes and detecting associated genes.AvailabilityThe methods have been implemented in a freely available R package BhGLM (http://www.ssg.uab.edu/bhglm/).Contactnyi@uab.eduSupplementary informationSupplementary data are available at Bioinformatics online.},
author = {Tang, Zaixiang and Shen, Yueping and Li, Yan and Zhang, Xinyan and Wen, Jia and Qian, Chen'Ao and Zhuang, Wenzhuo and Shi, Xinghua and Yi, Nengjun},
doi = {10.1093/bioinformatics/btx684},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang et al. - 2018 - Group spike-And-slab lasso generalized linear models for disease prediction and associated genes detection by incor.pdf:pdf},
issn = {14602059},
journal = {Bioinformatics},
number = {6},
pages = {901--910},
title = {{Group spike-And-slab lasso generalized linear models for disease prediction and associated genes detection by incorporating pathway information}},
volume = {34},
year = {2018}
}

@Article{Wood2011,
  title = {Fast stable restricted maximum likelihood and marginal 
likelihood estimation of semiparametric generalized linear models},
  journal = {Journal of the Royal Statistical Society (B)},
  volume = {73},
  number = {1},
  pages = {3-36},
  year = {2011},
  author = {S. N. Wood},
}

@article{Zhang2006GAM,
  title={Component selection and smoothing for nonparametric regression in exponential families},
  author={Zhang, Hao Helen and Lin, Yi},
  journal={Statistica Sinica},
  pages={1021--1041},
  year={2006},
  publisher={JSTOR}
}

@article{Storlie2011,
  title={Surface estimation, variable selection, and the nonparametric oracle property},
  author={Storlie, Curtis B and Bondell, Howard D and Reich, Brian J and Zhang, Hao Helen},
  journal={Statistica Sinica},
  volume={21},
  number={2},
  pages={679},
  year={2011},
  publisher={NIH Public Access}
}

@article{R,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  journal = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2021. https://www.R-project.org/}
}

@article{Whitehead1980,
  title={Fitting Cox's regression model to survival data using GLIM},
  author={Whitehead, John},
  journal={Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume={29},
  number={3},
  pages={268--275},
  year={1980},
  publisher={Wiley Online Library}
}

@article{Peterson2016,
  title={Joint Bayesian variable and graph selection for regression models with network-structured predictors},
  author={Peterson, Christine B and Stingo, Francesco C and Vannucci, Marina},
  journal={Statistics in medicine},
  volume={35},
  number={7},
  pages={1017--1031},
  year={2016},
  publisher={Wiley Online Library}
}

@article{Ferrari2020,
  title={Identifying main effects and interactions among exposures using Gaussian processes},
  author={Ferrari, Federico and Dunson, David B},
  journal={The Annals of Applied Statistics},
  volume={14},
  number={4},
  pages={1743--1758},
  year={2020},
  publisher={Institute of Mathematical Statistics}
}