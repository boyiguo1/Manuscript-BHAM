@article{Wood2020,
  title={Inference and computation with generalized additive models and their extensions},
  author={Wood, Simon N},
  journal={Test},
  volume={29},
  number={2},
  pages={307--339},
  year={2020},
  publisher={Springer}
}


@article{Cleveland1979,
abstract = {The visual information on a scatterplot can be greatly enhanced, with little additional cost, by computing and plotting smoothed points. Robust locally weighted regression is a method for smoothing a scatterplot, (xi, yi), i = 1, {\ldots}, n, in which the fitted value at zkis the value of a polynomial fit to the data using weighted least squares, where the weight for (xi, yi) is large if xiis close to xkand small if it is not. A robust fitting procedure is used that guards against deviant points distorting the smoothed points. Visual, computational, and statistical issues of robust locally weighted regression are discussed. Several examples, including data on lead intoxication, are used to illustrate the methodology. {\textcopyright} 1979, Taylor {\&} Francis Group, LLC.},
author = {Cleveland, William S.},
doi = {10.1080/01621459.1979.10481038},
file = {:C$\backslash$:/Users/boyiguo1/Downloads/2286407.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Graphics,Nonparametric regression,Robust estimation,Scatterplots,Smoothing},
number = {368},
pages = {829--836},
title = {{Robust locally weighted regression and smoothing scatterplots}},
volume = {74},
year = {1979}
}

@article{Hastie1987,
abstract = {Generalized additive models have the form $\eta$(x) = $\alpha$ + $\sigma$ fj(xj), where $\eta$ might be the regression function in a multiple regression or the logistic transformation of the posterior probability Pr(y = 1 x) in a logistic regression. In fact, these models generalize the whole family of generalized linear models $\eta$(x) = $\beta$â€²x, where $\eta$(x) = g($\mu$(x)) is some transformation of the regression function. We use the local scoring algorithm to estimate the functions fj(xj) nonparametrically, using a scatterplot smoother as a building block. We demonstrate the models in two different analyses: a nonparametric analysis of covariance and a logistic regression. The procedure can be used as a diagnostic tool for identifying parametric transformations of the covariates in a standard linear analysis. A variety of inferential tools have been developed to aid the analyst in assessing the relevance and significance of the estimated functions: these include confidence curves, degrees of freedom estimates, and approximate hypothesis tests. The local scoring algorithm is analogous to the iterative reweighted least squares algorithm for solving likelihood and nonlinear regression equations. At each iteration, an adjusted dependent variable is formed and an additive regression model is fit using the backfitting algorithm. The backfitting algorithm cycles through the variables and estimates each coordinated function by smoothing the partial residuals. {\textcopyright} 1976 Taylor {\&} Francis Group, LLC.},
author = {Hastie, Trevor and Tibshirani, Robert},
doi = {10.1080/01621459.1987.10478440},
file = {:C$\backslash$:/Users/boyiguo1/Downloads/2289439.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Generalized linear model,Logistic regression,Nonparametric regression,Smooth},
number = {398},
pages = {371--386},
title = {{Generalized additive models: Some applications}},
volume = {82},
year = {1987}
}


@article{Breiman1985,
  title={Estimating optimal transformations for multiple regression and correlation},
  author={Breiman, Leo and Friedman, Jerome H},
  journal={Journal of the American statistical Association},
  volume={80},
  number={391},
  pages={580--598},
  year={1985},
  publisher={Taylor \& Francis Group}
}

@article{Mallick2013,
author = {Mallick, Himel and Yi, Nengjun},
doi = {10.4172/2155-6180.S1-005},
file = {:C$\backslash$:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Manuscript - 2014 - NIH Public Access.pdf:pdf},
issn = {21556180},
journal = {Journal of Biometrics {\&} Biostatistics},
keywords = {bayesian,bayesian hierarchical models,bayesian model selection,bayesian subset regression,consistency,high dimensional linear models,mcmc,nonlocal priors,penalized regression,posterior,regularization,shrinkage methods,variable selection},
number = {205},
pages = {1--27},
title = {{Bayesian Methods for High Dimensional Linear Models}},
url = {https://www.omicsonline.org/bayesian-methods-for-high-dimensional-linear-models-2155-6180.S1-005.php?aid=15156},
year = {2013}
}


@article{Ravikumar2009,
abstract = {We present a new class of methods for high dimensional non-parametric regression and classification called sparse additive models. Our methods combine ideas from sparse linear modelling and additive non-parametric regression. We derive an algorithm for fitting the models that is practical and effective even when the number of covariates is larger than the sample size. Sparse additive models are essentially a functional version of the grouped lasso of Yuan and Lin. They are also closely related to the COSSO model of Lin and Zhang but decouple smoothing and sparsity, enabling the use of arbitrary non-parametric smoothers. We give an analysis of the theoretical properties of sparse additive models and present empirical results on synthetic and real data, showing that they can be effective in fitting sparse non-parametric models in high dimensional data. {\textcopyright} 2009 Royal Statistical Society.},
annote = {Ravikumar, Lafferty, Lue and Wasserman, extended the orginal backfitting algorithm to accommadate the high-dimensional setting. The sparsity and smoothness was handled saparately with a softthreshold. The authors also deomonstrated its similarity to Group Lasso},
archivePrefix = {arXiv},
arxivId = {0711.4555},
author = {Ravikumar, Pradeep and Lafferty, John and Liu, Han and Wasserman, Larry},
doi = {10.1111/j.1467-9868.2009.00718.x},
eprint = {0711.4555},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ravikumar et al. - 2009 - Sparse additive models.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Additive models,Lasso,Non-parametric regression,Sparsity},
month = {nov},
number = {5},
pages = {1009--1030},
title = {{Sparse additive models}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2009.00718.x},
volume = {71},
year = {2009}
}

@article{Yuan2006,
  title={Model selection and estimation in regression with grouped variables},
  author={Yuan, Ming and Lin, Yi},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={68},
  number={1},
  pages={49--67},
  year={2006},
  publisher={Wiley Online Library}
}


@article{Huang2010,
  title={Variable selection in nonparametric additive models},
  author={Huang, Jian and Horowitz, Joel L and Wei, Fengrong},
  journal={Annals of statistics},
  volume={38},
  number={4},
  pages={2282},
  year={2010},
  publisher={NIH Public Access}
}

@article{Xue2009,
  title={Consistent variable selection in additive models},
  author={Xue, Lan},
  journal={Statistica Sinica},
  pages={1281--1296},
  year={2009},
  publisher={JSTOR}
}

@article{Wang2007,
  title={Group SCAD regression analysis for microarray time course gene expression data},
  author={Wang, Lifeng and Chen, Guang and Li, Hongzhe},
  journal={Bioinformatics},
  volume={23},
  number={12},
  pages={1486--1494},
  year={2007},
  publisher={Oxford University Press}
}

@article{Fan2001,
  title={Variable selection via nonconcave penalized likelihood and its oracle properties},
  author={Fan, Jianqing and Li, Runze},
  journal={Journal of the American statistical Association},
  volume={96},
  number={456},
  pages={1348--1360},
  year={2001},
  publisher={Taylor \& Francis}
}

@article{Bai2020,
  title={Spike-and-slab group lassos for grouped regression and sparse generalized additive models},
  author={Bai, Ray and Moran, Gemma E and Antonelli, Joseph L and Chen, Yong and Boland, Mary R},
  journal={Journal of the American Statistical Association},
  pages={1--14},
  year={2020},
  publisher={Taylor \& Francis}
}

@article{Bai2021,
      title={Spike-and-Slab Group Lasso for Consistent Estimation and Variable Selection in Non-Gaussian Generalized Additive Models}, 
      author={Ray Bai},
      journal={arXiv:2007.07021v5.},
      year={Preprint posted online June 5, 2021. https://arxiv.org/abs/2007.07021}

}

@article{Xu2015,
  title={Bayesian variable selection and estimation for group lasso},
  author={Xu, Xiaofan and Ghosh, Malay and others},
  journal={Bayesian Analysis},
  volume={10},
  number={4},
  pages={909--936},
  year={2015},
  publisher={International Society for Bayesian Analysis}
}

@article{Yang2020,
  title={Consistent group selection with Bayesian high dimensional modeling},
  author={Yang, Xinming and Narisetty, Naveen N and others},
  journal={Bayesian Analysis},
  volume={15},
  number={3},
  pages={909--935},
  year={2020},
  publisher={International Society for Bayesian Analysis}
}

@article{Scheipl2013,
  title={Penalized likelihood and Bayesian function selection in regression models},
  author={Scheipl, Fabian and Kneib, Thomas and Fahrmeir, Ludwig},
  journal={AStA Advances in Statistical Analysis},
  volume={97},
  number={4},
  pages={349--385},
  year={2013},
  publisher={Springer}
}

@article{Scheipl2012,
abstract = {Structured additive regression (STAR) provides a general framework for complex Gaussian and non-Gaussian regression models, with predictors comprising arbitrary combinations of nonlinear functions and surfaces, spatial effects, varying coefficients, random effects, and further regression terms. The large flexibility of STAR makes function selection a challenging and important task, aiming at (1) selecting the relevant covariates, (2) choosing an appropriate and parsimonious representation of the impact of covariates on the predictor, and (3) determining the required interactions. We propose a spike-and-slab prior structure for function selection that allows to include or exclude single coefficients as well as blocks of coefficients representing specific model terms. A novel multiplicative parameter expansion is required to obtain good mixing and convergence properties in a Markov chain Monte Carlo simulation approach and is shown to induce desirable shrinkage properties. In simulation studies and with (real) benchmark classification data, we investigate sensitivity to hyperparameter settings and compare performance to competitors. The flexibility and applicability of our approach are demonstrated in an additive piecewise exponential model with time-varying effects for right-censored survival times of intensive care patients with sepsis. Geoadditive and additive mixed logit model applications are discussed in an extensive online supplement. {\textcopyright} 2012 American Statistical Association.},
annote = {semiparametric regression model can be solved when treating as mixed models. Scheipl, Fahrmeir, Kneib (2012) proposed an new spike-and-slab structure prior, normal-mixture -of-inverse gamma (NMIG) for semiparmtric regression using MCMC. The NMIG prior is mixtrue of t-distribution with an additional coerfficients mixing vector, where the vector follows a mixtureof two Gaussian distribution.},
archivePrefix = {arXiv},
arxivId = {1105.5250},
author = {Scheipl, Fabian and Fahrmeir, Ludwig and Kneib, Thomas},
doi = {10.1080/01621459.2012.737742},
eprint = {1105.5250},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Scheipl, Fahrmeir, Kneib - 2012 - Spike-and-slab priors for function selection in structured additive regression models.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Generalized additive mixed models,Parameter expansion,Penalized splines,Spatial regression,Stochastic search variable selection},
number = {500},
pages = {1518--1532},
title = {{Spike-and-slab priors for function selection in structured additive regression models}},
volume = {107},
year = {2012}
}

@article{Meier2009,
abstract = {We propose a new sparsity-smoothness penalty for high-dimensional generalized additive models. The combination of sparsity and smoothness is crucial for mathematical theory as well as performance for finite-sample data. We present a computationally efficient algorithm, with provable numerical convergence properties, for optimizing the penalized likelihood. Furthermore, we provide oracle results which yield asymptotic optimality of our estimator for high dimensional but sparse additive models. Finally, an adaptive version of our sparsity-smoothness penalized approach yields large additional performance gains. {\textcopyright} Institute of Mathematical Statistics, 2009.},
annote = {Meier, Van De Geer and BNuhlmann (2009) proposed an a spartsity-smoothness penalty,
$$
J(f_j) = \lambda_1 \sqrt{||f_j||^2_n + \lambda \int(f^"_j(x))^2dx},},
archivePrefix = {arXiv},
arxivId = {0806.4115},
author = {Meier, Lukas and {Van De Geer}, Sara and B{\"{u}}hlmann, Peter},
doi = {10.1214/09-AOS692},
eprint = {0806.4115},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Meier, Van De Geer, B{\"{u}}hlmann - 2009 - High-dimensional additive modeling.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Group lasso,Model selection,Nonparametric regression,Oracle inequality,Penalized likelihood,Sparsity},
number = {6 B},
pages = {3779--3821},
title = {{High-dimensional additive modeling}},
volume = {37},
year = {2009}
}

@article{Marra2011,
  title={Practical variable selection for generalized additive models},
  author={Marra, Giampiero and Wood, Simon N},
  journal={Computational Statistics \& Data Analysis},
  volume={55},
  number={7},
  pages={2372--2387},
  year={2011},
  publisher={Elsevier}
}

@article{Bai2021Review,
      title={Spike-and-Slab Meets LASSO: A Review of the Spike-and-Slab LASSO}, 
      author={Ray Bai and Veronika Rockova and Edward I. George},
      journal={arXiv:2010.06451.},
      year={Preprint posted online July 1, 2021. https://arxiv.org/abs/2010.06451}
}
