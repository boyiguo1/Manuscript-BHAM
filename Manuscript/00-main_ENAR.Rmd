---
title: Spike-and-Slab Generalized Additive Models and Scalable Algorithms for High-Dimensional Data

# author: 
# - name: Author 1
  # affiliation: Department of YYY, University of XXX
  # email: abc@def
  
# - name: Author 2
  # affiliation: Department of ZZZ, University of WWW
  # email: djf@wef

keywords: Spike-and-Slab Priors; High-Dimensional Data; Generalized Additive Models; EM-IWLS; EM-Coordinate Decent; Scalablility

abstract: |
  Many proposed high-dimensional generalized additive models (GAMs) use sparse regularization, and tend to overly smooth important nonlinear functions. The excess shrinkage on coefficients can in turn produce inaccurate predictions. Moreover, most of these GAMs consider an “all-in-all-out” approach for function selection, rendering them difficult to answer if nonlinear effects are necessary. While some Bayesian models can address these shortcomings, using Markov chain Monte Carlo algorithms for model fitting creates a new challenge, scalability. Hence, we propose Bayesian hierarchical generalized additive models as a solution: we incorporate the smoothing penalty that can separate linear and nonlinear spaces of smoothing functions. A novel spike-and-slab spline prior is used to select the components of smoothing functions. Two scalable and deterministic algorithms, EM-Coordinate Descent and EM-Iterative Weighted Least Squares, are developed for different utilities. Simulation studies and metabolomics data analyses demonstrate improved performance against state-of-the-art models, mgcv, COSSO and sparse Bayesian GAM. The software implementation is freely available via an R package BHAM.

bibliography: bibfile.bib
output: rticles::biometrics_article
# month: "`r format(Sys.Date(), '%b')`"
# year: "`r format(Sys.Date(), '%Y')`"
referee: true # Papers submitted to Biometrics should ALWAYS be prepared
              # using the referee option!!!! Turn off only to preview
              # two column-format
header-includes: 
  - \usepackage{tikz}
  - \usepackage{pgfplots}
  - \usepackage{amsfonts}
  # - \usepackage[modulo,pagewise,displaymath,mathlines]{lineno}
---

<!-- \usepackage{tikz} -->
<!-- \linenumbers -->
\pgfplotsset{compat=1.18}
\usetikzlibrary{shapes.geometric, arrows, positioning, calc, matrix, backgrounds, fit}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\tp}{*}
\newcommand{\pr}{\text{Pr}}
\newcommand{\repa}{\text{repa}}
\newcommand{\simiid}{\overset{\text{iid}}{\sim}}


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

library(knitr)
library(targets)
# library(here)
library(xtable)
library(flextable)


set_flextable_defaults(fonts_ignore=TRUE)

tar_load(intro_path)
tar_load(method_path)
tar_load(sim_path)
tar_load(real_data_path)
tar_load(disc_path)

tar_load(tab_binom)
tar_load(tab_gaus)

tar_load(WLM_train_dat)
# tar_load(tbl_one)
tar_load(ECB_bamlasso_cv)
tar_load(ECB_SBGAM_cv)
tar_load(WLM_bamlasso_cv)
tar_load(WLM_SBGAM_cv)
tar_load(tbl_real_time)
```

```{r child="01-Intro.Rmd"}
```
```{r child="02-Method.Rmd"}
```

```{r child="03-Simulation.Rmd"}
```

```{r child="04-Real_Data.Rmd"}
```

```{r child="05-Discussion.Rmd"}
```



\appendix
\section{}
\subsection{A.1 EM-Iterative Weighted Least Squares Algorithm\label{ap:IWLS}}
Similar to the EM-CD algorithm, the EM-IWLS algorithm is an iterative EM-based algorithm where the iterative weighted least squares algorithm is used to find the estimate of $\bs \beta, \phi$ that maximizes $E(Q_1)$. The iterative weighted least squares algorithm was originally proposed to fit the classical generalized linear models, and generalized to fit some Bayesian hierarchical models.[@Gelman2013] Yi and Ma [@Yi2012] formulated Student's t-distribution and double exponential distribution as hierarchical normal distributions such that generalized linear models with shrinkage priors can be easily fitted using IWLS in combination with EM algorithm. In this work, we adapt the EM-IWLS paradigm to fit BHAM with spike-and-slab spline prior, with the full potential to adopt Student's t, double exponential and mixture Student's t priors when sparse assumption is not necessary.
<!-- The primary purpose of the EM algorithm here is mitigate the computational burden associate with the hierarchical normal formulation.  -->
A double exponential prior, $\beta|S \sim DE(0, S)$ can be formulated as a hierarchical normal prior with unknown variance $\tau^2$ integrated out:
\begin{align*}
  \beta|\tau^2 &\sim N(0, \tau^2)\\
  \tau^2|S & \sim Gamma(1, 1/(2S^2)), 
\end{align*}
For the mixture double exponential priors, we can define the scale parameter $S = (1-\gamma)s_0 + \gamma s_1$ following Equation (\ref{eq:ssl}). The change in the prior formulation in turn leads to the change in the log posterior density function, as $Q_1$ needs to account for the hyperprior of $\tau^2$: 
\begin{equation}\label{eq:Q1_IWLS}
Q_1(\bs \beta, \phi) = \log f(\textbf{y}|\bs \beta, \phi) + \sum\limits_{j=1}^p\left[\log f(\beta_j|{\tau}^2_j) + \log f({\tau}^2_j| S_j)+\sum\limits_{k=1}^{K_j} \{\log f(\beta^{\tp}_{jk}|{\tau^{\tp}}^2_{jk})+\log f({\tau^\tp}^2_{jk}| S^\tp_j)\}\right].
\end{equation}
Since $\bs \tau^2$ are not of our primary interest, we treat them as the "missing" data in addition to the latent indicators $\bs \gamma$, and hence construct the expectation $E_{\bs \gamma, \bs \tau^2|\Theta^{(t-1)}}(Q_1)$ in the E-step. To note, unlike the same latent indicator $\gamma^\tp_j$ which is shared by the coefficients of the non-linear terms $\beta^\tp_{jk}$ for $k = 1, \dots, K_j$ , $\tau^2_{jk}$ is coefficient specific for $\beta^\tp_{jk}$. $E({S_j}^{-1}|\beta_j, s_0, s_1), E(S^{-1}_j|\bs \beta_j^\tp, s_0, s_1), E({\tau}^2_{j}|S_j, \beta_j) \text{ and } E({\tau^\tp}^2_{jk}|S_j^\tp, \beta^\tp_{jk})$ needs to be calculated to formulate $E(Q_1)$. As neither $E({S_j}^{-1}|\beta_j, s_0, s_1)$ nor $E(S^{-1}_j|\bs \beta_j^\tp, s_0, s_1)$ depends on $\tau^2$s, they can be derived using Equation (\ref{eq:exp_scale}) via Bayes' Theorem. On the other hand, $\tau^{2}$ following a gamma distribution is a conjugate prior for the normal variance, and the conditional posterior density of $\tau^{-2}$ is an inverse Gaussian distribution. $E({\tau}^{-2}_{j})$ and $E({\tau^\tp}^{-2}_{jk})$ are calculated using the closed form equation
\begin{align*}
 E({\tau}^{-2}_{j}|S_j, \beta_j) ={S_j}^{-1}/|\beta_j| \qquad E({\tau^\tp}^{-2}_{jk}|S_j^\tp, \beta^\tp_{jk})={S_j^\tp}^{-1}/|\beta^\tp_{jk}|,
\end{align*}
where $S_j$ and $S_j^\tp$ are replaced by the expectation and $\beta$s are replaced with $\beta^{(t-1)}$. With simplification (up to constant additive terms), we have 
\begin{equation}
E(Q_1) = \log f(\textbf{y}|\bs \beta, \phi) - \sum\limits_{j=1}^p\left[ {2E({\tau_j}^{-2})}{\beta_j}^2 +\sum\limits_{k=1}^{K_j} {2E({\tau_{jk}^\tp}^{-2})}{\beta_{jk}^\tp}^2\right].
\end{equation}
<!-- $E(Q_1)$ can be seen as a $l_2$ penalized likelihood function where the regularization parameter $\lambda = 2E({\tau}^{-2})$. Equivalently, we have  -->
$2E({\tau}^{-2})\beta^2$ can be seen as the kernel of a normal density with mean 0 and variance $E(\tau^{2})$, and we can formulate the coefficients $\bs \beta$ as a multivariate normal distribution with means $\bs 0$ and variance covariance matrix $\bs \Sigma_{\tau^2}$, where $\bs \Sigma_{\tau^2}$ is a diagonal matrix with $E(\tau^2)$s on the diagonal,
$$
\bs \beta \sim \text{MVN}(0, \bs \Sigma_{\tau^2}).
$$
Meanwhile, following the classical IWLS, we can approximate the generalized model likelihood at each iteration with a weighted normal likelihood:
$$
f(\textbf{y}|\bs \beta, \phi) \approx \text{MVN}(\textbf{z}|\bs X \bs \beta, \phi\bs \Sigma )
$$
where the ‘normal response’ $z_i$ and ‘weight’ $w_i$ are called the pseudo-response and pseudo-weight respectively. The pseudo-response and the pseudo-weight are calculated by
$$
\begin{aligned}
z_i &= \hat\eta_i - \frac{L^{'}(y_i|\hat\eta_i)}{L^{''}(y_i|\hat\eta_i)}& w_i &= - L^{''}(y_i|\hat\eta_i),
\end{aligned}
$$
where $\hat\eta_i = (\bs X {\hat{\bs\beta}})_i$, $L^{'}(y_i|\hat\eta_i, \hat \phi)$ and $L^{''}(y_i|\hat\eta_i, \hat \phi)$ are the first and second derivative of the log density, $\log f(\textbf{y}_i|\bs \beta, \phi)$ with respect to $\eta_i$. 
With $\bs z\sim \text{MVN}(\bs X \bs \beta, \phi \bs \Sigma)$ and $\bs \beta \sim \text{MVN}(0, \phi \bs \Sigma_{\tau^2})$, we can augment the two multivariate normal distributions and update the estimates for $\bs \beta$ and $\phi$ via least squares in each iteration of the EM algorithm. We create the augmented response, augmented data, and augmented variance-covariance matrix following \begin{align*}
& \bs z_* = \begin{bmatrix} \bs z\\ \bs 0\end{bmatrix} &&
  \bs X_* = \begin{bmatrix} \bs X \\ \bs I \end{bmatrix} &&
  \bs \Sigma_* = \begin{bmatrix} \bs \Sigma & \bs 0  \\ \bs 0 & \bs \Sigma_{\tau^2}/\phi \end{bmatrix}, &
\end{align*}
 such that 
$$
\bs z_* \sim \text{MVN}(\bs X_* \bs \beta , \phi \Sigma_*).
$$
Using the least squares estimators to update $\bs\beta$ and $\phi$, we have 
\begin{align*}
& \bs \beta^{(t)} = (\bs X_*^T \bs \Sigma^{-1} \bs X_*)^{-1}\bs X_*^T \bs \Sigma^{-1} \bs z_* && \phi^{(t)} = \frac{1}{n}(\bs z_*-X_*\bs \beta^{(t)})^T\bs \Sigma^{-1}(\bs z_*-X_*\bs \beta^{(t)}).&
\end{align*}
To note, the variance-covariance matrixof the coefficient estimates variance-covariance matrix can be derived in the EM-IWLS algorithm and in turn can be used for statistical inferences,
$$
  \text{Var}(\bs\beta^{(t)}) = (\bs X_*^T\bs \Sigma^{-1} \bs X_*)^{-1}\phi^{(t)}.
$$
The rest of the algorithm remains the same as described in Section \ref{sec:EMCD}.

\subsection{A.2 Weight Loss Maintenance Cohort Data Analysis\label{ap:WLM}}
We use the proposed models BHAM to analyze metabolomics data from a recently published study [@Bihlmeyer2021] on the association between metabolic biomarkers and weight loss, where the dataset is publically available [@Bihlmeyer2021_data]. In this analysis, we primarily focused on the analysis of one of the three studies included, weight loss maintenance (WLM) cohort [@Svetkey2008], due to the drastically different intervention effects. In the dataset, 765 metaboliltes in baseline plasma collected were profiled using liquid chromatography mass spectrometry. Quality control and natural log transformation were performed during metabolites data preparation. The outcome of interest was standardized percent change in insulin resistance, and hence modeled using a Gaussian model. After removing missing datapoints and addressing outliers in the data, there were `r nrow(WLM_train_dat)` remaining in the analysis. 5-Knot spline additive models for the gaussian outcome are constructed using two different models, the proposed BHAM and the SB-GAM. 10-Fold cross-validation(CV) are used to choose the optimal tuning parameters of each framework with respect to the default selection criterion implemented in the software. Out-of-bag samples were used for prediction performance evaluation, where deviance, mean squared error (MSE) defined as $\frac{1}{n}\sum\limits^{n}_{i=1}(y_i - \hat y_i)^2$, and mean abosolute error (MAE)defined as $\frac{1}{n}\sum\limits^{n}_{i=1}|y_i - \hat y_i|$ were calculated and tabulated in Table \ref{tab:WLM_res}.

```{r results = "asis"}
rbind(
  'BHAM' = WLM_bamlasso_cv %>% slice_min(mse) %>% select(-s0),
  'SB-GAM' = WLM_SBGAM_cv %>%  slice_min(mse) %>% select(-lambda)
) %>%
  rownames_to_column("Methods")  %>% 
  select(-mae) %>% 
  rename(Deviance = deviance,
         MSE = mse)%>% 
  xtable(
    caption = "Prediction performance of of optimal BHAM and SB-GAM models for Weight Loss Maintenance Cohort by 10-fold cross-validation, including deviance, $R^2$(R2) and mean squared error (MSE)",
    label = "tab:WLM_res"
  ) %>%
  print(comment = FALSE, include.rownames = FALSE, caption.placement = "top")


  # flextable() %>%
  # flextable::autofit() %>%
  # flextable::set_caption(caption = "Measures of optimal BHAM-SSL and SB-GAM models for Weight Loss Maintenance Cohort by 10-fold cross-validation\\label{tab:WLM_res}")
```


\newpage
